[["index.html", "Economics of Public and Social Issues ECO 3253, Fall 2024 About About me Organization of this site Data-driven course Topics to be covered Statistical tools Economic Concepts Projects FAQs", " Economics of Public and Social Issues ECO 3253, Fall 2024 Jonathan Moreno-Medina 2024-08-29 About Welcome to ECO 3253! In this course we will see how we can use data to understand and solve current and important economic problems! You will get a sense of what is the research frontier in applied economics and social science. These include topics like equality of opportunity and mobility, education, innovation and entrepreneurship, health care, climate change, and crime. How will we get there? By doing 3 things: covering the topics with a focus towards using data that can help answer these questions understanding the intuition for how to use data to answer these questions (basics of statistical analysis) using computational tools to help us with the statistical analysis (basics of R) About me Let me briefly tell you about me: I am Colombian. Did my PhD in Economics at Duke University, a Masters in Economics at Université Catholique de Louvain in Belgium, and an undergraduate in (you guessed it!) Economics at Universidad Nacional de Colombia. I am an applied (micro)economist. What is applied microeconomics? Well, first what I do not work on: macroeconomics (inflation, general unemployment, GDP growth, and so on). The ‘micro’ part just means I tend to focus on specific markets (housing and media, being the main ones), and the ‘applied’ just means I use data all the time in my research. Which leads me to my next point. Organization of this site In this site I will put the materials we cover in the lectures so you can refer to it later on your own. I will divide this ‘book’ into two main parts: economic content, and the tools. The tools are both ‘statistical’ (correlations, means, distributions, etc) and ‘computational’ (R). The content of the course is already posted, but as we go thrugh the semester it is likely that I will update some of the notes for the lectures we go through in that week. There is also an Appendix, which you will be able to see on the bottom of the navigation panel, where I will post complementary material where you could brush up several statistical concepts, for example. Lastly, I will also leave a link to the projects you will work on this semester at the end of the list on the left. Data-driven course We will work with data throughout the semester! This will not just be a ‘theoretical’ course. We will work with real world data, and we will try to make sense of it. We will need theoretical to make sense of the world for sure (either coming from economics or statistics), but we are looking at those tools always eyeing real world instances. The course here presented is partly based on the course by Prof. Raj Chetty at Harvard. As we will see later, I also rely on some material on the book ModernDive for teaching the basics of our statistical software R. Just to give you a sense of how much economics has became a much more empirical discipline in later years, below is the number of articles in leading journals that are data-drive in a way. (#fig:fig_econJournals)Source: Mamermesh (JEL 2013) Hence, you will learn to work with data to better think about social and economic issues. Fortunately, the same types of skills that you will acquire for this course are also used by large private companies to navigate their own markets. That is, the skills folks are places like Amazon, Google, or Microsoft might use to navigate analysis of their own market and data are in many ways the same skills that you would need to understand data in tackling challenges like growing inequality and climate change. In order to achieve that goal, the idea of this class is to introduce a broad range of topics, methods, and real-world applications of these sorts of ideas. Fundamentally, we want to start from the questions that motivate the methods we teach in economics and social science, rather than the traditional approach, which is to do the reverse. Topics to be covered The plan for what we will covered in the semester includes: Geography of Upward Mobility in America Causal Effects of Neighborhoods and Characteristics of High-Mobility Areas Historical and International Evidence on the Drivers of Inequality and Mobility Upward Mobility, Innovation, and Growth Higher Education and Upward Mobility Primary education Teachers and Charter Schools Racial Disparities in Economic Opportunity Improving judicial decisions Immigration Political Economy Income taxation Savings and wealth Housing markets and COVID Intro to air and water pollution, and externalities Discount rates, external validity You can check the Schedule, or for more details, please check the syllabus on Canvas. Statistical tools Although we are going to take a topic-oriented focus in this class, we will cover the basics of several methods that will help us make sense of the data. These methods include: Descriptive Data Analysis: correlation, regression Experiments: randomization, non-compliance Quasi-Experiments: regression discontinuity, difference-in-differences Machine Learning: prediction, overfitting, cross-validation R as our statistical software Economic Concepts We will cover and make use if several economic concepts you probably learned in your intro classes, but we will see several instances of how to use it practice. These include: Effects of price incentives Supply and demand Competitive equilibrium Adverse selection Behavioral economics vs. rational models Projects This is very important! A big part of the course will be the projects you will do during the semester. You will work on 4 projects through the semester. These are more involved than most homeworks you have probably worked up to now. The good news is that they involve doing economics! You will get hands-on experience working with real data on real problems. The main recommendation is to start working on this projects early! I cannot emphasize this enough. There are several moving parts to these projects, and you need to plan in advance your work so you can try, fail, come back to it, and so on. If you try to work on these projects just the night before the deadline, that will not leave much room for experimenting, and trying. Given than several of these tools might be new, you need to give yourself time to try more than once. Important! Do not work on these projects just the day before! They are relatively involved, so give yourself enough time. For more details, please check the syllabus on Schedule and on Canvas. FAQs Do I need to know how to program? No! I will give you the basic tools to understand and do basic analysis in R even if you have no background in this sort of things. Do I need to have taken econometrics? No. You will have a bit of a head start if you have, but you do not have to have taken the econometrics course to be successful in this class. I will give you some of the basic conceptual frameworks for how we think about statistical analysis and causal inference. Do I need to know statistics? Basic statistics is definitely recommended. We will have plenty of opportunity to brush up some of those concepts throughout the course, though. You can find a refresher in the Appendix Where can I find more details about this class? You can read more in the syllabus uploaded to Canvas. "],["schedule.html", "Updated Schedule", " Updated Schedule I will keep this page with the updated schedule for the semester. You can find these readings on Canvas. Week Session Topic Required reading Method Deliverable 1: 08/27 1 Introduction to the course 1: 08/29 2 Geography of Upward Mobility in America Chetty, Friedman, Hendren, Jones and Porter (2018)- Non-technical summary 2: 09/03 3 Intro to R and data 2: 09/05 4 Intro to Visualization, Wrangling and RMarkdown First short report (In class) - Project 0 3: 09/10 5 Intro to Causal Effects of Neighborhoods Bergman, Chetty, DeLuca, Hendren, Katz and Palmer (2019) [Non-technical summary] Experiments (RCTs) Project 1 – Part 1 (for Tuesday 09/11) 3: 09/12 6 Characteristics of High-Mobility Areas and Correlation Analysis Bergman, Chetty, DeLuca, Hendren, Katz and Palmer (2019) - Introduction Quasi-experiments 4: 09/17 7 Moving to Opportunity and place based policies Cost-benefit analysis Project 1 – Part 1 and 2 (for Tuesday 09/18) 4: 09/19 8 Historical and International Evidence on and Drivers of Inequality and Mobility 5: 09/24 9 Review for midterm 5: 09/26 10 Midterm 6: 10/01 11 Work in Class For Project 2 6: 10/03 - No Class 7: 10/08 12 Higher Education and Upward Mobility Dynarski, Libassi, Michelmore and Owen (2018) Regression discontinuity 7: 10/10 13 Recap of R and Quarto Documents Project 2 (Tuesday 10/11) 8: 10/15 14 Causal Effects of Higher Education 8: 10/17 15 Primary education Chetty, Friedman and Rockoff (2011) [Non-technical summary] Experiments 9: 10/22 16 Teachers and Charter Schools Event study designs, competitive equilibrium 9: 10/24 17 Racial Disparities in Economic Opportunity Bertrand, and Mullainathan (2004) Dynamic models and steady states 10: 10/29 19 Improving judicial decisions Kleinberg, Lakkaraju, Leskovec, Ludwig and Mullainathan (2017) Machine learning, implicit bias 10: 10/31 20 Immigration Clemens (2011) Welfare analysis 11: 11/05 21 Implementing a simple machine learning model in R - Part 1 11: 11/07 22 Implementing a simple machine learning model in R - Part 2 Project 3 (for Friday 11/08) 12: 11/12 23 Implementing a simple machine learning model in R - Part 3 12: 11/14 24 Political Economy 13: 11/19 - No Class 13: 11/21 25 Income taxation Diamond and Saez (2011) Supply and demand; synthetic control 14: 11/26 26 Intro to air and water pollution, and externalities Moore, Obradovich, Lehner and Baylis (2019) Difference in difference Project 4 - (for Friday 11/27) 15: 12/03 27 Review session 16: 12/12 27 Final Exam "],["lec1_geomobility.html", "Chapter 1 Upward Mobility in the US 1.1 Parental and children income rank 1.2 Interpreting the regression line 1.3 Geographic Variation in Upward Mobility by Commuting Zone 1.4 Local Area Variation in Upward Mobility: Los Angeles, CA", " Chapter 1 Upward Mobility in the US We will first dive into a relatively recent (from 2020) called “The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility.”, by Chetty, Friedman, Hendren, Jones and Porter. The previous link should take you to Blackboard where you will be able to download it. The main question in the paper is the following: ‘how do children’s chances of moving up vary across areas in America?’ To answer it, the authors need to measure upward mobility across the country. How do they do that? They will construct a database called the Opportunity Atlas. How do they measure upward mobility separately by geographic area in the United States? They take data from the 2000 and 2010 Censuses, and link that to information from federal income tax returns. They also use tax return data from 1989 to 2015. Linking those datasets yields information on essentially every American between 1989 and 2015, including how much they are earning, where they live, the dependents they have, and other information, year by year. In that dataset, they want to study economic opportunity across generations. But that requires knowing in the data who is the children of whom. In order to link parents to their children, they use information from dependency claims on tax returns. (In order to receive a tax deduction, parents must enter their child’s Social Security Number on their tax returns.) They’re able to use this information to link 99% of kids in America back to their parents, thereby generating an intergenerational sample where you can study income inequality and mobility across generations. There is a lot you can learn about the US and economic mobility with that sort of data! At the end, they end up with an 8-billion row dataset which covers 20.5 million children born between 1978 and 1983, representing 96% of our target population. They analyze children born during those particular years because we need the children to be old enough that we can measure their earnings reliably. They’re interested in people who were born in the U.S. or are authorized immigrants who came to the U.S. in childhood. They are focusing on authorized immigrants because these datasets don’t go a great job of covering undocumented immigrants. Note here that this is a limitation for the study only in the sense that it does not necessarily paint a picture of how economic mobility looks for undocumented immigrants. Additionally, the number is not 100% because there are some kids who you can’t link to their parents and people you can’t link the census form to the tax form. How do they measure parents’ and children’s incomes in tax data? They do so by measuring incomes using information from the anonymized tax return data. For parents, they use average income between 1994 and 2000 reported on Form 1040, the main tax return in the U.S. Similarly, for kids, we measure average income in 2014 and 2015, the last two years of the data theyworked with. That is when the children are in their mid-30s. Using this information, they’re going to focus on percentile ranks in the national distribution. What that means, concretely, is that they rank kids relative to all the other kids born in the same year, and parents relative to all other parents. They are comparing kids to other kids of the same age. Then likewise, they compare parents to other parents. The reason is that because they want to adjust for the fact that as people grow older, their incomes tend to rise. 1.1 Parental and children income rank The chart below was constructed using data for kids who were raised in the Chicago metro area, which consists of Chicago and the surrounding suburbs. Figure 1.1: Source: Chetty, Hendren, Kline, Saez (2014) Let’s interpret the figure. On the x-axis, it shows the parent rank in the national income distribution. There are a hundred dots here, one corresponding to each percentile of the distribution (see here for a refresher on what the percentiles are). Then in each of those hundred bins, we’re plotting the average ranking of the child in the national income distribution. Now as you go to the right, you’re looking at kids from richer and richer families, and you see that there’s a very strong upward-sloping pattern. This reflects the simple fact that if you were born to a richer family in America, you yourself tend to be richer in adulthood. Now let’s find the line that fits that data most accurately using a method called regression. Then I’m going to focus on the value of this line, called the predicted value, at the 25th percentile of the parent income distribution. There is a lot of information contained in each dot in the graph, but by focusing on the value of this line we can construct a digestible single statistic (i.e., a number) summarizing what upward mobility looks like in each place. 1.2 Interpreting the regression line In Chicago, on average, kids who start out in families at the 25th percentile end up at the 40th percentile. Kids growing up in low-income families in Chicago, roughly speaking, earn about $30,000, on average, when they’re adults. We can’t directly use the value of the dot on the above chart at the 25th percentile. Instead we use a regression line. This is because there is noise and random variation in the data, specifically with smaller samples of people. When working with small samples, it starts to become very important to fit that regression line. That is, we need to use the discipline of a statistical model. That’s the core idea of statistical models, to take the underlying data and represent it in a way that is more stable. 1.2.1 Percentiles The conversion to percentiles is very important here. If we did this analysis in dollars, that relationship is very far from linear. It is very curved, which makes it harder to fit systematically with a statistical model. To construct the Opportunity Atlas, we fit line like this to the kids who grew up in every different census tract in America. 1.2.2 What is a tract? A Census tract is a small definition of a neighborhood that the Census Bureau has created. There are 70,000 Census tracts in America, each of which has about 4,200 people. In order to handle children who might have moved while they were kids, we weigh children by the fraction of their childhood that they spent in each area. 1.3 Geographic Variation in Upward Mobility by Commuting Zone The map below plots average household earnings of children who grew up in low-income families. The map presents this statistic separately for each of the 741 commuting zones (CZs) in the United States. CZs are aggregations of counties based on commuting patterns that are similar to metro areas but cover the entire United States. Figure 1.2: Source: Chetty, Friedman, Hendren, Jones and Porter (2018) Note that the map shows household income in dollars, but the underlying statistic is based on the predicted percentile rank defined earlier. The ranks have been converted to dollars because it’s more intuitive and concrete. In the map, blue colors depict areas with high levels of upward mobility and red colors depict areas with low levels of upward mobility. The map shows broad geographic variation. One of the most interesting features of this map is that the highest upward mobility areas in America are the Great Plains, the rural parts of the country in the center of the country. Charlotte is one of the cities in America with the highest rates of job growth in the United States. Yet, somehow remarkably, for low-income kids who grow up in Charlotte, they do not have very good chances of moving up. The map shows that in the current generation, there are some parts of America where kids’ chances of moving up still look fantastic—actually better than any other country in the world. Then there’s some places, like in much of the industrial Midwest, where your odds of climbing up look worse than any country for which we currently have data. America is a land of tremendous variability in opportunity. 1.3.1 Adjustments for cost of living This map shows nominal incomes, meaning it does not adjust for differences in cost of living. You can redraw this map, adjusting for differences in cost of living. When you do that, you get a map that looks almost identical to the one that I’m showing you here. To put it more precisely, the correlation between that data and these data is .9, meaning that it looks essentially the same. We’re focusing specifically here on kids growing up in low-income families. If you look at kids growing up in middle-class families, it’s broadly similar. If you look at kids growing up in high-income families, you see that there’s significantly less variation across areas for kids growing up in very-high-income families. 1.4 Local Area Variation in Upward Mobility: Los Angeles, CA This geographic variation in upward mobility is not just about broad regional variation, but it’s actually about extremely local variation. We can use the Opportunity Atlas to visualize the data. Figure 1.3: Incarceration Rates for Black Men Raised in the Lowest-Income Households in Los Angeles, CA The Opportunity Atlas starts out with the national map of the same statistics by commuting zone that we were looking at before. However, it allows us to zoom in to areas of specific interest. Let us focus on one particular example: Nickerson Gardens in Los Angeles, CA, which is a public housing project in Watts. Let’s look at black men growing up in the lowest-income families in the bottom 1% of the income distribution, which is actually representative of the incomes of the families living in this public housing project. The average household income of black men who grew up in the poorest families in Watts is just $3,300 a year. It has to be the case that lots of people are basically not working at all. You can see that in a very direct way in these data because we’re able to look not just at income, but a variety of other outcomes, including incarceration. Focusing on incarceration rates, you will see a really shocking and disturbing statistic about the United States, and this area in particular, which is that 44% of the black men who grew up in these lowest-income families are incarcerated on a single day, the date of the 2010 census. If you go down to Compton, you see incarceration rates of 6.2%, which is a factor of 10 smaller than the 44% that we were seeing in Watts for black men growing up in low-income families. Compton is a different neighborhood than Watts, it’s not exactly the same, but I don’t think anybody from L.A. would have predicted that Compton would have drastically different outcomes like this from Watts. That shows you that you can go two miles away and just have a dramatically different picture in terms of what kids’ life trajectories look like. We see that in the stark example here within Los Angeles, but we see that sort of thing more broadly across the United States. "],["lec2_causaleffect.html", "Chapter 2 The Causal Effect of Neighborhoods 2.1 Introduction 2.2 Causal Effects of Neighborhoods vs. Sorting 2.3 Identifying Causal Effects of Neighborhoods 2.4 Stating and assessing the identification assumption for the movers research design 2.5 Why Does Upward Mobility Differ Across Areas? 2.6 The Five Strongest Correlates of Upward Mobility 2.7 Policy Interest in Increasing Upward Mobility 2.8 Policies to Improve Upward Mobility 2.9 Affordable Housing Policies in the United States 2.10 A Framework for Studying the Effects of Housing Vouchers 2.11 Randomized Experiments 2.12 Non-Compliance in Randomized Experiments", " Chapter 2 The Causal Effect of Neighborhoods 2.1 Introduction The national map from the Opportunity Atlas shows rates of upward mobility across America The statistic we are focusing on specifically is the average income in adulthood of kids who grow up in families at the 25th percentile of the parental income distribution. The red colors are areas with lower levels of upward mobility and blue colors are areas with higher levels of upward mobility. For instance, we can look at the household income in adulthood for children from low-income families in South Boston. Figure 2.1: Household Income for Children of Low Income Parents in South Boston In areas like Roxbury, you see outcomes that are on par with some of the lowest upward mobility places in the United States. There is significant local variation across Census tracts in South Boston. This type of variation gives us a potential way to learn about what is driving the difference between the blue and the red areas. 2.2 Causal Effects of Neighborhoods vs. Sorting There are two different explanations for the variation in children’s outcomes across areas. Sorting Causal effects Sorting reflects the simple observation that different people live in different areas. Geographic patterns in upward mobility may reflect the sorting of people across places. If sorting is the main explanation for the patterns we have seen, then the types of policies that we would use to address this issue of equality of opportunity would be people-focused. The causal effects explanation reflects the idea that if we were to take a given child and put that child in the red-colored part of the map versus the blue-colored part of the map, we’d see different outcomes for that given child. Under that causal effect explanation, we might actually want to have policies that target the red-colored places on the map. 2.3 Identifying Causal Effects of Neighborhoods An ideal experiment would be to assign children to neighborhoods at random. Social scientists approximate that sort of ideal experiment with a “quasi-experimental” design, where we come as close to an experiment as we can with the data that we have on-hand. To identify the causal effect of neighborhoods, Chetty and co-authors use a quasi-experimental approach to study three million families who move across Census tracts in observational data. In these data, there are many families who move with their kids across neighborhoods. We exploit variation in the age of the child when a family moves in order to isolate the causal effect of environment. Under an “identification assumption” that makes this as good as an experiment, this research design allows us to estimate the causal effect of neighborhoods on children’s long run outcomes. For example, imagine a hypothetical set of families that moved from Roxbury, where children from low-income families have an average adult income of \\23,000 to Savin Hill, where children from low-income families have an average adult income over $40,000, with kids of different ages, starting with families who moved when their child is exactly two years old. The first dot in the figure below plots what our estimates imply about the impact of this move for their earnings. Figure 2.2: Income Gain from Moving to a Better Neighborhood We then repeat that analysis for kids who moved when they’re three, four, five and so on. Those are the other dots in the figure. The figure shows a very clear declining pattern. The later you make that move from Roxbury to Savin Hill, the less of a gain you get. There are three key lessons that you can see from this chart. Where you grow up really matters. Childhood environment seems to matter more than where you live as an adult. Every extra year of exposure to a better childhood environment improves kids’ long-term outcomes. We use the data on all three million families who move across all the different neighborhoods in America to see how the age at which a child moves to a place where we see average incomes of kids who grew up there from birth impacts how much of that gain they pick up. We use a linear regression to regress the child’s own income in adulthood on the average incomes of the kids we see in the destination versus the origin Census tract. We run a separate regression for kids who move at each of these different ages. 2.4 Stating and assessing the identification assumption for the movers research design Every quasi-experimental design relies on an identification assumption to make it as good as a randomized experiment. In this case, the identification assumption is that the timing of a child’s move to a better or worse area is unrelated to other determinants of the child’s outcomes. Under that assumption, we can interpret the variation as a pure causal effect of moving at different ages. There are two possible concerns in making this assumption. First, the families who move when their children are young may just be different from the families who move when their children are older. To address this first concern, we compare siblings’ outcomes within the same family, in order to control for family effects. We get the same results comparing across siblings within a family, which demonstrates that the results cannot be driven by differences between families who are moving when their kids are young and families who are moving when their kids are old. The second concern with our identification assumption has to do with time-varying factors that are related to where a family moves. To address this second concern, we use differences in neighborhood effects across subgroups to implement what we call “placebo tests.” In the Opportunity Atlas data, there is variation across neighborhoods and subgroups, such as gender, racial group, and so forth that allows us to run this analysis. For example, imagine you have a family that has a son and a daughter. We measure what happens to those kids’ outcomes when they move to a place where boys have particularly good outcomes. We find that their son’s outcomes improve in proportion to the number of years they’re growing up there, but their daughter’s outcomes don’t change at all. We find that kid’s outcomes converge to the full distribution of outcomes that you see in the destination to which they’re moving in a very precise way, which rules out this type of bias. 2.5 Why Does Upward Mobility Differ Across Areas? What is driving these differences in mobility across places? Why do some places produce much better outcomes for disadvantaged kids than others? Figure 2.3: Income Gain from Moving to a Better Neighborhood There’s not much of a relationship between upward mobility and rates of job growth. Some cities with high job growth and low upward mobility are importing talent from other parts of America, so the kids who grew up there aren’t benefiting. High mobility is not fundamentally about indicators of the labor market strength, as measured by variables like job growth as well as wage growth, types of jobs, or types of industry. 2.6 The Five Strongest Correlates of Upward Mobility The five strongest correlates of upward mobility that we have found are: Segregation Places that are more segregated, by race or by income, tend to have significantly lower levels of upward mobility. In very segregated places, like Atlanta, low-income people of all races have poor chances of climbing the income ladder. One explanation is that segregation matters because it creates differences in resources, particularly schools. A different explanation is that the mechanism is spillovers in mentoring and knowledge. Income Inequality Places with a smaller middle class tend to have much less mobility across generations. As we have more inequality within a generation, it might also become harder for kids to climb up across generations. School Quality Places with better schools have higher rates of upward mobility. Family Structure Areas with more single parents have significantly lower rates of upward mobility. This is the strongest predictor in the data. Upward mobility is lower in a community with a lot of single parents, even for children who are growing up in two-parent households. It is not about whether your own parents are married or not, but rather the rate of single-parent households in the broader community. Social Capital Social capital is a bit of a nebulous and complicated concept. “It takes a village to raise a child.” Will someone else help you out even if you’re not doing well? These correlational analyses must be interpreted cautiously. It just gives you clues about what might be going on. 2.7 Policy Interest in Increasing Upward Mobility As a result of putting out these studies in the past four or five years, we have seen a real shift in the national conversation to focus on income mobility and the role of childhood environment, much of it coming through the media. A lot of local areas started to focus on these issues in a very systematic way. One example is Charlotte, N.C. Charlotte is a unique city in that it ranks lowest—50th out of 50—in terms of rates of upward mobility for kids born into low-income families despite being a very successful economy by all traditional measures. Charlotte formed a task force and a commission to focus with all its government agencies, local philanthropies, and so forth to make increasing upward mobility for kids growing up in Charlotte a central priority for the city. Our research group at Harvard has now started to team up with the Charlotte local government and local actors to try to address this problem. The release of the Opportunity Atlas data was really critical for doing this, because those data change the scale of the problem. It allows us to examine data at the level of Census tract, which collapses the scale of the problem. 2.8 Policies to Improve Upward Mobility There are two different conceptual approaches that try to improve outcomes. Figure 2.4: Two Approaches to Increasing Moibility The first is a Moving to Opportunity approach. The simplest thing that you can do to help low-income kids do better is to provide them better access to those neighborhoods. A different approach is Place-Based Investments. The idea is to take the places that are in the red colors of the map and turn them blue. There’s no way we’ll ever be able to achieve full scale purely through the Moving to Opportunity approach. Ultimately, the long run path is to improve low-opportunity places. 2.9 Affordable Housing Policies in the United States In the U.S., we spend a tremendous amount of money in the US on affordable housing policies to help low-income families move to better neighborhoods. These policies include subsidized housing vouchers to rent better apartments. There are also many efforts to create mixed-income affordable housing development, like the low-income housing tax credit. Are these types of housing policies effective in increasing social mobility? Does this Moving to Opportunity sort of approach work? How can we make it better? A very useful benchmark to start from is to think about the simplest alternative to address any problem, which is to just give people cash. Which policy will have a greater impact on kids’ rates of upward mobility, giving you $1,000 in cash or a $1,000 voucher to move to a higher opportunity neighborhood? The conventional economics answer from a traditional economic point of view is that cash grants of an equivalent dollar amount are strictly better than giving someone a voucher specifically focused on housing. Vouchers work out to be more effective than giving people cash, which violates the standard economic model. 2.10 A Framework for Studying the Effects of Housing Vouchers What is the impact of giving a family a housing voucher, the $10,000 of assistance to rent an apartment or a house, on kids’ rates of upward mobility that is kids’ earnings in adulthood? Imagine that you have 10,000 different children and we will number them \\(i=1,…,10000\\). Let us call child \\(i\\)’s earnings \\(Y_i\\). We are going to define \\(Y_i (V=1)\\) as child i’s earnings if the family gets a housing voucher and \\(Y_i (V=0)\\) as the child’s earnings if the family does not get a voucher. \\(Y_i (V=1)\\) and \\(Y_i (V=0)\\) are just two different hypothetical numbers. If your family had gotten a voucher, your earnings are \\(Y_i (V=1)\\), which might be $20,000 a year in the average case. If your family didn’t get a voucher, your earnings are \\(Y_i (V=0)\\), which might another number like $15,000 a year. The objective of the empirical analysis is to estimate the difference \\(Y_i (V=1)-Y_i (V=0)\\). That difference is the causal effect of getting the voucher on kids’ earnings. Let’s call that \\(G_i\\). My goal is to estimate that \\(G_i=Y_i (V=1)-Y_i (V=0)\\). The fundamental problem in empirical science is that you don’t observe \\(Y_i (V=1)\\) and \\(Y_i (V=0)\\) for the same person. The gold standard solution that people have come up with in order to solve that problem is to run a randomized experiment. 2.11 Randomized Experiments Suppose you take those 10,000 kids and you flip a coin to determine if each of them gets a voucher or not. Then you compute the average level of earnings for the 5,000 who randomly got a voucher and the 5,000 who randomly did not get a voucher. The difference between those two averages gives you an estimate of \\(G_i\\), the average treatment effect of getting a voucher. The randomized experiment works because it ensures that the two groups are identical, except for the fact that one of them got the vouchers and one of them didn’t. Everything else is going to be similar about them, which allows you to isolate just the causal effect of getting the voucher itself. Suppose you just compared the average earnings of people who applied versus people who didn’t. The core problem is that there is no guarantee that any difference in earnings that you see between those two groups is driven by getting the voucher itself. 2.12 Non-Compliance in Randomized Experiments A common practical problem that we face in randomized experiments when we try to do this in the field is called noncompliance, when subjects don’t comply with the treatment protocol. For instance, in the voucher case there’s no guarantee that all families given a voucher are going to use it to rent a new apartment. You can adjust the estimated impact for the rate of compliance by dividing the Estimated Impact by the Compliance Rate: \\[\\text{True Effect}=\\frac{\\text{Estimated Impact}}{\\text{Compliance Rate}}\\] In the previous example, we would do \\[\\text{True Effect}=\\frac{\\text{Estimated Impact}}{\\text{Compliance Rate}}=\\frac{1000}{0.5}=2000\\] An important issue is that this estimate is the implied effect for the compliers—the families who would comply with the experimental protocol. A separate question is what would the effect have been for the people who chose not to comply? The local average treatment effect (LATE) applies to a certain set of compliers, but It may not apply to noncompliers. From a policy perspective, we typically care about the treatment effect for the compliers typically. "],["lec3_mto-placebased.html", "Chapter 3 Policies to Increase Upward Mobility 3.1 The Moving to Opportunity Experiment 3.2 Reevaluating the Moving to Opportunities Experiment with a Big Data Approach 3.3 The Three Limits of Randomized Controlled Trials 3.4 How Quasi-Experimental Methods can Address Randomized Controlled Trial Limitations 3.5 Implications for Housing Voucher Policies 3.6 The Stability of Historical Measures of Opportunity 3.7 The Creating Moves to Opportunity Pilot in Seattle 3.8 Three Concerns about the CMTO Approach 3.9 The Idea Behind Place Based Investment 3.10 Place Based Intervention Policies", " Chapter 3 Policies to Increase Upward Mobility 3.1 The Moving to Opportunity Experiment The Moving to Opportunity experiment was implemented in the Clinton administration, by the Housing and Urban Development agency, from 1994 to 1998 in five major cities in the United States: Baltimore, Boston, Chicago, L.A., and New York. The Housing and Urban Development agency randomly assigned 4,600 families to three different groups. The experimental group was offered a housing voucher that required them to move to a low-poverty census tract (less than 10% poverty rate). The Section 8 voucher group was offered the same value of housing voucher but with no restrictions. The control group was not offered a voucher. There was non-compliance and 48% of families assigned to the experimental voucher group actually used the voucher to move to a new place in a low-poverty census tract, and 66% of the Section 8 group used the voucher to lease a new place. Several papers have been written on the Moving to Opportunity experiment. Early research found little effects of moving on earnings and employment rates for adults and older children. Motivated by the experiment on moving between neighborhoods, this experiment was reassessed to look for exposure effects among children. 3.2 Reevaluating the Moving to Opportunities Experiment with a Big Data Approach The hypothesis was that earlier studies didn’t find an effect because they didn’t look at kids who were young enough. Childhood exposure effects look at impacts proportional to how long a child was exposed to a better environment. The new question was does the MTO experiment, the Moving to Opportunity approach, improve outcomes for children who moved particularly when they were very young? To revaluate this experiment, the data from HUD was linked to tax records to follow young children over a long period of time. The way these bars are constructed is that the grey bar always represents the mean for the control group. The blue bar then adds the effect of being in the Section 8 group to the mean for the control group to display the mean for the Section 8 group. The red bar does the same but for the experimental group. These calculations are made adjusting for non-compliance. There’s a very clear difference in the earnings in adulthood for the kids who were assigned to the experimental group relative to Section 8 group relative to the control group. The values below the means are the p-values, which is a way of stating the likelihood that these differences occur due to random variation in the data as opposed to a real casual effect. To interpret these results it’s possible to think of them in two different ways. First, what is the percentage change between the control and the other groups? The difference in earnings between the control group and the experimental group is about a 30% increase, which is large. However, in a real world sense earning about $15,000 a year is still not a large income. One way to tell this story is that there is a large effect but another way to look at it is that this approach isn’t going to help people reach the middle class; it only seems to bring people out of extreme poverty. Below are the results of this analysis for several other outcomes. All graphs are constructed in the way explained above. Neighborhood quality measures the poverty rate in the neighborhood these children live in as adults. On a wide variety of dimensions, it really seems like the Moving to Opportunity intervention actually does work in quite a significant way in improving outcomes for kids who were young. However if this approach is applied to older children (over the age of 13 at the time of the experiment) or adults, then all of these impacts disappear. That is consistent with the previous literature that found little to no effect of this experiment on adults and older children. 3.3 The Three Limits of Randomized Controlled Trials There are three limits to randomized controlled trials. First is attrition. When long-term experiments are conducted, it’s very hard to follow people over long periods of time. However, big data can help address this problem by systematically tracking people via things such as tax records. Second is sample sizes. It’s difficult and expensive to run large experiments however lots of data is needed to have precise statistically analysis. Big data actually cannot solve this problem because while the cost of data has fallen, the cost of running an experiment has not. Third is generalizability. Experiments are always conducted in specific settings and that means that those results don’t always apply to other settings. There’s actually no way to know before hand if the results of an experiment will generalize to some other location with different conditions. 3.4 How Quasi-Experimental Methods can Address Randomized Controlled Trial Limitations Quasi-experimental methods using big data can address some of the limitations of randomized controlled trials. The way to think about quasi-experiments is that what they are doing is approximating experimental conditions by comparing groups that are similar. It’s like the same idea, at a conceptual level, as the experiment. Rather than achieving comparability by randomizing, a quasi-experiment makes a reasonable identification assumption. Quasi-experiments can be much larger which allows them to reach greater levels of statistically precision than randomized controlled trials. With larger samples, one can also run the analysis separately subgroups, such as by race, to that the findings are very generalizable. The limitation of the quasi-experimental method is that it relies on a stronger assumption. One must believe that the two groups in the quasi-experiment are comparable to each other whereas in a randomized controlled trial all you have to believe is that randomization occurred. 3.5 Implications for Housing Voucher Policies The first implication of this evidence is that vouchers should be targeted at families with young children. What the US tends to do currently is put families on wait lists, when they have a kid, to get a housing voucher. Often, those wait lists can be incredibly long; sometimes they can be as long as 10 years. What’s going to happen there is that families get on the wait list when their kids are relatively young. They finally get to move when their kids are older, exactly when it has less benefit. Thus the current US system seems to have this large inefficiency of not targeting the right group. Second, it’s very important to explicitly design these policies to help families move to affordable, high-opportunity areas. The families who had to find a low-poverty census tract to live in to use their voucher are the ones that saw the biggest improvements for their kids. This is especially important because it turns out that something like 80% of the 2.1 million vouchers that are issued by the US government each year are currently used in very high-poverty, low-opportunity neighborhoods. This is possibly happening because moving to opportunity is too expensive. To examine that point look at the following chart. This it the price of buying opportunity for children in Seattle. Each dot here represents a different census tract or a different neighborhood in Seattle and the vertical axis is the measures of upward mobility. The horizontal axis is median two-bedroom rent in that neighborhood in 2015. There is a strong upward-sloping relationship, which shows that more expensive areas tend to be better for kids. However at each median rent there is a lot of variety in outcomes. Take a place like the Central District (shown above), which is where a lot of families with housing vouchers in Seattle currently live. It has poor outcomes. However, Normandy Park is actually less expensive than the Central District but produces dramatically better outcomes for children. This is the idea of an “opportunity bargain” which is currently being used in a pilot study. 3.6 The Stability of Historical Measures of Opportunity On concern is that this data is outdated. To prove that the historical estimates useful predictors of opportunity for children who are growing up in these neighborhoods now this study also examined the accuracy of the predictive power of the data. The methodology was to predict outcomes for kids in 1990 using data that is only one year old, then to predict outcomes for kids in 1990 using data that is two years old. The process was repeated for a total of 10 years. Using data that was 10 years old had a 90% accuracy rate, which is not perfect but still highly informative. 3.7 The Creating Moves to Opportunity Pilot in Seattle A current pilot project is being run in Seattle using all of the previous findings. The pilot provides information to tenants on which neighborhoods might provide better opportunities for their children. It also recruits landlords to accepting tenants with housing vouchers by simplifying the process of dealing with the housing authority. Finally, it offers housing search assistance. 3.8 Three Concerns about the CMTO Approach There are three important potential concerns with the Moving to Opportunity approach. First there is a large cost to the housing voucher program. However, one also has to consider the improved outcomes of children in the future and what that will contribute to future taxes. These children can also have lower incarceration rates and they might depend less on the welfare system so there are lower social costs as well from implementing this policy. Analysis from the Moving to Opportunities experiment showed that the incremental cost of providing vouchers relative to having a family in traditional public housing is actually negative. That is, taxpayers in the long run will actually save money from this program because they’ll end up benefiting more down the road than it cost them to pay for the program in the beginning. The second concern is negative spillovers. Some might be concerned that the integration of lower income families into neighborhoods hurts the higher income families living there already. This, I think, is the most important concern from a political point of view. That can be evaluated by looking at how the outcomes of the rich vary across areas in relation to the outcomes of the poor. Empirically, more integrated cities do not have worse outcomes for the rich, on average. The third concern is a simple limit to scalability. It is impossible to move everyone from one neighborhood to another and expect to have significant effects. Ultimately, this means that a policy-based approach will need to use strategies to improve low-mobility neighborhoods as well as use the moving to opportunity approach. 3.9 The Idea Behind Place Based Investment The goal with place-based investment is to figure out what makes high mobility areas into what they are and change the low mobility areas to resemble the high mobility areas. The first step to do this is to look at key predictors of differences in mobility across places. There are four strong predictors. The strongest predictor these differences are differences in poverty rates. Places that have less concentrated poverty tend to have better outcomes for low-income kids. Another way to say this is that the more mixed-income areas have better outcomes for low-income kids. The second predictor is more stable family structures. That means these places have a larger share of two-parent families. The third predictor is more social capital. Social capital can be defined as connections between low and high-income people. This is the kind place where somebody else might help you out even if you’re not doing well. One example was Salt Lake City with the Mormon Church. The fourth predictor, as you might expect intuitively, is better school quality. The limitation of this type of correlational analysis is that it gives you a sense of what factors might be associated with these differences in outcomes across places, but it doesn’t indicated which of these things have a causal effect. Current research is attempting to figure what actually is having a casual effect in these neighborhoods. Researchers have found that these correlations hold at an incredibly granular level. Once the poverty rate in one specific neighborhood is accounted for, the poverty rate of the next closest neighborhood has practically no predictive power on children’s mobility. This is important because it indicates that environment matters in a very local way. Rerunning this correlation using city blocks rather than census tracts yields that only the poverty rates in the blocks within a .6 mile radius of the block of interest matter in predicting children’s outcomes. This evidence should motivate policy makers to look at addressing mobility not on a national scale but on a local scale. 3.10 Place Based Intervention Policies There are lots of efforts by local governments, by non-profits, lots of people who care about these issues in the United States and elsewhere to try to revitalize neighborhoods. The problem there is very little systematic evidence to date on which of these programs actually work and which of them don’t work. The reason this data doesn’t exist at the moment is that just because neighborhoods get new houses or better schools doesn’t mean that the people living there originally are better off, it could be that better off people moved into the area once the neighborhood was improved. Currently social scientists are working on ways to track these people, who lived in neighborhoods before interventions, across time. Other work at the moment is focusing on building a pipeline for economic opportunity, starting from interventions at very early ages which could be things related to family stability or prenatal care, things like that, to early childhood education. This process would continue throughout childhood with different programs as children age. Throughout this whole process, affordable housing is key since as neighborhoods are improved then more people want to move into the area, which drives up prices, and drives out the original inhabitants. Other issues with this approach are price and scalability. These large interventions can be incredibly expensive. Also, when many interventions are implemented at the same time then it can be difficult to pinpoint which interventions are the most important to improve children’s outcomes. Harlem Children’s Zone is an example of building a pipeline through its Promise Academy, which offers much more support than a typical school in this neighborhood. Other programs such as Becoming a Man or Credible Messengers seek to build social capital by providing mentoring to minority youth. Other programs that are examples of place-based investments include Peer Forward and the Harmony Project. Place based investments are also being investigated using historical data. Census Bureau records actually exist going back to the 1950s, which has allowed social scientists to identify individuals who were affected by government programs such as the Harlem Children’s Zone or the Hope IV Demolitions, when HUD demolished lots of public housing projects to build mixed income housing in other areas. Social scientists are still working on getting through all of this data to be able to study these historical events using a quasi-experimental design. In conclusion, lots of work is currently being done to pinpoint what place-based investments might be the most efficient but at the moment no one knows what the best solution is. "],["lec4_mob-hist-int.html", "Chapter 4 The American Dream in Historical Perspective 4.1 A Historical Perspective on the American Dream 4.2 Measuring the American Dream 4.3 What Policies Can Increase Economic Mobility? 4.4 Policies to Revive Absolute Mobility 4.5 Trends in Upward Mobility: International Comparisons 4.6 Restoring the American Dream: Two Approaches", " Chapter 4 The American Dream in Historical Perspective 4.1 A Historical Perspective on the American Dream A historical perspective can often be useful for understanding the determinants of current outcomes. Researchers today can’t yet study mobility by neighborhood because they don’t have historical data linking parents to their children. However, they have made progress in understanding mobility over time at a national level. The American Dream is typically defined as the aspiration that children should end up better off than their parents. To measure this concretely, researchers can look at children’s earnings compared to their parent’s earning. 4.2 Measuring the American Dream The statistic that we are focusing on measuring in the data is the fraction of children earning more than their parents and how has that changed over time. The paper that examines this is: Chetty, Grusky, Hell, Hendren, Manduca, Narang. “The Fading American Dream: Trends in Absolute Income Mobility Since 1940.” Science 2017 The biggest obstacle to answering this question in the past has been that nobody had historical data linking parents to kids. The study in Science circumvented this problem by estimating historical rates of mobility, even in the absence of parent-children links. That method after briefly examining the findings of this paper. The figure below shows the percentage of children earning more than their parents by the parent’s income percentile. In the 1940s, it didn’t matter what income percentile a child was born into, almost all children earned more than their parents. However, with each passing decade ,there is a dramatic and steady drop in the fraction of kids who are doing better than their parents across essentially the entire income distribution. This is especially true for the middle class, which represents kids born to parents between the 40th and 60th percentile. There’s a dramatic reduction in the odds that you are going to do better than your parents. Another way to examine this is to ask what, “On average what fraction of the kids born in 1940 ended up doing better than their parents did?” The answer to that question is shown in the graph below. In 1940, 92% of kids went on to earn more than their parents. That number has steadily declined To compute this series without the parent-child link,s researchers were able to exploit the fact that almost all kids born in 1940 earned more than all the parents in the pervious generation. Thus, it doesn’t matter which kids are linked to which parents. This point is clear when looking at the distribution of incomes of parents in the 1940s and the incomes of children born in the 1940s. The key thing to note here is that these two distributions are almost non-overlapping. As a result, there is this situation where almost all kids are earning more than all parents. That means that mobility rates are incredibly high. This point is not true for children born in later decades, such as the 1980s, however, there are data linking parents to children in the 1980s, which makes the calculation more straightforward because researchers can directly see which children earned more than their parents. 4.3 What Policies Can Increase Economic Mobility? Since 1940, there have been two major changes in the American economy at the big-picture level. First, the total economic growth rate is lower. Second, there is less equal distribution of that growth, with more going to higher-income people than lower-income compared to the past. This chart is from “The Race between Education and Technology” by Larry Katz and Claudia Goldin. The blue bars show how much household income grew for people between 1947 to 1973 based on which income quintile they were in. The lowest quintile saw growth of three percent in these years, which means that their incomes doubled in about 25 years. The red bars show the same statistic but for 1973 to 2013. The red bars are generally lower, which reflects that income growth has slowed in the whole economy. The red bars are also less equally distributed than the blue. This shows two big changes in the American economy in the past thirty years relative to before: lower growth rates and growing inequality. This graph from a study by Thomas Piketty, Emmanuel Saez, and Gabriel Zucman shows the same statistic but broken down by each percentile in the income distribution. Notice how the curves invert over time. In 1980 (shown by the grey dots), the lower end of the income distribution had the highest income growth rates. In 2014 (shown by the red dots), the wealthiest see the largest growth in their incomes. 4.4 Policies to Revive Absolute Mobility There are two possible scenarios to analyze to revive absolute mobility. The first is the higher growth scenario. Here, the economic growth rate of the 1940s with income distributed as it is today. The second is the more broadly shared growth scenario. This means that Incomes have the same growth rates as today, but they are distributed across income groups as in 1940 cohort. -This is the same chart as before. The orange line shows the higher growth scenario and the green line shows the more broadly shared growth scenario. The lesson from this is that 2/3 of the decline in upward mobility in the U.S. over the past fifty years is due to changes in inequality, and 1/3 is due to the reduction in growth rates. 4.5 Trends in Upward Mobility: International Comparisons Now let’s compare the American experience to trends in other countries. Yonatan Berman applied the methodology from above to several countries across the globe. This graph is very similar to the above graph except that absolute mobility rates are benchmarked to 1940. Baseline refers to the actual data and the fixed inequality series refers to the higher growth scenario from above. Fixed income growth refers to the higher growth scenario from above while fixed inequality refers to the more equally shared growth scenario from above. Berman then repeated this analysis for several countries, shown below. Denmark is interesting because it shows that their incomes have remained distributed in almost the same way since the 1950s. In Denmark, the entire decline in absolute mobility is accounted for by a reduction in growth rates rather than changes in inequality. Denmark has also experienced an overall smaller decline in mobility rates when compared to the US; their total change is about 20% and the US has seen declines of about 40%. Again, almost all of these countries have that pattern where the blue line looks considerably flatter than the black line. In particular, declining growth explains more than rising inequality in all of the other countries it looks at. This means the U.S. is a bit of an outlier in terms of both the magnitude of the decline and the sources of that decline, coming from changes in the distribution of growth rather than the level of growth. 4.6 Restoring the American Dream: Two Approaches What types of policies might actually increase incomes in the bottom and in the middle of the distribution? There are two broad approaches that you might think about. The first is redistribution. This can be done through taxes or a minimum wage law. A different approach is to try to increase the skills of lower-income Americans so that they end up earning more money. Economists typically call these human capital development policies. Piketty and Saez are two economists that have argued that reductions in top income tax rates in the U.S combined with the erosion of unions, and the fall in the minimum wage in real terms has led working-class Americans to fall behind. Broadly, they argue for shifting towards higher tax rates at the top and increase transfers and increase support for incomes at the bottom. A different view is given by Claudia Goldin and Larry Katz in the book ‘The Race Between Education and Technology’, where their central thesis is basically that you need education to keep pace with technological change in order to increase wage rates. They argue that education fell behind in the 1980s. One quick point to make is that education doesn’t just refer to pure technical skills, it can also refer to social skills. Dave Deming has done research on this. This graph shows growth in hourly wage rates by occupation and he classifies occupations into different types based on the technical skills and social skills they require. The occupations that require high social skills have seen the most growth. Intuitively, as more routine tasks get automated then businesses don’t have to pay as much for workers to fill those jobs because they can automate them. Whereas social skills are very high and very difficult to replicate using at least the technology we have available today. As a result, it’s the occupations that require high social skills where wages have grown a lot. The point is that if one thinks that society needs to increase education as a way to address declining mobility rates then society also needs to think about more than just increasing math test scores. Social skills and other non-cognitive skills also need to be improved. "],["lec5_mob-ed.html", "Chapter 5 Higher Education and Upward Mobility 5.1 Education and Upward Mobility 5.2 How do Colleges Shape Income Mobility in the U.S.? 5.3 Effect of the Higher Education System on Mobility 5.4 Estimating the Three Parameters: Data. 5.5 Parents’ Income Distributions by College: Income Segregation in the American Higher Education System. Measuring Parents’ Income. 5.6 Trends in Income Segregation 5.7 Outcomes: Students’ Earnings Distributions 5.8 Differences in Mobility Rates Across Colleges", " Chapter 5 Higher Education and Upward Mobility 5.1 Education and Upward Mobility Education is viewed as one of the most scalable pathways to upward mobility. Education is important in terms of its predictive power, differences in mobility, policy relevance, and changeability. Social capital, family structure, segregation, and other factors may impact long-term outcomes more than education. Education may no longer provide a strong pathway to opportunity in the United States. Lower average standardized test scores compared to other countries. The local property tax finance system means that kids from disadvantaged backgrounds may not have access to the same quality of K-12 education as kids from more advantaged backgrounds. The lack of access to higher education for low-income students due to rising costs. Growth of potentially unhelpful for-profit institutions. Administrative data from colleges and school districts are giving us a much more scientific understanding of the “education production function.” 5.2 How do Colleges Shape Income Mobility in the U.S.? This section is based on “Mobility Report Cards: The Role of Colleges in Intergenerational Mobility” Higher education could provide a pathway to upward mobility that is attractive because it’s not directly shaped by the neighborhood where a child happens to grow up. If children from higher-income families tend to attend better colleges, then the higher education system may not actually promote mobility and may worsen it. 5.3 Effect of the Higher Education System on Mobility The effect of the higher education system on mobility across generations fundamentally depends upon three factors: [Inputs] Parental income distributions by college If there are very few low-income students at a given college, then that college cannot be contributing a great deal to mobility [Outputs] Students’ earning outcomes conditional on parental income by college. How do kids from low income families at Harvard end up doing five or ten years after graduation? [Causal Share] Portion of variation in students’ earnings outcomes that is due to colleges’ causal effects If it’s all about selection and there is no actual treatment effect of attending a different institution, then changing which students attend which colleges may not have a big impact on mobility. 5.4 Estimating the Three Parameters: Data. We estimate these three sets of parameters for every college in America using data covering all college students in the U.S. from 1990 to 2013 using Income tax and IRS records and data from the Department of Education and the College Board. Based on whether or not you attend a college, not whether you graduate. 5.5 Parents’ Income Distributions by College: Income Segregation in the American Higher Education System. Measuring Parents’ Income. We’re going to define parent income as average pre-tax household income during the five-year period when the child is between ages 15 and 19. We’re going to rank parents relative to other parents with children in the same birth cohort. We use the percentile cutoff that’s relevant for your birth cohort. Whether or not you attend college is strongly related to your parent’s income. 65-percentage point gap in college attendance between kids from the lowest income families and the highest income families. In the parent income distribution of students by quintiles, if you were drawing uniformly from the U.S. income distribution using random sampling, you’d have 20% in each quintile. Instead, your probability of attending Harvard is 103 times higher if you’re from the top 1% than the bottom 20%. There are similar trends at other elite universities, especially with legacy admission. The percent of students coming from each quintile only increases substantially once you reach the extreme upper tail of the income distribution ($500,000). Many elite state flagship schools have fewer kids from high-income families than Harvard does, but they do not have significantly more kids from low and middle-income families. In many community colleges, you’ll see a larger share of low-income students than higher-income students. If you’re from a low-income family, you have fewer peers attending your college who are from high-income families than from low-income families. The degree of integration across colleges is mostly similar to the degree of integration across neighborhoods. If you look at the fifth quintile, in both your college and pre-college residential neighborhood, about 10% of the people you’re around are going to be from the top quintile on average. Kids from high-income families at elite universities have fewer peers from low and middle-income backgrounds in college than they did in their own childhood neighborhoods. This pattern continues throughout the income distribution. 5.6 Trends in Income Segregation Up to 2011, there’s no real change in the amount of isolation of kids from high-income families from other backgrounds at either the neighborhood or college level. During 2006-2007, elite private colleges effectively became free for kids from low and middle-income families. There was not a dramatic change in the fraction of low-income kids attending these colleges. As government support shrinks, many state universities are under increasing pressure to increase tuition and admit more out of state students who tend to be higher income. They’re serving a smaller and smaller share of low-income students. There are more low-income kids now attending for-profit institutions and certain two-year community colleges. Combined, these changes essentially leave the amount of segregation in the system as a whole roughly unchanged. To increase the representation of low-income students, colleges must be affordable, but financial aid is not sufficient by itself. 5.7 Outcomes: Students’ Earnings Distributions Some patterns are driven by differences in marriage rates rather than differences in a child’s own earnings if you use household income measures in the early or mid-30s. Kids from low-income families and those from high-income families had virtually the same likelihood of reaching the top fifth of the income distribution at essentially all of the colleges we’ve looked at. Most of the gap in outcomes that we see between children from high and low-income families can be explained by differences between colleges—the colleges they attend—rather than within colleges. Reallocating students across colleges through changing admissions policies could potentially have a significant impact on intergenerational mobility. 5.8 Differences in Mobility Rates Across Colleges Top quintile outcome rate: the fraction of students who reach the top quintile. Low-income access: the fraction of parents from the bottom quintile. Mobility rate of a college: the fraction of a college’s students who come from the bottom quintile and end up in the top quintile. This is a joint probability: what’s the probability that you both come from the bottom quintile and end up in the top quintile? Calculated as low-income access times the top-quintile rate. Colleges that serve a lot of low-income kids tend to have poorer earnings outcomes. That could either be because those colleges don’t have a great treatment effect or because of selection. Treatment effect: maybe those colleges don’t provide good instruction or networks. Selection: the backgrounds of the kids who attend these colleges are different than those of the kids at Harvard. The Ivy Plus institutions are all concentrated on the upper left of this chart. Relatively few low-income kids and some of the best earnings outcomes. State public flagships have very good outcomes, but they don’t have dramatically higher representation of low-income kids than the elite private colleges. The colleges that have the highest mobility rates are in the upper right. A lot of low-income students and pretty good outcomes for low-income kids. Mid-tier public institutions like Cal State L.A, Stony Brook, and the City University of New York. Mobility rates of around 8%, or even 10% at Cal State L.A. If you have relatively few low-income kids, you can’t have a lot of mobility, as defined in this way, because you’re working with a small pool of low-income kids to begin with. There are very sharp differences in mobility rates across colleges. If you want to increase mobility through the higher education system, you want to increase the representation of low-income kids at colleges with the best outcomes like Harvard and the University of Michigan Ann Arbor. There are colleges with many low-income students, but low mobility rates due to poor outcomes. Improving the outcomes in colleges with poor outcomes involves improvements on the margin, like trying to get kids not to drop out and do better in the college. "],["lec6_ed-higher.html", "Chapter 6 The Causal Effect of Colleges 6.1 Causal Effects of Colleges 6.2 Estimating the Causal Effects of Colleges 6.3 Regression Discontinuity Methods 6.4 Zimmerman Study 6.5 Causal Effects of Colleges 6.6 University of Michigan HAIL Experiment", " Chapter 6 The Causal Effect of Colleges 6.1 Causal Effects of Colleges What fraction of the earnings variation across colleges that is due to causal effects as opposed to selection? Only selection: students who come to Harvard are a very highly selected group of people and they presumably would have done quite well even if they had gone to college somewhere else. Reducing segregation across colleges through changes in admissions rules would have no impact on economic mobility. Causal effect: value added from attending Harvard; going to Harvard changes your earnings relative to going to your local state school. Changes in admission policies could have a big impact on mobility. 6.2 Estimating the Causal Effects of Colleges Ideal experiment to estimate this causal effect: randomly assign kids to all of the different colleges in the U.S. and compare their earnings at age 30. Instead: quasi-experimental design that effectively allocates comparable students to different colleges. We need to find some source of variation that allows quasi-random allocation of students across colleges, to compare students at college A to college B. 3,000 colleges in the U.S., 3,000 different causal effects. Ideal: 3,000 different experiments comparing college A to college B, college B to college C… 6.3 Regression Discontinuity Methods In this paper by Seth Zimmerman, called “The Returns to College Admission for Academically Marginal Students”, he estimates the causal effects of Florida International University (FIU) by exploiting admissions cutoffs. 6.4 Zimmerman Study Zimmerman compares students just above, and just below, the state-level GPA cutoff for admission to the Florida State University system. Depends upon your SAT score, and then the GPA that you need, in order to get into FIU (lowest-ranked four-year state college in Florida). Lots of students with SAT scores slightly below 970. Then think about the student who also has a GPA of 2.99: by this rule, this student is not going to get in, but a student with a GPA of 3.01 will get in. Exploit this variation to identify the causal effect of attending FIU. Graph on the left: the horizontal axis is how far away your GPA is from the cutoff that’s relevant for you, given your SAT score. If your SAT score was below 970, then zero corresponds to 3.0. The vertical access is plotting what fraction of kids get admitted to FIU. Big jump in adherence with the rule right at the cutoff. Not from zero to 100%—they’re not mechanically following this rule. Your odds of getting into FIU are 23 percentage points higher if your GPA happens to be 3.01, rather than 2.99. Graph on right: we care about actually attending, not just getting in. The cutoff actually affects who shows up at FIU, not just the admission margins. The people on the right of the cutoff are 10 percentage points more likely to attend FIU than the people just to the left of the cutoff. Those with a GPA just above the cutoff are the treatment group in an experiment, they got the FIU treatment. Those just below the GPA cutoff are the control group, the people who did not get into FIU. The control group typically ends up attending a two-year community college in Florida. The experiment is attending a four-year state university, rather than a two-year community college. Compare the earnings outcomes of the kids who ended up just above the cutoff (GPA of 3.01), versus the earnings outcomes of the kids who ended up just below the cutoff (GPA of 2.99). Quasi-experiments require an identification assumption that allows you to estimate causal effects. The assumption must make your quasi-experiment as good as an experiment. Key identification assumption for regression discontinuity: all other determinants of earnings are balanced (comparable) on either side of the cutoff. People in the control group are comparable to the people in the treatment group. An experiment guarantees that by randomization. Here we’re going to make an assumption that the people with a GPA of 2.99 are pretty similar to the people with a GPA of 3.01. Suppose we make the assumption that those two sets of people are comparable in terms of overall merit, it’s just by chance you ended up with a GPA that was a bit lower than someone else. Then any jump in earnings, at the threshold, when we cross from 2.99 to 3.01, has to be due to the discrete jump of the chance of attending FIU instead of a community college. We can attribute it to the causal effect of attending FIU instead of a community college. Plausible assumption because the admissions threshold was not widely publicized. If the cutoffs were well-publicized, you might worry that the students just above the cutoff would be different from those just below the cutoff because there’s a strong incentive if you want to get into the university to try to do something to get your GPA just over 3.0. The students who managed to do that maybe have put in a little bit of extra effort, or they came from a certain school where their guidance counselor helped them figure out how to get over the threshold. This is manipulation of the running variable that is creating this regression discontinuity. Evaluate the validity of the assumption: make sure the observable characteristics that you can see in the data, students’ characteristics, look relatively similar on both sides of the cutoffs. Plot the fraction of students who are black around the same admissions cutoff: that number hovers around 30%, both for the students who are just below the cutoff and just above the cutoff. If we saw a huge difference here on the left and the right, we don’t have a valid experiment; basically, we wouldn’t be comparing similar people on the left and the right. Gender also looks relatively similar. Treatment effect of attending FIU: plot mean quarterly earnings around that cutoff. Administrative data on earnings of students who previously either attended FIU or didn’t. Plot earnings now around the GPA cutoff, eight to 14 years after you graduated from high school. Clear jump in earnings, to the right of the cutoff, relative to the left. The people to the right had a significantly higher chance of attending FIU than the people to the left. Because we assume these people are comparable on other dimensions, we can attribute the $372 jump in earnings at that point to the causal effect of attending FIU instead of your local community college. That’s the regression discontinuity method: plot the outcomes, versus the variable of interest, and then look for a jump at the relevant cutoff. 6.5 Causal Effects of Colleges Use earnings controlling for SAT scores, and parent income, as an estimate of each college’s causal effect. Infeasible to do a random or quasi-random assignment at every college; instead, take a set of kids who have the same SAT scores and parent income. Find a set of kids who have those characteristics at Harvard and a set of kids who have those characteristics at the University of Michigan. Attribute any difference in earnings on average, between those two sets of kids, to the causal effect of attending Harvard instead of attending Michigan. Using SAT scores and parent income to control for selection, or differences in student background, works perfectly if the only dimension on which colleges are actually selecting students is based on SAT scores and parent income. Compare SAT/parent income control method to the quasi-experimental estimates you get at FIU: similar results. There is some scope to increase low and middle-income shares at highly selective colleges by admitting and enrolling more high-achieving, lower-income students. It could be because they’re not applying, they’re not getting in, or they’re not choosing to enroll because of the cost. Lower application rates of well-qualified low-income students (undermatching). Cost of college: the red bars are the sticker prices. For a kid from a low-income family the cost that you’d actually face, taking financial aid into account, is shown by the blue bar, which is lower than the cost of going to some of the less competitive schools. At top colleges, cost by itself doesn’t seem like the key barrier. Lack of information or support in the application process for low-income students seems more plausible. 6.6 University of Michigan HAIL Experiment Sue Dynarski and peers test this information hypothesis by running an experiment at the University of Michigan (UofM). High-achieving students: GPA above 3.3 and an SAT score above 1100. Low-income families: kids growing up in families that have an income below about $50,000 a year. Administrative data from schools of every student in Michigan. Proxy for low-income families: eligibility for a free or reduced-price lunch. Assign 50% of those students, randomly, to receive a treatment of additional information and support to apply to the UofM. Brochures saying you can attend with free tuition. Everybody was eligible given their income cutoffs. But they’re making that very clear. Waiving fees that are often deterrents to applying. Informing them the FAFSA is free. Reach out to both students and parents with personalized mailing. Controlled treatment comparisons: 67% of the students in the treatment group applied to the University of Michigan, only 26% of the students in the control group did. 32% of the students in the treatment group got into the UofM; 15% in the control group. 27% of the students in the treatment group are currently attending the UofM; 12% of the students in the control group are. With simple, low-cost mailing, they increased the fraction of highly qualified students who attend the UofM, by about 15 percentage points, doubling the rate of attending the UofM in this group. Students continue to attend the UofM two years after the experimental intervention. Many of these students would have attended community colleges had they not attended the UofM. Removing cost and informational barriers, for high-achieving, low-income students can increase their access to highly selective colleges appreciably. By itself, this is not the solution. Starting from a pool of 2,000 students, you’re increasing the fraction of students who attend the UofM by about 15%, or 300 extra students per year. This will not alone desegregate the higher education system. Even if you hypothetically eliminated all barriers, and had pure SAT-based applications and admission, that would not change low-income shares appreciably, especially at elite colleges. "],["lec7_k12.html", "Chapter 7 Primary Education 7.1 A Recap of the Discussion on the Causal Effects of College 7.2 Questions on Causal Effects of College 7.3 An Overview of the K-12 System in the US 7.4 Do Test Scores Predict Long-Run Outcomes? 7.5 Making Test Scores Comparable 7.6 Achievement Gaps in Test Scores by Socioeconomic Status 7.7 Two Policy Paradigms 7.8 Government Based Solutions to Improving Schools 7.9 The Effect of Class Size 7.10 Class Size Cutoffs in Sweden 7.11 Questions on Primary Education", " Chapter 7 Primary Education 7.1 A Recap of the Discussion on the Causal Effects of College Potential trade-off (value judgment): is there an amount of meritocracy you’ll have to sacrifice in order to increase social mobility? Meritocracy: the people who are most qualified to get into college should be the ones who get in (Note: SAT score is not a pure measure of merit). Increasing social mobility: giving kids from low-income families access to the types of institutions that might serve as a launching pad to reaching higher levels of the income distribution or other beneficial outcomes. Potential to achieve both of these objectives at the same time. 7.2 Questions on Causal Effects of College Would the fraction of racial groups at Harvard likely change if we moved to a purely class-based affirmative action system? Yes, significantly. Race plays a very important role even conditional on class and kids’ outcomes and qualifications. Race is not interchangeable with class. They’re correlated and minorities tend to have lower income. Admitting more low-income kids would likely result in more low-income white kids. There are a number of more middle-class kids from minority backgrounds who are now admitted at a higher rate. 7.3 An Overview of the K-12 System in the US In principle, if you improve the environment that kids are born into, you potentially face less of a tradeoff later and have more of an impact. The US spends nearly $1 trillion per year on K-12 education. Variation in school funding: public schools are funded by local property taxes. More affluent neighborhoods tend to have better-funded public schools than less affluent neighborhoods. You could instead have a national tax base for local schools. 7.4 Do Test Scores Predict Long-Run Outcomes? Data: standardized test scores at the end of the year obtained directly from school districts. In the US, there are now standardized tests administered in virtually all states to all students. You see test scores much more quickly than you do long term outcomes. Are test scores actually a good measure of learning? Are they actually a good measure of long-run outcomes? If a teacher is really good at raising kids’ test scores, is it that the kids in that classroom are better at taking tests? Maybe a teacher’s really good at teaching to the test and that makes the kids get higher test scores in third grade, but they don’t necessarily do better later in life. Or is it actually about the acquisition of skills that have value later in life? Data on 12,000 kids who were in various kindergarten classrooms in Tennessee in 1985. Link the school district data and test score data to the tax records. Then see whether kindergarten test score performance predicts later outcomes. Your kindergarten test score percentile is on the x-axis. Your earnings are on the y-axis. You can see that there’s quite a strong relationship between those two things. Average incomes go from 10,000 to 25,000. It’s a two and a half fold difference in income but the numbers are relatively low. We’re looking at a disadvantaged population in Tennessee. Binned scatter plot: too many data points to see them all, so we show how average incomes vary with test scores by binning the data. We’re dividing the x-axis test scores into 20 equal-sized bins. That means that there’s 5% of the data in each of those bins. Each dot represents the average level of earnings versus the average level of test scores in each of those bins. Students scoring between the 45th and 50th percentile earn about $17,000 on average. In all of these bins, we’re going to see a lot of variance around those averages. Kindergarten test scores are on average a really good predictor of your future earnings, but they are not perfect predictors. The R squared statistic, which measures the amount of explained variation in the outcome variable, shows that the test scores explain only 5% of the variation in earnings. Kindergarten test scores are strong predictors of many outcomes to compare performance across schools and subgroups: For those who were at the bottom of their kindergarten class, the college attendance rate is less than 20%. For those who were at the top of their kindergarten class, the college attendance rate is over 80%. 7.5 Making Test Scores Comparable The standardized tests are usually not national tests, so they are not all on the same scale. Sean Reardon created a standardized measure of test score performance for all schools in America. He used information on 215 million test scores for students from 11,000 school districts across the US from 2009 to 2013 in grades three to eight. He ranked each school district’s average scores within the statewide distribution: from best to worst within a given grade, year, and subject. He used data from a national test that’s administered to a sample of students by the Department of Education every year to convert the state-specific rankings to a national scale. That gives you an exchange rate of sorts between different states, because everything is being put on the same scale in that test.  If California students score five percentiles below the national average on the national test. In the district-specific data, suppose you’ve got a California school that’s 10 percentiles below the California average. And we know California on average relative to the nation is five percentile points below the national mean. That tells you that this specific district in California is 15 percentile points below the national mean, because it’s 10 percentiles below the California average and California itself is five percentiles below the nation. Reardon converted everything into grade-level equivalents: if a kid is reading at a fifth grade level, etc. Not necessarily about the causal effect of the schools on test scores. A big part of what’s driving this is the socioeconomic status of the students. There are more low-income kids in the districts on the left of this graph and more high-income kids in the districts on the right. In the following graph, purple represents kids who are two and a half or more grades below where they ought to be, given the national average. Green represents kids who are two and a half grades above. This is somewhat similar to the maps of upward mobility. The Northeast, parts of the West Coast, and much of the Great Plains have particularly good outcomes. This shows how there is tremendous local variation in these test score outcomes. 7.6 Achievement Gaps in Test Scores by Socioeconomic Status How do test scores vary across socioeconomic groups? Define an index of socioeconomic status using census data on things like poverty rates, income, the fraction of college graduates, and single-parent rates. There are all correlated with socioeconomic status. Reardon averaged those all together to create an index of places that are basically more affluent versus less affluent. Plot how many grades ahead or behind your district is versus this measure of socioeconomic affluence. The places on the right side of this graph are very high-income places and the places on the left side of the graph are very low-income places. Each dot represents a different school district and then the size of the circle corresponds to the size of the school district in terms of the number of students. Strong upward-sloping relationship, a lot of the variation in average achievement levels across school districts in the US is likely explained by socioeconomic status. Variation even conditional on socioeconomic status. If you pull out all the Massachusetts school districts and all the California school districts, even at the same level of socioeconomic status, kids in Massachusetts public schools are about one grade level or one and a half grade levels ahead of kids in California. There’s likely something quite different about the school system in California. Another way to cut the data separately is by cutting on the student’s own parents’ income using a free and reduced-price lunch measure. Cutting each school district into two groups: red (nonpoor) and yellow (poor). The gap in achievement between low and high-income kids is actually larger in the best school districts. The higher-income kids benefit a lot from being in these more affluent places and the lower-income kids only benefit a little bit. The US often doesn’t do very well in terms of average test scores in comparison to other countries. Plot average test scores versus average spending in primary education, you don’t see a strong relationship. Spending more money might lead to better outcomes, but the core reason the US has poorer performance is not that we spend dramatically less money on education than other places. Lots of countries spend less per pupil than the United States and have dramatically better outcomes on the PISA test. 7.7 Two Policy Paradigms Government based approach: how do you improve public schools by doing specific things to them (reducing class size, increasing teacher quality, etc.). Private market forces: instead of providing education, the government only funds it. 7.8 Government Based Solutions to Improving Schools The education production function: Output: student learning, which may be measured by test scores. Inputs: things you can control, such as the number of students in each class, the types of teachers you hire, and technology. Restricted by your budget. 7.9 The Effect of Class Size Can’t just compare year-end test scores of classes that are smaller to classes that are bigger. This is an invalid estimate of the causal effect, because students in schools with smaller classes will tend to be different in many other ways: higher income backgrounds, from school districts with more money, which are the ones that can hire more teachers. Overstates the effect of class size: kids in small classes do much better, but it might be because of other things that are correlated with being in a small class. 7.9.1 The Tennessee Star Experiment The Student Teacher Achievement Ratio experiment aimed to understand whether class sizes affect student outcomes. Between 1985 and 1989 in Tennessee they took about 12,000 kids in grades K through three at 79 mostly disadvantaged schools. They randomized students and teachers into classrooms within each school. Student who got assigned to small classes had an average of 15 students in their class; students assigned to large classes had 22 students on average. Compare the average outcomes of kids in small versus large classes. Random assignment means their characteristics will be comparable on average. Comparison of means: Indicator variable: a variable that takes two values, zero or one. Zero signifies being in a large class, one means the student was in a small class. Regression table: Top row: the dependent (outcome) variable. First column: the dependent variable is test score. The first number reflects the impact of being in a small class. It is the coefficient when you regress test scores on that indicator for being in a small class. The kids randomly assigned to the small class are doing 4.8 percentile points better on average than the kids in the large class. The number you see underneath in parentheses is the standard error. If I repeated this experiment thousands of times, I won’t always get 4.81. The standard error is a way of quantifying how much uncertainty there is in the estimate due to random chance (margin of error). Use the standard error to construct the 95% confidence interval. The 95% confidence interval is plus or minus 1.96 times the standard error. Suppose we were to repeat the Tennessee STAR experiment 100 times; 95 of those 100 times, we would expect the estimate to lie within the 95% confidence interval. We’re decently confident that there’s a significant effect of being in a small class: three to seven percentile points. The average test score in this sample is the 48.6th percentile. Our estimated effect says you’re going up 4.8 percentiles relative to that average. Second column: your probability of going to college goes up by about two percentage points if you were assigned to a small kindergarten class. We’re evaluating the effect of being assigned to a small class for one year. Being in a small class with the same effect for multiple years would have a more significant effect. Having two percentage points higher probability of going to college for each year of exposure to a small class is going to give you about a 20 percentage point impact from reducing class size throughout school. Third column: the earnings estimate is minus $4, which is basically zero. Concluding that putting kids in smaller classes doesn’t improve their earnings is incorrect. The 95% confidence interval (plus or minus 1.96 times 327) for the earnings impact is very large. The estimate could be anywhere from minus $640 to plus $637 of annual income per year relative to an average of 15,000. It’s possible that placing children in a smaller class for just one year increases their earnings by as much as 4%. This estimate of minus $4 means you don’t have enough data to say for sure whether small classes matter or not for earnings. Class size seems to increase test scores. There’s some evidence that it increases college attendance rates. But it’s too small to estimate the impacts of class size on earnings precisely. 7.10 Class Size Cutoffs in Sweden Fredriksson et al use administrative data from Sweden for a quasi-experimental method, regression discontinuity, with more precise results. Sweden imposes a maximum class size of 25 students. If your school has 26 students, you split your grade into two classes of 13 students each. If you have 25 students, you could have one class of 25 students. When you cross this threshold of going from 25 to 26 students, the average size of the class that students are in is going to suddenly fall. This will happen every time you cross an integer multiple of 25. The schools that happen to have 26 students in a given grade are likely to be very similar to the schools that happen to have 25 students just by chance in a grade. That allows you to identify the causal effects of class size by comparing the outcomes of students in schools with 26 students versus 25 students in a given grade. The vertical line is the cutoff for maximum class size. The x-axis is distance from the cutoff: zero is the point at which you cross that threshold. The vertical axis is the average class size that students are actually in versus the enrollment relative to the threshold in that grade. Right when you cross that threshold, when you have 26 students instead of 25, the average class size falls by on average about five students. That five-student reduction in class size is basically a treatment for the kids who happen to be just on the right relative to the kids who happen to be in schools just on the left of the threshold. The underlying assumption that we’re going to make is that there’s no other difference for the students who happen to be just to the right relative to just to the left. That appears to be the case if you look at the characteristics of students. There’s a declining relationship between test scores and class size. Clear jump right when you cross the cutoff: when the average class size in your school is five students smaller, your test scores go up by about 0.2 standard deviations (y-axis unit). Earnings jump by about 4% at the cutoff. The quasi-experimental methodology is consistent with the Tennessee STAR experiment and suggests that class size has important impacts not just on student achievement as measured by test scores but on later outcomes as well. Reducing class sizes in primary school by hiring more teachers can have quite large returns. If you take a kid in a family at the 25th percentile in the US and add up their total earnings over their life, on average it’s going to be $1.3 million. But money earned 50 years from now is worth less than money earned today, because money earned today can be invested and grown (present value). If you put less weight on the income of these children based on how far in the future that income is earned and then sum all of those figures up, the cash equivalent in terms of average earnings today for a child born to a low-income family is about half a million dollars. If we put a single kid in a smaller class, that’s going to increase the present value of their lifetime earnings by about $20,000. 7.11 Questions on Primary Education The first test in Tennessee you said found the size of a one-year effect. Is the Fredriksson also the one-year effect of smaller class size? You should interpret it as basically the effect of a one-year difference in class size. Is it reasonable to expect that if we scale this everyone will do better? Or at some point will one group of students lose as another group gains? Our studies involve partial equilibrium: we’re changing one kid’s environment and looking at the impacts on that kid’s earnings, holding fixed everything else in the economy. But general equilibrium asks does the equilibrium itself change if you put everybody in smaller classes? Say you put everybody in small classes. You might see that for a given kid if they’re in a smaller class, they’re more likely to get into Harvard. That doesn’t mean the total number of kids who go to Harvard is going to change. There’s 100% crowd out effect: if you do better, I do worse. Human capital building: you’re smarter because you were in a smaller class, so you invent new things. That just allows you to earn more and doesn’t take away my slice of the pie. When you have millions of people getting more education because the minimum drop out age changed, crowd out effects are not that large, so most of it is about making people more productive than competing for the slice of the pie in a zero sum game. Just from this evidence, you wouldn’t necessarily be able to tell. When we talked about moving to opportunity, we saw that lower-income kids gained from moving to higher resource neighborhoods. And yet the data you just showed up says that low-income kids in high resource schools do worse than their peers. Is this possible? Low-income kids do better from being in a more affluent school district, but high income kids do even better, so the gap gets worse. With moving to opportunity, it wasn’t just about average outcomes. Lots of other things mattered in terms of defining a good neighborhood and a good school. So there might be ways to help people move to opportunity which is not just on the dimension moving to the right side of that chart. You’re going to the same school; it’s just the size of the class you’re in. "],["lec8_teachers-charters.html", "Chapter 8 Teachers and Charter School 8.1 Using Big Data to Study Teachers’ Impacts 8.2 Measuring Teacher Quality: Test-Score Based Metrics 8.3 Debate about Teacher Value-Added Measures 8.4 Measuring the Impacts of Teachers 8.5 A Quasi-Experiment: Entry of a High Value-Added Teacher 8.6 Lesson 1: VA Estimates are Unbiased Measures of Teacher Effectiveness 8.7 Lesson 2: VA Estimates Based on Test Scores Predict Teachers’ Long Term Impacts 8.8 Lesson 3: VA Estimates Based on a Few Years of Data are Sufficiently Reliable to Generate Large Gains on Average 8.9 Policy Impacts 8.10 Market Based-Solutions: Charter Schools 8.11 Do Charter Schools Work?", " Chapter 8 Teachers and Charter School 8.1 Using Big Data to Study Teachers’ Impacts John Friedman and Jonah Rockoff use data on all kids who went to New York City public schools between 1989 and 2009. Link that information to the information from tax records so that we can look at kids’ earnings, college attendance rates, teenage birth rates, and various other outcomes. 8.2 Measuring Teacher Quality: Test-Score Based Metrics One measure of teacher quality: ‘teacher value-added’ or ‘test score-based metrics of teacher performance’. How much does a teacher raise his or her students’ test scores on average? Example: for a fourth grade teacher, my value-added is my students’ average test scores at the end of fourth grade minus their average test score at the beginning of fourth grade or at the end of third grade. Adjust for noise and control for differences in student characteristics. If my students’ test score performance is much higher at the end of fourth grade than it was at the beginning of fourth grade, I’m a high value-added teacher. 8.3 Debate about Teacher Value-Added Measures Potential for bias in these measures of teacher quality. Sorting versus causal effects: do differences in test score gains across teachers capture the causal impact of that teacher or does it capture differences in the types of students that different teachers happen to get? Lack of evidence on teachers’ long-term impacts: are these teachers who raise test scores actually good at improving students’ long-term outcomes or are they good at teaching to the test? Potential instability of value-added estimates: When you only have a few classes of data, these estimates could be too unstable to be useful inputs in evaluating teachers. 8.4 Measuring the Impacts of Teachers Ideal: randomly assign students to teachers with different levels of value-added. Identify with the historical data some teachers as being high value-added, some teachers as being low value-added. Take a fresh set of students and randomly assign those students to teachers and see what happens to their outcomes. In practice: use a quasi-experimental approximation. Exploit the high turnover in teachers across school years. If a high value-added teacher enters a new school in a given year, you can compare students’ outcomes before that high value-added teacher entered versus after. Like an experiment because from the students’ point of view when that teacher happened to show up is essentially random. 8.5 A Quasi-Experiment: Entry of a High Value-Added Teacher Event study design example: tracking a given school in New York City over time. Take fourth graders in the school who get to fourth grade in different school years: fourth graders in 1993, fourth graders in 1994, etc. At the end of the 1995 school year, a new teacher who comes in who’s in the top five percent of the distribution in terms of their teacher value-added. Average over thousands of such events. Test scores immediately jump up and stay high as this teacher continues to teach subsequent cohorts of kids in the school. It looks like a significant causal effect on raising the average performance of students in fourth grade in this school. The treatment group experiences the entry of the new teacher. Worry that when this better teacher came in, that was correlated with other changes in the school that benefited kids and this is not all the causal effect of the teacher. Control group: test scores for third graders in each of these years. The new teacher shouldn’t have any impact on achievement in third grade because those students haven’t had the better teacher. Third grade test scores are flat around the point when the new teacher comes in fourth grade. If you get a better math teacher, math scores go up and English scores do not. The year of entry is the year of the event, zero. Think about time relative to that year. Re-normalize the point of entry to be zero for all of these 2000 different events in the data. Take all of those events and stack them on top of each other and then take an average in the year before entry, the year after entry, etc. The entry of a teacher in the bottom five percent of the distribution of value-added immediately pulls down test scores relative to the prior grade. The best teachers having a positive impact, the lowest-rated teachers have a relatively negative impact, and then it’s mostly linear throughout the distribution. Where you have good teachers entering might be different in the base years from where you happen to have the lower value-added teachers entering. Focus on the change exactly around the point of entry. Don’t just want to compare across schools, since there are lots of differences across the schools that might generate differences in scores. 8.6 Lesson 1: VA Estimates are Unbiased Measures of Teacher Effectiveness Being assigned to a teacher who is a 10 percentile higher value-added teacher does raise students’ scores by about 10 percentiles. If I estimate value-added in observational data where there’s no random assignment of students and I find one teacher is 10 percentiles higher value-added than another, the vast majority of that difference reflects a causal effect, shown by the fact that if I randomly assign or quasi-randomly assign a new set of students to those teachers, I get about a 10-percentile point difference. Kids who were assigned to a higher value-added teacher in third grade are significantly more likely (two percentage points) to attend college than kids who are assigned to lower value-added teachers. This is the causal effect of having a higher value-added teacher for a single year. If you have a higher value-added teacher in third grade, and fourth grade, etc., that can add up. These value-added measures are actually picking up quite a bit of variation in what teachers do in terms of later outcomes. If you’re assigned to a higher value-added teacher, you tend to have higher earnings than if you’re assigned to a lower value-added teacher. Noisier because earnings are much more variable than the zero-one college attendance variable. Girls who have lower value-added teachers are significantly more likely to have teenage births than girls who have higher value-added teachers in elementary school. 8.7 Lesson 2: VA Estimates Based on Test Scores Predict Teachers’ Long Term Impacts Take the bell curve of teacher quality as measured by teacher value-added estimates. Identify the teachers who were in the bottom five percent. Give them training or hire new teachers such that we bring them up to average quality. That would increase the lifetime earnings of a single child by $80,000: a couple thousand dollars a year, added up over 40 years. That is a gain of about $2.2 million per classroom. Money 50 years from now is worth less than money today. Discount the future stream of payments back at a five percent interest rate (high), you end up with a present value of about $400,000. 8.8 Lesson 3: VA Estimates Based on a Few Years of Data are Sufficiently Reliable to Generate Large Gains on Average The $400,000 gain overstates the feasible gain because I assumed we knew with perfect certainty every teacher is value-added. Value-added estimates based on just a couple of classes are going to be statistically imprecise because teachers who happen to have students who do well by chance will get a high value-added score. Identify the teachers in the bottom five percent of the distribution of value-added. Ideal: identify those guys in the bottom five percent highlighted in yellow. In practice: given only a few years of classes to look at per teacher, the true value-added of the teachers who we identify might not actually be the bottom five percent. Some teachers’ true value-added is going to be relatively good at the 25th or 30th percentile, but whose estimated value-added after three years is in the bottom five percent. Those teachers happen to end up in the bottom five percent just because they had a bad day on the test, etc. But essentially no teacher whose true quality is above the median will get rated in the bottom five percent just because of random chance. After you’ve taught for three years, you have enough information to not identify teachers completely incorrectly. Quantify how much of a gain you get when you have limited data to measure teacher quality. Hypothetically, if you perfectly measure teacher value-added, you would gain $407,000 from replacing teachers in the bottom five percent with teachers of average quality. What is the gain you can actually achieve if you use one year of data to estimate teacher quality, two years of data, etc.? Those numbers are lower because you’re going to make a bunch of mistakes. But once you have two or three years of data, that number quickly reaches about $250,000. Waiting beyond that point doesn’t get you that much more information. 8.9 Policy Impacts In most districts in the U.S., at most public schools, 98% or more of teachers get tenure within three years and there’s no performance evaluation. Pay is set purely based on experience. Reducing class size can be quite valuable, but it’s absolutely critical to hire highly effective teachers if you’re trying to reduce class sizes. If I were to pick between having better teachers and smaller classes I would probably pick better teachers. It all depends upon the magnitudes: if you’re talking about a dramatically better teacher that’s going to have much higher value. It’s imperative that we do more to retain and attract top teachers in public schools. Build better methods of evaluating the quality of teachers: classroom ratings, student evaluations, what principals think, etc. Figuring out how you get the best teachers in our public schools can have enormous returns for the country and really improve rates of upward mobility, especially in disadvantaged schools. 8.10 Market Based-Solutions: Charter Schools Idea: leverage market forces by permitting school choice. If you’ve got a really good school, it might attract more students in equilibrium and other schools will have to compete and improve their game in order to stay in business. Charter schools: publicly funded but independent of the public school system. Voucher system: rather than paying tax dollars and then having to use that money in your local school, you get a voucher from the government and you can use that voucher at a private school or any school you want that you choose. 8.11 Do Charter Schools Work? You can’t compare outcomes at charter schools and public schools because charters, in particular, tend to be started in lower-income urban areas and disadvantaged areas (selection bias). Estimate the effects of charter schools on students’ outcomes by exploiting lotteries for admission—an experiment. Compare the people who won the lottery and the people who didn’t win the lottery and see how well they’re doing in terms of later outcomes (Josh Andres and Perog Puttock). Compare the effects of charter schools and pilot schools in Boston. Charter schools are exempt from all public school regulations. Pilot schools are like charters, but they’re covered by the Boston public school regulations and by teachers’ union contracts. The tax payment that your public school would’ve gotten gets transferred to the charter or pilot that your kid actually attends. The kids who, through pure chance, got into charter schools are doing quite a bit better than the kids who ended up going to public schools. But the kids who got into pilot schools get no gain. The flexibility that the charters have to potentially hire better teachers and structure things in different ways, is likely leading to significantly better impacts in this particular context relative to public schools. Boston charters also improve later outcomes like college attendance rates. Across the country, there are small, positive effects on test scores on average, but there’s a lot of variability across different types of charters. No excuses schools tend to have the most positive impact. They tend to be academically rigorous schools where you have extra hours. It’s not that the students have no excuses, it’s that teachers take the view that they have no excuse for any child underperforming (KIPP schools). Does market discipline in the school sector lead to the growth of better schools and improvement in performance over time? (Rick Stanusheck) The distribution of the quality of schools measured in the same value-added ways. How much did students’ test scores go up when they attend a given school? The distribution of their value-added estimates for Texas public schools is very sharply peaked around the median, meaning all Texas public schools are kind of similar to each other. In 2001, there’s a pretty broad distribution for the charter schools. The peak of it is slightly to the left of the Texas public schools. The average charter in Texas was probably a little bit worse for kids than the average public school. Those charters that were on the far left at minus one standard deviation or below were really not doing well in serving kids and are not there anymore. There’s some shift upward and reduction in variance in the quality of schools, exactly as you might expect to happen over time in any market. Three key limitations of market competition of the pure private market approach: Markets generally function poorly when quality is not well observed. You would like to measure value-added looking at longer term impacts, which takes many years. Cream skimming: private schools have an incentive to reject less qualified applicants. This can exacerbate inequality by leaving less qualified students behind because they’re not able to get into these better schools. They’re left behind in a public school that has fewer resources and perhaps weaker peers because all of the better students now got cream skimmed out into the charter school. Markets tend to work well when you can rely on people to make good choices. Families, especially low-income families, don’t necessarily choose the best charters. This can amplify inequality. It’s not obvious that you actually want to go to a strict market-based approach. The current constraints in the public school system, like regulations on teacher hiring, limit its effectiveness to some extent, but an unregulated market system, while it might improve outcomes on average, could deliver very variable outcomes across schools, for different subpopulations. Hybrid system might be best: preserves flexibility within schools while offering uniform quality and resources across schools. "],["lec9_race-ineq.html", "Chapter 9 Racial Disparities in Economic Opportunity 9.1 Introduction 9.2 Theories of Racial Disparities 9.3 An Intergenerational Perspective on Racial Disparities 9.4 Intergenerational Mobility in the U.S. 9.5 Intergenerational Mobility for Whites vs. Blacks 9.6 Steady States 9.7 Other Racial Disparities 9.8 Gender Differences in Racial Gaps 9.9 Explaining the Black-White Intergenerational Income Gap: Family Level Factors 9.10 The Role of Environmental Factors 9.11 Neighborhood Environments and the Black-White Gap 9.12 Conclusions", " Chapter 9 Racial Disparities in Economic Opportunity 9.1 Introduction Median household income in 2016: $63,000 for white Americans, $38,000 for Black Americans, $80,000 for Asians, $46,000 for Hispanic Americans, and about $40,000 for Native Americans. 9.2 Theories of Racial Disparities Theories: family-level factors, differences in parental income, wealth, education, or family structure. Black kids are much more likely to grow up in single-parent families than white kids are. The most prominent set of theories focus on structural features of the environment. William Julius Wilson: segregation of American cities by race is highly detrimental for black kids and isolation from where the good jobs are located might be very problematic for Black Americans. Differences in school quality that black and white kids experience, discrimination in the labor market, and discrimination in the criminal justice system. Cultural factors and social norms: identity, oppositional norms, aspirations, and role models. 9.3 An Intergenerational Perspective on Racial Disparities Most of the prior work on racial disparities studies disparities within a single generation. Intergenerational perspective: types of disparities that are likely to persist in the long run and isolate the exact factors that drive the persistence of these gaps. An analysis of the dynamics of income and steady states (like stochastic processes). If you have a system that’s going forward over time and there are certain dynamics in it, where is it going to converge? Where is it going to reach a resting state? 9.4 Intergenerational Mobility in the U.S. Plot the average income percentile of kids versus their parents’ income percentile. Strong upward sloping relationship. On average, kids born to richer parents end up having higher incomes themselves. Approximately linear: there’s about a 35 percentile point difference in terms of where kids born to the highest income families end up in the income distribution on average versus kids born to the lowest income families. Hypothetical: both black and white Americans faced the same intergenerational mobility curve that’s plotted below in black. That intergenerational mobility curve allows you to then make a prediction about what is going to happen in the next generation to average incomes. The average black kid is growing up in a family at the 33rd percentile. If you read off of that black line, it tells you that kids at the 33rd percentile, on average, end up at the 44.8th percentile themselves in the next generation. The average white kid is growing up in a family at the 58th percentile. White kids would end up at the 54th percentile. You have an initial gap of 25 percentiles between black and white kids in the parents’ generation. But, in the next generation, that gap shrinks down to 8.8 percentiles if both black and white kids have the same intergenerational mobility curve. You don’t have perfect persistence of income across generations: some mobility upward for kids born to low-income families and some downward mobility for kids born to high-income families, so the gap is going to shrink over time if blacks and whites have the same rates of intergenerational mobility. The gap is going to shrink in proportion with the slope of this relationship. The slope of this relationship is .35, and 8.8 is .35 times 25.2. Each generation, the gap is going to shrink to about one-third of what it originally was. If intergenerational mobility does not vary across groups then racial disparities would shrink at an exponential rate across generations (Gary Becker). 9.5 Intergenerational Mobility for Whites vs. Blacks Looking at the data, rates of intergenerational mobility are not the same for blacks and whites. We are plotting that same intergenerational mobility curve but splitting it out separately for whites in blue and blacks in the red triangles. Black kids born to families at the 25th percentile of the income distribution end up 12.6 percentiles points lower in the income distribution than white kids growing up in families with comparable incomes. There’s a tremendous gap in terms of rates of upward mobility for kids growing up in low-income families by race. The gap is almost exactly the same at the 75th percentile and even the 99th percentile. Black kids growing up in the highest income families in the U.S. still have significantly lower incomes in adulthood than white kids growing up in similarly affluent families. Black kids have lower rates of upward mobility than white kids at the bottom and much higher rates of downward mobility than white kids. New York Times data visualization using our data: income mobility for black versus white kids growing up in high-income families. Take a set of kids who grew up in families in the top fifth of the income distribution and ask, in which fifth of the income distribution did these kids themselves end up in adulthood? As a black kid, even if you grew up in a high-income family, you have a tremendously high chance of ending up in the bottom fifth or the lower middle class. If you’re a white kid who grew up in an affluent family, you tend to stay high-income. 9.6 Steady States Intergenerational mobility charts: if the slope is below one, then we’ll converge to an equilibrium where the previous generation and the next generation are going to have the same income: the steady state of the system. It’s the stable point, the equilibrium. Where the two mobility curves intersect the 45-degree line. The steady-state prediction for whites based on this intergenerational mobility curve is 54.4. The steady-state prediction for blacks is just 35.2. The steady-state gap of 19.2 percentiles is roughly equal to the current gap that we actually see between blacks and whites, 21 percentiles. Racial disparities are going to persist in a steady state. The fundamental reason that we have persistent black-white gaps in the US over hundreds of years is that there are different rates of intergenerational mobility. The gaps in income that we see in the data between blacks and whites are driven by these differences in rates of mobility rather than transitory factors that might vanish over time. In order to really have an impact on racial disparities in the long run in the US, you have to fix the mobility issue: you have to change rates of mobility for black Americans. If you only transfer incomes to black Americans in the current generation or provide temporary jobs, it will only have a transitory impact on one generation without fixing the persistence. 9.7 Other Racial Disparities The intergenerational mobility relationship for Native Americans looks almost identical to the relationship for Black Americans. The relationship for Hispanic Americans is considerably closer to what you see for whites than what you see for Blacks. Over time, Hispanics are going to catch up to whites to some extent in terms of average incomes in a way that blacks and Native Americans are not because of their persistently lower rates of intergenerational mobility. The relationship for Asian-Americans looks much flatter: for Asian-Americans kids, parental income is almost not at all predictive of outcomes. Low-income Asian kids have exceptionally high incomes. However, repeat this chart but restrict yourself to kids who are not second-generation immigrants: the Asian series looks identical to the series for whites. The Asian phenomenon is actually an immigrant phenomenon. A high-skilled person ends up taking a lower paying job in order to come to the United States. They still begin with a high level of education, and so maybe their kids end up doing quite well. Predicted steady states using the process described before are shown on the x-axis. On the y-axis, we’re showing the mean household income rank that we actually see in the data. The circles show the observed mean household income rank for the parents of the kids in our sample. What we observe in the data for the kids themselves when they’re in their mid-30s is represented by the diamonds. There is very little change for black parents and black kids. Similar to black Americans, for Native Americans and American Indians, the parents, kids, and steady state are all stacked on top of each other on the 45-degree line. Both Hispanics and blacks have relatively high rates of poverty and low incomes. But, we predict that future generations of Hispanics are going to end up with incomes much higher than that of their parents. Whites and Asians are also very close to their steady states. Asians currently have much higher incomes than whites. However, because their intergenerational mobility curve looks similar to that of whites, we think they’re going to converge to incomes that look rather similar to whites over time, especially as the immigrant share falls across generations. 9.8 Gender Differences in Racial Gaps Understanding the sources of intergenerational gaps: why is it that black kids earn less than white kids who grow up in families with comparable incomes? Cut the data by gender. Everything I showed you before was household income. We’re now going to focus on individual income (excludes spouse). How male children’s incomes in adulthood vary with their parent income: looks similar to what I showed you before with gaps of about 10 to 12 percentiles across the income distribution. Black and white women, conditional on parental income, have essentially the same incomes. Black women have slightly higher incomes, conditional on parent income, than white women do. This do not mean, on average, black women and white women in America have similar incomes. On average, because we have racial disparities in the U.S., white women tend to grow up in much higher income families than Black women. However, conditional on their parent income, if you take a black girl and a white girl each growing up in family earning the same amount per year, they end up having pretty similar incomes on average. You can’t conclude from this that black women are going to end up in a steady state with the same average incomes as white women because of marriage. Household income is going to matter in the next generation. The fact that black men have lower rates of intergenerational mobility is going to affect women’s incomes in the next generation. The black-white gap is heavily driven by what’s going on for black men relative to white men. Only 55% of black men growing up in the lowest income families are employed in a given year. There’s a roughly 20 percentage point gap in employment rates between black and white men growing up in families at the 25th percentile. Part of what is going on is about the criminal justice system and incarceration. Black women and white women have extremely similar employment rates, conditional on parental income. In fact, black women have slightly higher rates of employment than white women do. 21% of Black men growing up in the lowest income families in the US are incarcerated on a single day. This is not incarceration rates over a lifetime, but rather whether or not someone was incarcerated on the day of the 2010 census, April 1, 2010. Higher incarceration rates at the bottom of the distribution for white men, but it doesn’t rise to anywhere near the same level as it does for black men. For women, there’s some uptick in incarceration rates at the bottom, but it’s obviously not at the same scale as it is for men. Even if you look at black men who have not had contact with the criminal justice system, or before they end up incarcerated, they are on very different paths than white men and you see very big gaps. 9.9 Explaining the Black-White Intergenerational Income Gap: Family Level Factors Potentially family-level factors: things that vary across individual families. Even conditional on income, there are big differences in wealth between black and white families. Education and family structure: white kids are much more likely to be raised in two-parent families than single-parent families. Family-level factors do not account for these intergenerational gaps. Black men who grow up in two-parent families with comparable income, education, and wealth to white men still fare worse than white men do. 9.10 The Role of Environmental Factors Environmental factors (outside the family): analyze differences in black-white gaps across neighborhoods. Look at average income in adulthood for men with parents at the 25th percentile of the income distribution. These maps on the same color scale. The fact that they look completely different illustrates that the very best places in terms of upward mobility for black men have lower rates of upward mobility than the very worst places for white men. The distribution of upward mobility across areas for black men is almost non-overlapping with the distribution of upward mobility across areas for white men—like two Americas. There is essentially no place in America where you don’t have a quite significant black-white disparity in terms of rates of upward mobility. There are still significant differences in terms of rates of upward mobility within the two groups. Racial disparities absolutely persist in affluent, educated areas. 9.11 Neighborhood Environments and the Black-White Gap Black boys have lower earnings than white boys in 99% of census tracts in America, controlling for parental income. Both black and white boys have better outcomes in more expensive places with lower poverty places, better schools, etc. However, the black-white gap is actually bigger in such areas. A better neighborhood—in terms of schools, poverty rates, educational attainment, etc.—is more associated with better outcomes for white men than they are for black men. Racial disparities are actually bigger in relatively high-income neighborhoods places than in more disadvantaged areas. Now, I’m going to move on to the third point. We’re interested in identifying places where we see better outcomes for black kids and smaller racial disparities. Again, what are those kinds of places? How can you find places that deliver better outcomes for black kids while narrowing the racial disparity? What are the characteristics of places like Silver Spring, MD, which have pretty good outcomes for both black and white kids? Within the lower poverty areas where you see relatively good outcomes: a greater presence of black fathers in the area and lower levels of racial bias. Black men who grow up in census tracts where there are lots of fathers present tend to have much higher employment rates than black men who grew up in areas where there are very few fathers present. Why are there very few fathers present? It could be because of issues like incarceration or mortality rates, which are so high in this population. If you look at the same plot for white men, the presence of black fathers in an area is essentially not associated at all with the outcomes of white men in the same area. It suggests that this pattern is not just driven by some broad factor such as better schools in the places on the right than the places on the left. Black women’s employment rates are completely unrelated to the presence of black fathers in an area. There are exposure effects that are gender and race specific. White men’s outcomes are similarly associated with the presence of white fathers. However, it’s much less important because most white kids are growing up in areas where lots of white fathers are present. Places with less racial bias tend to have higher rates of upward mobility for black men. Use a Google Search Index-based measure of racial animus, where you basically look at searches for racial epithets relative to searches for other things and use that as an index for how racially biased people are in a given area. Places with more racial animus tend to have significantly worse outcomes for black men than they do for white men. None of this proves causality, but it’s consistent with the idea that racial discrimination plays a role. These differences across neighborhoods are actually reflecting causal childhood exposure effects. The moving to opportunity study and our quasi-experimental analysis of kids who moved at different ages: black boys who move to areas where we see good outcomes for black men, at a younger age in proportion to the age at which they got there, tend to have better outcomes. If a black kid moves to a place where white men are doing well, that doesn’t necessarily predict better outcomes. The black-white gap itself appears to be causally related to childhood exposure. Environment matters potentially because of differences across places in terms of mentors, resources, discrimination, and so forth. Key takeaway: black boys do well in neighborhoods with good resources, low poverty rates, and good race-specific factors that affect white kids less, like high father presence and less racial bias. There are essentially no neighborhoods in America that provide good environmental conditions for black men to thrive. What fraction of kids grow up in a neighborhood with a poverty rate below 10% and more than 50% of kids being raised in household with a father. For black kids, 66% of them are being raised in neighborhoods that have low father presence and high rates of poverty, and only 4.2% of them are being raised in a neighborhood with low poverty and high father presence. The vast majority of white kids in the U.S. are growing up in low poverty neighborhoods with high rates of father presence. Environment matters, but there are really a tiny set of places that have low levels of discrimination, good resources, high rates of father presence, and don’t have serious incarceration issues that create the conditions where black men thrive. That’s what seems to be driving these disparities. 9.12 Conclusions Mobility into and out of poverty is a central determinant of racial disparities. Black kids have much lower rates of upward mobility and greater rates of downward mobility than white kids. That is why we have persistent black-white gaps in America. Commonly proposed policies are likely to be insufficient to close the black-white gap by themselves. Changes in transfer programs, like providing bigger tax credits to certain areas or changes in minimum wages, they’re unlikely to have a persistent effect on the black-white gap unless they directly change mobility rates across generations. Within 99% of census tracts in America, even if black and white kids go to the same school and grow up in similar families, you still continue to have really substantial gaps in outcomes. Reducing racial gaps will require policies that cut within neighborhoods and improve environments for specific sub-groups, particularly black men. Mentoring programs, efforts to reduce racial bias, criminal justice reform, and achieving greater actual racial integration within schools, meaning that kids are not only going to the same school but actually interacting with each other. "],["lec10_criminal-justice.html", "Chapter 10 Improving Judicial Decisions 10.1 Motivation: Bias in Human Decision Making 10.2 Decisions to Jail vs. Release Defendants 10.3 Data Used for Empirical Analysis 10.4 Methodology 10.5 Comparing Machine Predictions to Human Predictions 10.6 Predictive Policing 10.7 Conclusion", " Chapter 10 Improving Judicial Decisions 10.1 Motivation: Bias in Human Decision Making Marianne Bertrand and Sendhil Mullainathan sent out fictitious resumes with identical credentials in response to real job ads (audit study). They searched a database of names from social security and get predictions of the probability that someone is black or white based on their name. They varied the name of the applicant on the resume to be either white-sounding (Emily Walsh), versus black-sounding (Lakisha Washington). They sent 5,000 resumes in response to 1300 real “Help Wanted” ads in newspapers in Boston and Chicago. This is a randomized trial. The resumes are identical in everything including education and work history, except for the name. So you’re seeing whether the name matters, presumably because people are making an inference about race from that name. A simple comparison of callback rates: systematically across all different groups, in Chicago, in Boston, among women, and among men, there are always significantly higher callback rates for white-sounding names than black-sounding names. The callback rate for white-sounding names is around 50% higher than for black-sounding names. There’s very clear evidence of discrimination by race, conditional on credentials in the labor market. Ben Edelman and peers use an audit study approach to analyze discrimination among Airbnb hosts. Sent out fictitious requests to book listings in response to real postings. Randomly varied the name of the applicant across the fake accounts to be white or black-sounding. They didn’t include profile pictures in these accounts so that the only information one can use to infer race comes from the names. This is a randomized trial. Airbnb hosts are more likely to respond if someone has a white-sounding name than a black-sounding name to various degrees. These degrees vary from an outright “Yes, you can rent my house.” to a, “No.” There’s a significantly higher rate of a straight “No.” for black American guests than white guests. Roughly 40% of these listings remained vacant on the date that they had proposed to visit the house. People are losing money by not accepting these invitations. On average, by rejecting black guests, Airbnb hosts are losing about $65 to $100 of revenue. This is systematic bias and bias in decision making. It could be driven by subconscious mistakes or deliberate racial discrimination. Danziger et al. analyze data on judges’ decisions to grant prisoners parole in Israel. Judges review about 20 cases on average each day in succession. The order in which these cases appear before a judge is essentially random. There are two breaks during the day, during which the judges take meals. If you happen to be the person who came up right before the judges broke for their snack, your odds of getting parole are almost zero. It then jumps up to 60% immediately after that meal break, and then it comes down again. After the lunch break, it goes up again and then comes back down. The rate changes from about 60% down to 10%: shockingly large. 10.2 Decisions to Jail vs. Release Defendants After arrest, judges decide whether to hold defendants in jail or to let them go on bail. By law, the objective is to minimize the risk of flight, or the failure to appear at the trial. Kleinberg et al. compare machine learning predictions and judges’ actual decisions on bail in terms of performance in minimizing flight risk. Kleinberg et al. have access to the same data that judges get when making these decisions, like the rap sheet. They then use data on subsequent crimes: whether or not people show up for the trial. They observe that outcome for the people who were released in order to evaluate the performance for their algorithm and compare it to judges’ actual decisions. Kleinberg et al. define crime as failing to show up at the trial. The objective is to jail those with the highest risk of committing this crime. The clear objective helps the analysis. 10.3 Data Used for Empirical Analysis You start out with your set of data (left side of this flow chart). You then break it into three sets. Middle set: the training set. That is the data that you use to actually fit your crime prediction model (80% of the data). Lock Box: before they get to the training set, they take part of the data and put it in what they call a Lock Box. That’s data that’s not touched and is used later after you’re done with the paper to evaluate whether your model is actually working as you had hoped. The big risk here comes from overfitting. If I give you lots of variables to predict something and you have a certain amount of data, you’re always going to be able to design a model that works well within that data. But, you’re not going to have any confidence in whether the model is going to predict well when faced with a new set of data, like a new set of defendants who appear. Hold-Out sample: you start with all the data, set aside the Lock Box, put 80% of what remains in the training set, and then keep 20% in what they call a Hold-Out sample, which is the sample that they use to compare the decisions that judges make with the decisions that you would get from the machine learning prediction model. You need to use a different set of data from the data that you used for training the model, otherwise, it’s going to be completely circular. If you’re using the same data to evaluate the performance of your model and estimate it, the model’s going to look like it performed very well because you used it to fit the exact same data. 5-fold Cross-Validation: within the training data set, they’ve split it into five separate groups of 20% each (Folds of the Data). They estimate the model on four of these folds and then evaluate the predictions on the fifth fold until they optimize their model. 10.4 Methodology Decision trees: split the data using a tree structure in order to form these predictions. Avoid overfitting the data, given that you have a very large number of potential predictors. You can get very good in-sample fit within a given set of data but still have very poor performance out-of-sample. Solve overfitting with cross-validation. Split the data based on the variable that is most predictive of differences in the outcome that you care about (crime rates). Imagine you have 10 different variables, such as the arrest charge, the defendant’s age, their gender, where they’re from, and their prior criminal background. Take each of these 10 variables and imagine splitting the data on the basis of that variable. Which variable creates the biggest difference between the two groups in terms of the probability that the defendant appears at the trial or doesn’t appear at the trial? Start at the first node of the decision tree by splitting on the variable that is most predictive. Repeat that process, growing the tree up to a given number of nodes. I’ve split the data into these two groups. I do the same exercise within each of the two groups. Looking in just the felony category, I determine which variable is most predictive of the probability of not showing up at the trial among the people who’ve committed felonies. At each point we’re going to make a split based on the variables we have. We keep growing this tree up to n nodes, or levels in the tree. Cross validation helps answer where do you want to stop and how many layers do you want in your tree. Use the separate validation sample to evaluate the accuracy of the predictions based on a tree of a given size n. You might build a tree with five, ten, or twenty nodes within four folds of your training data. Take the fifth fold, the data that you haven’t used in that estimation step and ask how well your tree of size n is doing at making predictions. If you have a tree that’s too deep, the predictions are actually worse past some point. Repeat with trees of different depths and choose the optimal depth. Red curve (below): shows how the prediction error changes as they make that tree bigger and bigger. Prediction error: the model spits out a prediction of whether or not the defendant is going to appear at trial, and you can compare that to whether or not the defendant appears at trial in the actual data. Initially, the prediction error is falling as they make the tree bigger. A tree depth around 35 minimizes the prediction error. Overfitting: if you go beyond that with a deeper tree, you have higher prediction errors. Green curve: if instead, you looked at prediction errors within the sample that you used for estimation, as you add more information, you start to fit the observations that you’re trying to predict in that estimation sample better and better. Prediction error is constantly decreasing. If I’m trying to predict 1000 observations, and I give you more variables, you’re going to do a better job predicting those 1000 observations. That is the wrong notion of prediction error to use when trying to figure out what will happen out of sample. Use the cross-validation approach where you estimate on part of the data and then calculate the prediction errors using a different part of the data. 10.5 Comparing Machine Predictions to Human Predictions Kleinberg et al fit the model and get a prediction for every defendant on the probability that they’re going to show up at the trial. The decision rule, if you’re just using the machine predictions, is a simple threshold, above which everyone is jailed, and below which everyone is granted parole. 3D plot: the predicted crime risk from the model is on the horizontal axis. They also show the the observed crime rate, whether or not people actually showed up. Lastly, the decision that is being made in terms of the release rate is shown on the vertical axis. The relationship between predicted crime risk and observed crime rate is on the plane at the bottom. Those shadowy dots there are a projection of the 3D plot onto the two dimensions. Predicted crime risk and observed crime rate are very closely related to each other (you only see the observed crime rate for the defendants who are released). You’ve built a good machine learning model that seems to work out of sample. The plane showing the release rate and predicted crime risk shows that judges release about 50% of the defendants whose predicted crime risk exceeds 60%. Dots near the blue arrow: we think there’s a 60% chance these people are not going to show up for trial if you release them. However, the judges’ release rate for that group is around 50%. People who have very low predicted crime risk (less than 20%), 30% of those folks are being jailed. Key takeaway: if judges were following the machine learning predictions closely, they would be very unlikely to release the guys with high predicted crime risks and very likely to release the guys that have a low predicted crime risk. Judges are releasing lots of people they probably shouldn’t be and jailing lots of people who could probably be safely let go. By swapping low and high-risk defendants, crime could be reduced by 25% with no change in jailing rates if you used the machine learning predictions instead of the judges’ current decisions. Or you could reduce the number of people that you currently hold in jail by 42%, keeping fixed the current rate of crime. Why does the machine significantly outperform judges? Limits to human cognition: decision fatigue. Judges see how someone is acting in a courtroom, which they may use to inform their decision, but which may not actually be that predictive of whether they’re going to commit a crime. Probably too extreme to say we should replace judges with these models and make decisions purely on that basis. However, there are people designing software that gives the judge a score that is the predicted risk for the defendant. Judges then can choose to deviate from that score if they want. 10.6 Predictive Policing Predict and prevent crime before it happens. An area level approach makes sense fundamentally because there tends to be a clustering of criminal activity by place and time. Plot a frequency distribution of the time between burglaries that are separated by .1 miles or less. To think about this mechanically, suppose you take a .1-mile radius in a neighborhood in which a crime occurred on January 1st, how likely was it that there was also a burglary that also occurred on January 2nd, 3rd, or 4th? How large is the gap between two burglaries, and what is the empirical distribution of those gaps? You can see here that there is a sharp spike at zero and one. When there’s a burglary on a given day, it’s very likely that there’s another burglary in that same neighborhood on that same day or the next day. There’s a type, spatial, and temporal clustering of crime. Why is that? It could be that the same set of people burglarize multiple houses or that there’s some kind of correlation in criminal activity that occurs in given places and given times. Why is this useful? This suggests that you might be able to detect where these hotspots of crime are going to occur. That potentially motivates spatial tools that are oriented around detecting crime at high frequencies in certain areas. An example of a technique like that is a database called ShotSpotter. When there are gunshots, there’s a very distinctive audio signal that you can pick out from some distance. There are police departments around the country that have set up sensitive microphones on top of buildings that will detect gunshots. You can aggregate that data and figure out when there are gunshots being fired through the signature. Again, it’s similar to using a machine learning algorithm to pick out what seems like a gunshot. That can give you high frequency data on where crime is potentially occurring. You might ask, why don’t you just wait for people to self-report a gunshot? If you look at these sorts of data, you see that a huge number of incidents of gunshots are not reported. In fact, a very small fraction is reported. Therefore, this type of information from machine learning can be potentially useful in predicting where additional crime might occur given spatial clustering. That’s one approach to predictive policing. You can think of the second approach as individual level methods, which are potentially even more powerful. We don’t just rely on where you are or aggregate data by area. Instead, we use information on individual characteristics like people’s background, social networks, and behaviors. As an example, if you see that kids who have certain profiles in school, like those who have been suspended many times or have gotten expelled, you might be able to fit a machine learning model and see which types of children have certain behavioral patterns associated with crime down the road. That can give you predictions of the types of subgroups where you might be more worried and to which you might allocate resources to try to prevent crime. The methods there are quite similar conceptually to the methods discussed in the other application, so I’m not going to show those examples here. 10.7 Conclusion Even if you’re able to build them, it’s not always completely clear when to implement them. There’s a serious ethical debate about the use of predictive analytics in settings like this. There are two prominent views, both of which are reasonable depending on your point of view. One view questions whether a person should be treated differently simply because they share attributes with others who have higher risks of crime. An extreme version of this is that there are sharp differences in terms of the rate of incarceration by race. In particular, black men are incarcerated at much higher rates than other groups. As you can imagine race is potentially going to be a powerful predictor of crime and incarceration in models like this. Even if you were to explicitly exclude race from your prediction model, typically variables that are correlated with race, like zip code, will start to become predictors. Then, effectively what you’re doing is treating people differently on the basis of race just because they happen to share a race with somebody else. You might be subjecting them to more policing or a different set of interventions, which of course is at some level unfair given that they haven’t done anything yet. Thus, I think that’s a serious ethical concern with using these sorts of prediction tools. On the flip side, I think you can also ask if police, judges, and decision-makers should discard information that could make society fairer and potentially more just than it is now on average. You could potentially target your resources much better with the use of these prediction tools as we saw in the judge example. It’s true that you’re treating people differently on the basis of their attributes, but if you think about it from the perspective of the ends not the fairness of the process, maybe the outcomes that you achieved are better overall. This is not completely clear. There’s an interesting set of literature developing on fairness and algorithms that tries to get at these ethical issues, think rigorously about which algorithms are fair or less fair, and precisely quantify it. "],["getting-started.html", "Chapter 11 Very Brief Intro to Data in R 11.1 Why R again? 11.2 Key concepts before we start: 11.3 Let’s open R! 11.4 How do I code in R? 11.5 In Class Exercise 11.6 What are R packages? 11.7 Hands-on exercise! 11.8 Conclusion", " Chapter 11 Very Brief Intro to Data in R 11.1 Why R again? 11.1.1 Why are we learning R? I wanted to learn about economics of public and social issues… This class is about social issues in economics. But what are those social issues? Economic mobility and inequality Effects of education on wages and inequality Criminal justice system outcomes Pollution and climate change and so on… How can we know if going to school increases wages? Or if economic mobility is low or high? We need to analyze data! We can do that analysis by hand… but that would be very time consuming. Or we can use a super calculator with amazing capabilities to explore data, maps, etc: enter R and R Studio. This course is not about teaching you all about R! We will only cover the very basics so you can jump into doing some empirical analysis by yourself. You will be able to expand much more on the tools briefly described here in other, more advanced, courses in the Economics sequence. Important! For the vast majority of exercises in our course, I will give you all the code you will have to run. So, it’s not like you need to write anything from scratch! I do want you to get a basic understanding of what we will be doing when we run those lines. 11.1.2 Ok, but why R? R is free and open source! R has a vibrant online community! R is very flexible and powerful — adaptable to nearly any task ( e.g., correlations, econometrics, spatial data analysis, machine learning, web scraping, data cleaning, website building, teaching.) Employers like R over alternatives 11.1.3 Added benefits of learning R Employers hire people that knows R. Again, we will only cover the essentials, but maybe you want to keep this in mind as you go along with your studies. 11.2 Key concepts before we start: Before we can start exploring data in R, there are some key concepts to understand first: What are R, Posit Cloud and RStudio? How do I code in R? What are R packages? We’ll introduce these concepts in upcoming Sections 11.2.1-11.6. Then we’ll introduce our first data set: data on the economic mobility for all neighborhoods across the US in the atlas dataset. 11.2.1 What are R and RStudio? For much of this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest: R is like a car’s engine. RStudio is like a car’s dashboard. R: Engine RStudio: Dashboard More precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well. 11.2.2 R and RStudio: In your computer or in the cloud? The benefit of Posit Cloud To use R and RStudio, you can: install it in your computer (see the book) or … run it on someone else’s computer (the cloud!). We will do that, so you don’t have to worry about installations. In this course I won’t require you to install R and RStudio in your own computer. Instead, we will use the cloud! In particular, the website that we will use to manage RStudio is called Posit Cloud. You should have received an invitation for the course in your email. If you would still like to install R and RStudio in your own computer, please follow the instructions in Section 1.1.1 in this link. Of course, you are encouraged to experiment in your own machine as well. Notice, however, that you should carry out the assignments and projects in Posit Cloud. Important! Homework and projects should be carried in Posit Cloud (not your personal computer). 11.3 Let’s open R! 11.3.1 RStudio in Posit cloud Let’s jump in! You should have received a link for you to access RStudio in the cloud through Posit Cloud. There is a monthly fee to use this service of $5. We will be using the service for the next 4 months, so you should be paying a total of $20 across these 4 months. Recall also that there is no required textbook for this class. Once the Fall semester finishes, it is your responsibility to cancel your subscription. Once more: remember that all the code is running on someone’s computer. In this case, it is running on a computer owned by the folks at RStudio. Receive link with invitation You should have received an email with a link inviting you to join Posit Cloud. Once you click on the link, you should land in a page like this: Then you should fill out the information for your account. Please use your utsa email account (the account you should have received the invitation to). Once you click on Sign Up, it will show this: Then you need to go back to your email and verify it. Then log back in again! Inside Posit Cloud select ECO3523-Fall23 Once you have logged back in, you should land in the main page, which should look like this: Click on the left where it says ECO3523-Fall23. That is where you will see all the material and projects for this class. Select the appropriate project you want to work on Once you are in the ECO3523-Fall23 tab, you should see a list of individual projects we will work on throughout the semester. It should look something like this: For our first class on R you will select the link that says introRClass. That is it! Now we are ready to get to work! 11.3.2 Using R via RStudio in Posit Cloud Recall our car analogy from above. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you open Posit Cloud and follow the previous instructions, you should see a panel like the following: Note the three panes, which are three panels dividing the screen: The Console pane, the Files pane, and the Environment pane. Over the course of this chapter, you’ll come to learn what purpose each of these panes serve. 11.4 How do I code in R? Now that you’re set up with Posit Cloud, you are probably asking yourself “OK. Now how do I use R?” The first thing to note as that unlike other statistical software programs like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code. In other words, you have to code/program in R. Note that we’ll use the terms “coding” and “programming” interchangeably in this book. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this course is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 11.4.1 Basic programming concepts and terminology We now introduce some basic programming concepts and terminology. Instead of asking you to learn all these concepts and terminology right now, we’ll guide you so that you’ll “learn by doing.” Note that in this book we will always use a different font to distinguish regular text from computer_code. The best way to master these topics is, in our opinions, “learning by doing” and lots of repetition. Basics: Console: Where you enter in commands. Running code: The act of telling R to perform an action by giving it commands in the console. Objects: Where values are saved in R. In order to do useful and interesting things in R, we will want to assign a name to an object. For example we could do the following assignments: x &lt;- 44 - 20 and three &lt;- 3. This would allow us to run x + three which would return 27. Data types: Integers, doubles/numerics, logicals, and characters. Vectors: A series of values. These are created using the c() function, where c() stands for “combine” or “concatenate.” For example: c(6, 11, 13, 31, 90, 92). Factors: Categorical data are represented in R as factors. Data frames: Data frames are like rectangular spreadsheets: they are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. We’ll cover data frames later in Section 11.7.1. Conditionals: Testing for equality in R using == (and not = which is typically used for assignment). Ex: 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). Logical operators: &amp; representing “and” as well as | representing “or.” Ex: (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE. Functions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a function’s arguments or use the function’s default values. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn’t be very useful, especially for novices. Rather, we feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. I’m confident you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more.1 11.5 In Class Exercise Try running the following commands in the console. What do you see for each one? 2*3 2*pi log(10) exp(2) sqrt(25) 3==3 3==4 3&lt;=4 3!=4 x &lt;- c(1,3,2,5)# this is called a &#39;vector&#39; x #what do you see? x &lt;- c(1,6,2) x #now what do you see? y &lt;- c(1,4,3) # USE ARROW! length(x) length(y) x+y #write this write this again 11.5.1 Errors, warnings, and messages Noticed the last thing that appeared in the console when you wrote write this again? It had scary red letters. It is an example of something that intimidates new R and RStudio users: how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad. R will show red text in the console pane in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with “Error in…” and try to explain what went wrong. Generally when there’s an error, the code will not run. For example, we’ll see in Subsection 11.6.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first. Warnings: When the red text is a warning, it will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter 12 if you create a scatterplot based on a dataset where one of the values is missing, you will see this warning: Warning: Removed 1 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining values, but it is warning you that one of the points isn’t there. Messages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load R packages in the upcoming Subsection 11.6.2 or when you read data saved in spreadsheet files with the read_csv() function as you’ll see in Chapter ??. These are helpful diagnostic messages and they don’t stop your code from working. Additionally, you’ll see these messages when you install packages too using install.packages(). Important! When you see red text in the console, don’t panic! Just check out what it could be. Remember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. Rather: If the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong! If the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine. 11.5.2 Tips on learning to code Learning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. Such frustrations are very common and it is very normal to feel discouraged as you learn. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone are “smart,” but really people spent a lot of time and energy designing them to appear “smart.” Rather you have to tell a computer everything it needs to do. Furthermore the instructions you give your computer can’t have any mistakes in them, nor can they be ambiguous in any way. Take the “copy, paste, and tweak” approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy, paste, and tweak approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. Don’t be afraid to play around! The best way to learn to code is by doing: Rather than learning to code for its own sake, we feel that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in. Practice is key: Just as the only method to improving your foreign language skills is through practice, practice, and practice; so also the only method to improving your coding is through practice, practice, and practice. Don’t worry however; we’ll give you plenty of opportunities to do so! 11.6 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a world-wide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package for data visualization which we will cover later (or you can check here) or the dplyr package for data wrangling (again, we will cover later, but check this if you want to know more). A good analogy for R packages is they are like apps you can download onto a mobile phone: R: A new phone R Packages: Apps you can download So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play. Let’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a recent photo you have taken on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set. You might do this again in the future any time there is an update to the app. Open the app: After you’ve installed Instagram, you need to open the app. Once Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version. “Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio. Let’s now show you how to perform these two steps for the ggplot2 package for data visualization. 11.6.1 Package installation For the most part, in Posit Cloud, I will pre-install the packages you are going to need to use. But just in case you also want to work on your own machine, or install your own packages, here I explain that a bit more. There are two ways to install an R package. For example, to install the ggplot2 package: Easy way: In the Files pane of RStudio/Posit Cloud: Click on the “Packages” tab Click on “Install” Type the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2 Click “Install” Slightly harder way: An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the Console pane of RStudio and hitting enter. Note you must include the quotation marks. Much like an app on your phone, you only have to install a package once. However, if you want to update an already installed package to a newer verions, you need to re-install it by repeating the above steps. Learning check (LC2.1) Repeat the above installing steps, but for the dplyr, and knitr packages. This will install the earlier mentioned dplyr package, and the knitr package for writing reports in R. 11.6.2 Package loading Recall that after you’ve installed a package, you need to “load” it, in other words open it. We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the Console pane. What do we mean by “run the following code”? Either type or copy &amp; paste the following code into the Console pane and then hit the enter key. library(ggplot2) If after running the above code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If however, you get a red “error message” that reads… Error in library(ggplot2) : there is no package called ‘ggplot2’ … it means that you didn’t successfully install it. In that case, go back to the previous subsection “Package installation” and install it. Learning check (LC2.2) “Load” the dplyr, and knitr packages as well by repeating the above steps. 11.6.3 Package use One extremely common mistake new R users make when wanting to use particular packages is that they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to: Error: could not find function R is telling you that you are trying to use a function in a package that has not yet been “loaded.” Almost all new users forget do this when starting out, and it is a little annoying to get used to. However, you’ll remember with pratice. 11.7 Hands-on exercise! 11.7.1 Explore your first dataset: economic mobility in the US Let’s put everything we’ve learned so far into practice and start exploring some real data! These “spreadsheet”-type datasets are called data frames in R; we will focus on working with data saved as data frames throughout this course. Step 1: Load all the packages needed for this exercise (assuming you’ve already installed them). library(dplyr) library(tibble) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_fall2023/master/data/atlas.rds&quot;))) atlas&lt;-tibble(atlas) 11.7.2 Economic mobility data The Opportunity Atlas is a freely available interactive mapping tool that traces the roots of outcomes such as poverty and incarceration back to the neighborhoods in which children grew up. The atlas dataset we loaded has the underlying data to describe equality of opportunity across the 73,278 different neighborhoods in the United States. Let’s unpack these data a bit more! 11.7.3 atlas data frame We will begin by exploring the atlas data frame we just loaded to get an idea of its structure. Run the following code in your console (either by typing it or cutting &amp; pasting it): it loads the atlas dataset into your Console. Note depending on the size of your monitor, the output may vary slightly. atlas ## # A tibble: 73,278 × 62 ## tract county state cz czname hhinc_mean2000 mean_commutetime2000 frac_coll_plus2010 frac_coll_plus2000 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20100 1 1 11101 Montgomery 68639. 26.2 0.254 0.156 ## 2 20200 1 1 11101 Montgomery 57243. 24.8 0.267 0.147 ## 3 20300 1 1 11101 Montgomery 75648. 25.3 0.164 0.224 ## 4 20400 1 1 11101 Montgomery 74852. 23.0 0.253 0.230 ## 5 20500 1 1 11101 Montgomery 96175. 26.2 0.375 0.321 ## 6 20600 1 1 11101 Montgomery 68096. 21.6 0.239 0.161 ## 7 20700 1 1 11101 Montgomery 65182. 23.2 0.0691 0.117 ## 8 20801 1 1 11101 Montgomery 76874. 30.3 0.283 0.188 ## 9 20802 1 1 11101 Montgomery 77310. 30.7 0.195 0.189 ## 10 20900 1 1 11101 Montgomery 66234. 36.4 0.128 0.0893 ## # ℹ 73,268 more rows ## # ℹ 53 more variables: foreign_share2010 &lt;dbl&gt;, med_hhinc2016 &lt;dbl&gt;, med_hhinc1990 &lt;dbl&gt;, popdensity2000 &lt;dbl&gt;, ## # poor_share2010 &lt;dbl&gt;, poor_share2000 &lt;dbl&gt;, poor_share1990 &lt;dbl&gt;, share_black2010 &lt;dbl&gt;, share_hisp2010 &lt;dbl&gt;, ## # share_asian2010 &lt;dbl&gt;, share_black2000 &lt;dbl&gt;, share_white2000 &lt;dbl&gt;, share_hisp2000 &lt;dbl&gt;, share_asian2000 &lt;dbl&gt;, ## # gsmn_math_g3_2013 &lt;dbl&gt;, rent_twobed2015 &lt;dbl&gt;, singleparent_share2010 &lt;dbl&gt;, singleparent_share1990 &lt;dbl&gt;, ## # singleparent_share2000 &lt;dbl&gt;, traveltime15_2010 &lt;dbl&gt;, emp2000 &lt;dbl&gt;, mail_return_rate2010 &lt;dbl&gt;, ## # ln_wage_growth_hs_grad &lt;dbl&gt;, jobs_total_5mi_2015 &lt;dbl&gt;, jobs_highpay_5mi_2015 &lt;dbl&gt;, nonwhite_share2010 &lt;dbl&gt;, … Let’s unpack this output: A tibble: 73,278 x 62: A tibble is a kind of data frame used in R. This particular data frame has 73,278 rows (one for each neighborhood) 62 columns corresponding to 62 variables describing each observation (e.g. neighborhood in this case) tract county state cz czname hhinc_mean2000 mean_commutetime2000 ... are different columns, in other words variables, of this data frame. We then have the first 10 rows of observations corresponding to 10 neighborhoods. ... with 73,268 more rows, and 52 more variables: indicating to us that 73,268 more rows of data and 52 more variables could not fit in this screen. Unfortunately, this output does not allow us to explore the data very well. Let’s look at different tools to explore data frames. 11.7.4 Exploring data frames Among the many ways of getting a feel for the data contained in a data frame such as atlas, we present three functions that take as their “argument”, in other words their input, the data frame in question. We also include a fourth method for exploring one particular column of a data frame: Using the View() function built for use in RStudio. We will use this the most. Using the glimpse() function, which is included in the dplyr package. Using the $ operator to view a single variable in a data frame. 1. View(): Run View(atlas) in your Console in RStudio, either by typing it or cutting &amp; pasting it into the Console pane, and explore this data frame in the resulting pop-up viewer. You should get into the habit of always Viewing any data frames that come your way. Note the capital “V” in View. R is case-sensitive so you’ll receive an error is you run view(atlas) instead of View(atlas). Learning check (LC2.3) What does any ONE row in this atlas dataset refer to? A. Data on an neighborhood B. Data on a state C. Data on an person D. Data on multiple neighborhood By running View(atlas), we see the different variables listed in the columns and we see that there are different types of variables. Some of the variables like poor_share2010, hhinc_mean2000, and share_hisp2010 are what we will call quantitative variables. These variables are numerical in nature. Other variables, like tract are categorical: they are just names (even if they have numbers). For example tract represents a Tract FIPS Code, that is, a 6-digit code assigned by the census folks to each neighborhood in 2010. Note that if you look in the leftmost column of the View(atlas) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row corresponds to. 2. glimpse(): The second way to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after you’ve loaded the dplyr package. This function provides us with an alternative method for exploring a data frame: glimpse(atlas) ## Rows: 73,278 ## Columns: 62 ## $ tract &lt;dbl&gt; 20100, 20200, 20300, 20400, 20500, 20600, 20700, 20801, 20802, 20900, 21000, 21100, 10… ## $ county &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,… ## $ state &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ cz &lt;dbl&gt; 11101, 11101, 11101, 11101, 11101, 11101, 11101, 11101, 11101, 11101, 11101, 11101, 11… ## $ czname &lt;chr&gt; &quot;Montgomery&quot;, &quot;Montgomery&quot;, &quot;Montgomery&quot;, &quot;Montgomery&quot;, &quot;Montgomery&quot;, &quot;Montgomery&quot;, &quot;M… ## $ hhinc_mean2000 &lt;dbl&gt; 68639, 57243, 75648, 74852, 96175, 68096, 65182, 76874, 77310, 66234, 58866, 52435, 58… ## $ mean_commutetime2000 &lt;dbl&gt; 26.2, 24.8, 25.3, 23.0, 26.2, 21.6, 23.2, 30.3, 30.7, 36.4, 41.7, 28.5, 40.2, 31.8, 30… ## $ frac_coll_plus2010 &lt;dbl&gt; 0.2544, 0.2672, 0.1642, 0.2527, 0.3751, 0.2394, 0.0691, 0.2826, 0.1949, 0.1278, 0.1136… ## $ frac_coll_plus2000 &lt;dbl&gt; 0.1565, 0.1469, 0.2244, 0.2305, 0.3212, 0.1607, 0.1169, 0.1882, 0.1895, 0.0893, 0.0829… ## $ foreign_share2010 &lt;dbl&gt; 0.00995, 0.01634, 0.02710, 0.01508, 0.04649, 0.02499, 0.01989, 0.00245, 0.00691, 0.009… ## $ med_hhinc2016 &lt;dbl&gt; 66000, 41107, 51250, 52704, 52463, 63750, 45234, 74603, 61242, 44591, 49567, 40801, 45… ## $ med_hhinc1990 &lt;dbl&gt; 27375, 19000, 29419, 37891, 41516, 29000, 26895, 30957, 31137, 18430, 17188, 17191, 19… ## $ popdensity2000 &lt;dbl&gt; 195.72, 566.38, 624.20, 713.80, 529.93, 408.37, 129.42, 17.49, 38.19, 15.81, 6.92, 6.2… ## $ poor_share2010 &lt;dbl&gt; 0.1050, 0.1476, 0.0804, 0.0632, 0.0596, 0.1052, 0.1663, 0.0797, 0.1161, 0.0991, 0.2023… ## $ poor_share2000 &lt;dbl&gt; 0.1268, 0.2271, 0.0766, 0.0455, 0.0368, 0.1522, 0.1100, 0.0877, 0.0844, 0.1384, 0.1759… ## $ poor_share1990 &lt;dbl&gt; 0.0989, 0.1983, 0.1140, 0.0679, 0.0547, 0.1781, 0.1773, 0.1081, 0.1015, 0.2456, 0.2932… ## $ share_black2010 &lt;dbl&gt; 0.11925, 0.56498, 0.19804, 0.04674, 0.13970, 0.21156, 0.15635, 0.09510, 0.13608, 0.122… ## $ share_hisp2010 &lt;dbl&gt; 0.02301, 0.03456, 0.02579, 0.01938, 0.03297, 0.04798, 0.03390, 0.01850, 0.01524, 0.016… ## $ share_asian2010 &lt;dbl&gt; 0.004707, 0.002304, 0.004744, 0.003648, 0.026032, 0.001636, 0.003459, 0.004544, 0.0051… ## $ share_black2000 &lt;dbl&gt; 0.07548, 0.62209, 0.14915, 0.02590, 0.06010, 0.16903, 0.09690, 0.13313, 0.13134, 0.137… ## $ share_white2000 &lt;dbl&gt; 0.897, 0.355, 0.820, 0.938, 0.897, 0.799, 0.868, 0.840, 0.842, 0.825, 0.710, 0.395, 0.… ## $ share_hisp2000 &lt;dbl&gt; 0.00625, 0.00846, 0.01647, 0.02217, 0.01573, 0.01954, 0.01759, 0.01049, 0.01005, 0.017… ## $ share_asian2000 &lt;dbl&gt; 0.003644, 0.003171, 0.003893, 0.007288, 0.010596, 0.001480, 0.003793, 0.002242, 0.0022… ## $ gsmn_math_g3_2013 &lt;dbl&gt; 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.79, 2.79, 2.… ## $ rent_twobed2015 &lt;dbl&gt; NA, 907, 583, 713, 923, 765, 645, 532, 671, 710, NA, NA, NA, 619, NA, 880, 636, 523, 9… ## $ singleparent_share2010 &lt;dbl&gt; 0.1139, 0.4885, 0.2281, 0.2275, 0.2597, 0.3164, 0.5796, 0.1630, 0.2583, 0.2648, 0.2953… ## $ singleparent_share1990 &lt;dbl&gt; 0.1812, 0.3525, 0.1259, 0.1268, 0.0744, 0.2380, 0.2438, 0.1456, 0.1373, 0.1893, 0.2239… ## $ singleparent_share2000 &lt;dbl&gt; 0.251, 0.393, 0.245, 0.191, 0.168, 0.289, 0.393, 0.204, 0.200, 0.238, 0.262, 0.399, 0.… ## $ traveltime15_2010 &lt;dbl&gt; 0.2730, 0.1520, 0.2055, 0.3507, 0.2505, 0.3416, 0.1270, 0.2324, 0.1796, 0.0618, 0.1025… ## $ emp2000 &lt;dbl&gt; 0.567, 0.493, 0.579, 0.597, 0.661, 0.643, 0.670, 0.645, 0.645, 0.547, 0.512, 0.549, 0.… ## $ mail_return_rate2010 &lt;dbl&gt; 83.5, 81.3, 79.5, 83.5, 77.3, 82.8, 83.2, 85.5, 85.4, 83.0, 86.7, 81.2, 73.6, 78.0, 81… ## $ ln_wage_growth_hs_grad &lt;dbl&gt; 0.03823, 0.08931, -0.17774, -0.07231, -0.09614, -0.04856, -0.18548, NA, NA, 0.03584, -… ## $ jobs_total_5mi_2015 &lt;dbl&gt; 10109, 9948, 10387, 12933, 12933, 9193, 11578, 1567, 2615, 279, 212, 151, 133, 1199, 6… ## $ jobs_highpay_5mi_2015 &lt;dbl&gt; 3396, 3328, 3230, 3635, 3635, 3052, 3389, 989, 1176, 106, 126, 26, 47, 540, 189, 518, … ## $ nonwhite_share2010 &lt;dbl&gt; 0.1627, 0.6111, 0.2476, 0.0812, 0.2162, 0.2715, 0.2065, 0.1366, 0.1712, 0.1567, 0.2370… ## $ popdensity2010 &lt;dbl&gt; 504.8, 1682.2, 1633.4, 1780.0, 2446.3, 1184.4, 334.1, 64.2, 141.6, 50.2, 19.4, 18.0, 1… ## $ ann_avg_job_growth_2004_2013 &lt;dbl&gt; -0.00677, -0.00425, 0.01422, -0.01984, 0.01863, -0.05159, 0.00552, -0.03930, 0.00816, … ## $ job_density_2013 &lt;dbl&gt; 92.133, 971.318, 340.920, 207.386, 800.273, 336.778, 141.123, 3.501, 14.049, 2.566, 1.… ## $ kfr_natam_p25 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ kfr_natam_p75 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ kfr_natam_p100 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ kfr_asian_p25 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ kfr_asian_p75 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ kfr_asian_p100 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ kfr_black_p25 &lt;dbl&gt; 26819, 18138, 20515, 12883, 26594, 19108, 17269, 10235, 23543, 24216, 25893, 22547, 22… ## $ kfr_black_p75 &lt;dbl&gt; 45926, 33842, 34133, 40334, 42575, 26062, 32101, 19439, 42654, 34438, 44593, 30808, 45… ## $ kfr_black_p100 &lt;dbl&gt; 84690, 60512, 56516, 105250, 72565, 35737, 56359, 32350, 80709, 50392, 82002, 42879, 9… ## $ kfr_hisp_p25 &lt;dbl&gt; NA, NA, NA, 26363, 17234, NA, NA, NA, 32946, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ kfr_hisp_p75 &lt;dbl&gt; NA, NA, NA, 67532, 44642, NA, NA, NA, 65539, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ kfr_hisp_p100 &lt;dbl&gt; NA, NA, NA, NA, 93976, NA, NA, NA, 147274, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ kfr_pooled_p25 &lt;dbl&gt; 27621, 22303, 28215, 33331, 34633, 23583, 27612, 22428, 27915, 26084, 27221, 24694, 29… ## $ kfr_pooled_p75 &lt;dbl&gt; 51531, 46650, 50754, 52337, 57007, 47735, 42969, 51959, 51986, 54122, 61254, 42912, 57… ## $ kfr_pooled_p100 &lt;dbl&gt; 78922, 74225, 76055, 72586, 81792, 75188, 58086, 88557, 79622, 88273, 107923, 61531, 9… ## $ kfr_white_p25 &lt;dbl&gt; 30328, 42189, 33670, 34181, 39540, 27835, 32270, 29066, 31379, 29027, 30816, 33295, 35… ## $ kfr_white_p75 &lt;dbl&gt; 50820, 54239, 51579, 52848, 58699, 51198, 44085, 54860, 53174, 57420, 64735, 50792, 60… ## $ kfr_white_p100 &lt;dbl&gt; 75126, 66646, 71991, 74330, 80415, 80144, 56134, 88027, 79508, 95584, 117001, 70607, 9… ## $ count_pooled &lt;dbl&gt; 519, 530, 960, 1123, 1867, 994, 772, 632, 2114, 1373, 760, 851, 1090, 705, 1716, 1265,… ## $ count_white &lt;dbl&gt; 457, 173, 774, 1033, 1626, 756, 630, 523, 1756, 1125, 478, 289, 724, 591, 1239, 1138, … ## $ count_black &lt;dbl&gt; 42, 336, 151, 40, 137, 198, 111, 89, 290, 211, 262, 552, 319, 83, 433, 88, 94, 787, 85… ## $ count_asian &lt;dbl&gt; 3, 1, 1, 6, 13, 2, 1, 1, 5, 0, 0, 0, 0, 3, 1, 0, 8, 3, 10, 9, 5, 17, 10, 2, 3, 3, 2, 4… ## $ count_hisp &lt;dbl&gt; 4, 5, 21, 37, 39, 19, 14, 9, 29, 17, 7, 3, 18, 8, 21, 14, 11, 18, 23, 31, 24, 52, 20, … ## $ count_natam &lt;dbl&gt; 6, 1, 2, 0, 8, 2, 9, 1, 5, 4, 8, 3, 20, 8, 9, 3, 6, 3, 3, 5, 1, 4, 1, 9, 11, 12, 6, 11… We see that glimpse() will give you the first few entries of each variable in a row after the variable. In addition, the data type of the variable is given immediately after each variable’s name inside &lt; &gt;. Here, int and dbl refer to “integer” and “double”, which are computer coding terminology for quantitative/numerical variables. In contrast, chr refers to “character”, which is computer terminology for text data. Text data, such as the czname (the name of the metro area), are categorical variables. 3. $ operator Lastly, the $ operator allows us to explore a single variable within a data frame. For example, run the following in your console atlas$tract We used the $ operator to extract only the tract variable and return it as a vector of length 73,278. We will only be occasionally exploring data frames using this operator, instead favoring the View() and glimpse() functions. 11.8 Conclusion We’ve given you what we feel are the most essential concepts to know before you can start exploring data in R. There is much more to explore in R but this is a great place to get started! 11.8.1 Additional resources If you want to dive more and feel you could benefit from a more detailed introduction, check this short book: Getting used to R, RStudio, and R Markdown short book. It has screencast recordings that you can follow along and pause as you learn. Furthermore, there is an introduction to R Markdown, a tool used for reproducible research in R. We will see more about that in the next class. If you truly insist on getting more information, you can check this link explaining some of the basics: https://rstudio-education.github.io/hopr/basics.html ;but again, this is not required or expected.↩︎ "],["viz.html", "Chapter 12 Data Visualization 12.1 The Grammar of Graphics 12.2 Three Important Graphs - 12.3 Scatterplots 12.4 Histograms 12.5 Barplots 12.6 Conclusion", " Chapter 12 Data Visualization This chapter is based in big part on the chapter on visualization by the folks at Northwestern. Please be check that link if you want to dive into how to make even cooler graphs than the ones we will cover here. We will learn basic tools to visualize our data. By visualizing our data, we gain valuable insights that we couldn’t initially see from just looking at the raw data in spreadsheet form. We will use the ggplot2 package as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as The Grammar of Graphics (wilkinson2005?). At the most basic level, graphics/plots/charts (we use these terms interchangeably in this guide) provide a nice way for us to get a sense for how quantitative variables compare in terms of their center (where the values tend to be located) and their spread (how they vary around the center). Graphics should be designed to emphasize the findings and insight you want your audience to understand. This does however require a balancing act. On the one hand, you want to highlight as many meaningful relationships and interesting findings as possible; on the other you don’t want to include so many as to overwhelm your audience. As we will see, plots/graphics also help us to identify patterns in our data. We will see that a common extension of these ideas is to compare the distribution of one quantitative variable (i.e., what the spread of a variable looks like or how the variable is distributed in terms of its values) as we go across the levels of a different categorical variable. Needed data and packages Let’s load all the packages and data needed for this chapter (this assumes you’ve already installed them). Read Section 11.6 for information on how to install and load R packages. As before, we will load the atlas dataset as well. We will also plot some of the information in the package gapminder for some of our examples. atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) library(ggplot2) library(dplyr) library(gapminder) 12.1 The Grammar of Graphics We begin with a discussion of a theoretical framework for data visualization known as “The Grammar of Graphics,” which serves as the foundation for the ggplot2 package. Think of how we construct sentences in English to form sentences by combining different elements, like nouns, verbs, particles, subjects, objects, etc. However, we can’t just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, “The Grammar of Graphics” define a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (wilkinson2005?) and has been implemented in a variety of data visualization software including R. 12.1.1 Components of the Grammar In short, the grammar tells us that: A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. Specifically, we can break a graphic into the following three essential components: data: the data set composed of variables that we map. geom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars. aes: aesthetic attributes of the geometric object. For example, x-position, y-position, color, shape, and size. Each assigned aesthetic attribute can be mapped to a variable in our data set. You might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example unrelated to our mobility data, but worry not! We will return to the atlas data. First… 12.1.2 Gapminder data In February 2006, a statistician named Hans Rosling gave a TED talk titled “The best stats you’ve ever seen” where he presented global economic, health, and development data from the website gapminder.org. For example, for the 142 countries included from 2007, let’s consider only the first 6 countries when listed alphabetically in Table 12.1. Table 12.1: Table 12.2: Gapminder 2007 Data: First 6 of 142 countries Country Continent Life Expectancy Population GDP per Capita Afghanistan Asia 43.8 31889923 975 Albania Europe 76.4 3600523 5937 Algeria Africa 72.3 33333216 6223 Angola Africa 42.7 12420476 4797 Argentina Americas 75.3 40301927 12779 Australia Oceania 81.2 20434176 34435 Each row in this table corresponds to a country in 2007. For each row, we have 5 columns: Country: Name of country. Continent: Which of the five continents the country is part of. (Note that “Americas” includes countries in both North and South America and that Antarctica is excluded.) Life Expectancy: Life expectancy in years. Population: Number of people living in the country. GDP per Capita: Gross domestic product (in US dollars). Now consider Figure 12.1, which plots this data for all 142 countries in the data. Figure 12.1: Life Expectancy over GDP per Capita in 2007 Let’s view this plot through the grammar of graphics: The data variable GDP per Capita gets mapped to the x-position aesthetic of the points. The data variable Life Expectancy gets mapped to the y-position aesthetic of the points. The data variable Population gets mapped to the size aesthetic of the points. The data variable Continent gets mapped to the color aesthetic of the points. We’ll see shortly that data corresponds to the particular data frame where our data is saved and a “data variable” corresponds to a particular column in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. Other plots involve lines while others involve bars. Let’s summarize the three essential components of the Grammar in Table 12.3. Table 12.3: Table 12.4: Summary of Grammar of Graphics for this plot data variable aes geom GDP per Capita x point Life Expectancy y point Population size point Continent color point 12.1.3 Other components There are other components of the Grammar of Graphics we can control as well. As you start to delve deeper into the Grammar of Graphics, you’ll start to encounter these topics more frequently. In this book however, we’ll keep things simple and only work with the two additional components listed below: faceting breaks up a plot into small multiples corresponding to the levels of another variable (Section ??) position adjustments for barplots (Section 12.5) Other more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science (rds2016?). Generally speaking, the Grammar of Graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them. 12.1.4 ggplot2 package In this book, we will be using the ggplot2 package for data visualization, which is an implementation of the Grammar of Graphics for R (R-ggplot2?). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the Grammar of Graphics are specified in the ggplot() function included in the ggplot2 package, which expects at a minimum as arguments (i.e. inputs): The data frame where the variables exist: the data argument. The mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved. After we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include layers specifying the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section ??). Let’s now put the theory of the Grammar of Graphics into practice. 12.2 Three Important Graphs - In order to keep things simple, we will only focus on 3 types of graphics in this section, each with a commonly given name. scatterplots histograms barplots We will discuss some variations of these plots, but with this basic repertoire of graphics in your toolbox you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables and while others are only appropriate for quantitative variables. You’ll want to quiz yourself often as we go along on which plot makes sense a given a particular problem or data set. 12.3 Scatterplots The simplest of the figrue we will cover are scatterplots, also called bivariate plots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the Grammar of Graphics. Specifically, we will visualize the relationship across neighborhoods between the following two numerical variables in the atlas data frame: kfr_pooled_p25: upward mobility for children with parents on the percentile 25 on the horizontal “y” axis med_hhinc2016: median household income in 2016 on the vertical “x” axis 12.3.1 Scatterplots via geom_point Let’s now go over the code that will create the desired scatterplot, keeping in mind our discussion on the Grammar of Graphics in Section 12.1. We’ll be using the ggplot() function included in the ggplot2 package. ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) + geom_point() Let’s break this down piece-by-piece: Within the ggplot() function, we specify two of the components of the Grammar of Graphics as arguments (i.e. inputs): The data frame to be atlas by setting data = atlas. The aesthetic mapping by setting aes(x = med_hhinc2016, y = kfr_pooled_p25). Specifically: the variable med_hhinc2016 maps to the x position aesthetic the variable kfr_pooled_p25 maps to the y position aesthetic We add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object. In this case the geometric object are points, set by specifying geom_point(). After running the above code, you’ll notice two outputs: a warning message and the graphic shown in Figure 12.2. Let’s first unpack the warning message: “y” axis ## Warning: Removed 1372 rows containing missing values (`geom_point()`). Figure 12.2: Median Household Income in 2016 vs Mobility for Children with Parents in Percentile 25 After running the above code, R returns a warning message alerting us to the fact that 1372 rows were ignored due to them being missing. For 1372 rows either the value for med_hhinc2016 or kfr_pooled_p25 or both were missing (recorded in R as NA), and thus these rows were ignored in our plot. Turning our attention to the resulting scatterplot in Figure 12.2, we see that a positive relationship exists between med_hhinc2016 and kfr_pooled_p25: as the median income level of the neighborhood increases, the mobility for children of parents in the percentile 25 tend to also increase. Before we continue, let’s consider a few more notes on the layers in the above code that generated the scatterplot: Note that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning. When adding layers to a plot, you are encouraged to start a new line after the + so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code. To stress the importance of adding layers in particular the layer specifying the geometric object, consider Figure 12.3 where no layers are added. A not very useful plot! ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) Figure 12.3: Plot with No Layers Learning check (LC3.1) What are some other features of the plot that stand out to you? (LC3.2) Create a new scatterplot using different variables in the atlas data frame by modifying the example above. 12.3.2 Over-plotting Sometimes you end up with a large mass of points, which can cause some confusion as it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to values being plotted on top of each other over and over again. It is often difficult to know just how many values are plotted in this way when looking at a basic scatterplot as we have here. The main methods to address the issue of overplotting is: By adjusting the transparency of the points. Changing the transparency The main way of addressing overplotting is by changing the transparency of the points by using the alpha argument in geom_point(). By default, this value is set to 1. We can change this to any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. Note how the following code is identical to the code in Section 12.3 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point(): ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25))+ geom_point(alpha = 0.2) Figure 12.4: Delay scatterplot with alpha=0.2 The key feature to note in Figure 12.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line above to read geom_point(aes(alpha = 0.2)). Learning check (LC3.3) Why is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot? 12.3.3 Summary Scatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful! With medium to large data sets, you may need to play around with the different modifications one can make to a scatterplot. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships come about as you make subtle changes to your plots. Last thing: remember Figure 12.1? Here is the code of how we did that. It has a few pieces that you would need to figure out on your own, but you should get the essence by now. In particular, in the first line we pass on a filtered data for the year 2007. I’ll explain that more in the next Section 13 on Data Wrangling. Still, try it out! ggplot(data = gapminder%&gt;%filter(year == 2007), mapping = aes(x=gdpPercap, y=lifeExp, size=pop, col=continent)) + geom_point() + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy&quot;) 12.4 Histograms Let’s consider the kfr_pooled_p25 variable in the atlas data frame once again, but now we want to understand how the values of kfr_pooled_p25 distribute. In other words, for economic mobility for children from parents in the percentile 25: What are the smallest and largest values? What is the “center” value? How do the values spread out? What are frequent and infrequent values? One way to visualize this distribution of this single variable kfr_pooled_p25 is to plot what is know as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows: We first cut up the x-axis into a series of bins, where each bin represents a range of values. For each bin, we count the number of observations that fall in the range corresponding to that bin. Then for each bin, we draw a bar whose height marks the corresponding count. Let’s drill-down on an example of a histogram, shown in Figure 12.5. Figure 12.5: Example histogram. Observe that there are six bins of equal width between $ 30,000 and $ 60,000, thus we have three bins of width $ 5,000 each: one bin for the 30-35k range, and so on, until the bin for the 55-60k range. Since: The bin for the 30-35k range has a height of around 9000, this histogram is telling us that around 9000 neighborhoods in the US have an average mobility measure for children of parents in the percentile 25th of between $30,000 and $35,000. The remaining bins all have a similar interpretation. 12.4.1 Histograms via geom_histogram Let’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable temp. The y-aesthetic of a histogram gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram() ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1267 rows containing non-finite values (`stat_bin()`). Figure 12.6: Histogram of mobility for children of p25 in the US. Let’s unpack the messages R sent us first. The first message is telling us that the histogram was constructed using bins = 30, in other words 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We’ll see in the next section how to change this default number of bins. The second message is telling us once again that there are some missing values: that because one row has a missing NA value for kfr_pooled_p25, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case. Now’s let’s unpack the resulting histogram in Figure 12.6. Observe that values less than $12,000 as well as values above $80,000 are rather rare. However, because of the large number of bins, its hard to get a sense for which range of temperatures is covered by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram(): ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(color = &quot;white&quot;) Figure 12.7: Histogram of mobility for children of p25 in the US with white borders. We can now better associate ranges of mobility to each of the bins. We can also vary the color of the bars by setting the fill argument. Run colors() to see all 657 possible choice of colors! ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;) Figure 12.8: Histogram of mobility for children of p25 in the US with white borders. 12.4.2 Adjusting the bins Let’s now adjust the number of bins in our histogram in one of two methods: By adjusting the number of bins via the bins argument to geom_histogram(). By adjusting the width of the bins via the binwidth argument to geom_histogram(). Using the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows: ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(bins = 40, color = &quot;white&quot;) Figure 12.9: Histogram with 40 bins. Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 10°F. ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(binwidth = 4000, color = &quot;white&quot;) Figure 12.10: Histogram with binwidth 10. Learning check (LC3.4) What does changing the number of bins from 30 to 40 tell us about the distribution of mobility? (LC3.5) Would you classify the distribution of mobility as symmetric or skewed? (LC3.6) What would you guess is the “center” value in this distribution? Why did you make that choice? (LC3.7) Is this data spread out greatly from the center or is it close? Why? 12.4.3 Summary Histograms, unlike scatterplots, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question. 12.5 Barplots Histograms are tools to visualize the distribution of numerical variables. Another common task is visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories, also known as levels, of a categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with a barplot (also known as a barchart). One complication, however, is how your data is represented: is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges. fruits &lt;- data_frame( fruit = c(&quot;apple&quot;, &quot;apple&quot;, &quot;orange&quot;, &quot;apple&quot;, &quot;orange&quot;) ) fruits_counted &lt;- data_frame( fruit = c(&quot;apple&quot;, &quot;orange&quot;), number = c(3, 2) ) We see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually… ## # A tibble: 5 × 1 ## fruit ## &lt;chr&gt; ## 1 apple ## 2 apple ## 3 orange ## 4 apple ## 5 orange … fruits_counted has a variable count which represents pre-counted values of each fruit. ## # A tibble: 2 × 2 ## fruit number ## &lt;chr&gt; &lt;dbl&gt; ## 1 apple 3 ## 2 orange 2 Depending on how your categorical data is represented, you’ll need to use add a different geom layer to your ggplot() to create a barplot, as we now explore. 12.5.1 Barplots via geom_bar or geom_col Let’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer. ggplot(data = fruits, mapping = aes(x = fruit)) + geom_bar() Figure 12.11: Barplot when counts are not pre-counted However, using the fruits_counted data frame where the fruit have been “pre-counted”, we map the fruit variable to the x-position aesthetic as with geom_bar(), but we also map the count variable to the y-position aesthetic, and add a geom_col() layer. ggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) + geom_col() Figure 12.12: Barplot when counts are pre-counted Compare the barplots in Figures 12.11 and 12.12. They are identical because they reflect count of the same 5 fruit. However depending on how our data is saved, either pre-counted or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize is: Is not pre-counted in your data frame: use geom_bar(). Is pre-counted in your data frame, use geom_col() with the y-position aesthetic mapped to the variable that has the counts. Let’s now go back to the atlas data frame and visualize the distribution of the categorical variable state. In other words, let’s visualize the number of neighborhoodsin the data belonging to each state. Recall from Section 11.7.4 when you first explored the atlas data frame you saw that each row corresponds to a neighborhood. In other words the atlas data frame is more like the fruits data frame than the fruits_counted data frame above, and thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable state gets mapped to the x-position. ggplot(data = atlas, mapping = aes(x = state)) + geom_bar() Figure 12.13: Number of neighborhoods by State in the atlas data using geom_bar Observe in Figure 12.13 that state 06, which is California, has the most number of neighborhoods in the data. If you don’t know which State FIPS code correspond to which State, you can see it here. For example: TX is State 48, while NC is State 37. Learning check (LC3.8) Why are histograms inappropriate for visualizing categorical variables? (LC3.9) What is the difference between histograms and barplots? (LC3.10) How many neighborhoods are there in Texas in the atlas data? 12.5.2 Summary Barplots are the preferred way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories called levels occur. They are easy to understand and make it easy to make comparisons across levels. When trying to visualize two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the joint distribution you are trying to emphasize, you will need to make a choice between these three types of barplots. 12.6 Conclusion 12.6.1 Summary table Let’s recap all five of the three main figures in Table ?? summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. 12.6.2 Argument specification Run the following two segments of code. First this: ggplot(data = atlas, mapping = aes(x = state)) + geom_bar() then this: ggplot(atlas, aes(x = state)) + geom_bar() You’ll notice that that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() by default assumes that the data argument comes first and the mapping argument comes second. So as long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. Going forward for the rest of this book, all ggplot() will be like the second segment above: with the data = and mapping = explicit naming of the argument omitted and the default ordering of arguments respected. 12.6.3 Additional resources If you want to further unlock the power of the ggplot2 package for data visualization, you can check out RStudio’s “Data Visualization with ggplot2” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular the many more than the 3 geom geometric objects we covered in this Chapter, while providing quick and easy to read visual descriptions. You can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Data Visualization with ggplot2”: Figure 12.14: Data Visualization with ggplot2 cheatsheat "],["wrangling.html", "Chapter 13 Data Wrangling 13.1 The pipe operator: %&gt;% 13.2 filter rows 13.3 mutate existing variables 13.4 Other verbs 13.5 Conclusion", " Chapter 13 Data Wrangling So far in our journey, we’ve seen how to look at data saved in data frames using the glimpse() and View() functions in Chapter 11 on and how to create data visualizations using the ggplot2 package in Chapter 12. In particular we studied what we term the the following three graphs: scatterplots via geom_point() histograms via geom_histogram() barplots via geom_bar() or geom_col() We created these visualizations using the “Grammar of Graphics”, which maps variables in a data frame to the aesthetic attributes of one the above 3 geometric objects. We can also control other aesthetic attributes of the geometric objects such as the size and color as seen in the Gapminder data example in Figure 12.1. Recall however in previous chapters we discussed that for two of our visualizations we needed transformed/modified versions of existing data frames. Recall for example the scatterplot of economic mobility only for neighborhoods in Wisconsin. In order to create this visualization, we needed to first pare down the atlas data frame to a new data frame atlas_wisconsin. consisting of only state == 55 neighborhoods using the filter() function. atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) ggplot(data = atlas_wisconsin, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) + geom_point() In this chapter, we’ll introduce two very useful functions from the dplyr package that will allow you to take a data frame and filter() its existing rows to only pick out a subset of them. For example, the atlas_wisconsin data frame above. mutate() its existing columns/variables to create new ones. For example, convert dollars to log dollars. There are other functions we will not cover here, but that could be really useful for you to know if you ever get to work with data outside this course: summarize(), group_by(), arrange() and join(). I will leave those to learn on your own if you want to. You can check this very good explanation. Back to our functions! Notice how we used computer code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling that we’ll introduce in this chapter has intuitively verb-named functions that are easy to remember. We’ll start by introducing the pipe operator %&gt;%, which allows you to combine multiple data wrangling verb-named functions into a single sequential chain of actions. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 11.6 for information on how to install and load R packages. We will also load the atlas data once more. library(dplyr) library(ggplot2) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) 13.1 The pipe operator: %&gt;% Before we start data wrangling, let’s first introduce a very nifty tool that gets loaded along with the dplyr package: the pipe operator %&gt;%. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of f(x) as an input to a function g() then Use the output of g(f(x)) as an input to a function h() One way to achieve this sequence of operations is by using nesting parentheses as follows: h(g(f(x))) The above code isn’t so hard to read since we are applying only three functions: f(), then g(), then h(). However, you can imagine that this can get progressively harder and harder to read as the number of functions applied in your sequence increases. This is where the pipe operator %&gt;% comes in handy. %&gt;% takes one output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then.” For example, you can obtain the same output as the above sequence of operations as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this above sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So while both approaches above would achieve the same goal, the latter is much more human-readable because you can read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling: The starting value x will be a data frame. For example: atlas. The sequence of functions, here f(), g(), and h(), will be a sequence of any number of the 6 data wrangling verb-named functions we listed in the introduction to this chapter. For example: filter(state == 55). The result will be the transformed/modified data frame that you want. For example: a data frame consisting of only the subset of rows in atlas corresponding to neighborhoods in the State of Wisconsin. Much like when adding layers to a ggplot() using the + sign at the end of lines, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence with pipe operators %&gt;% at the end of lines. So continuing our example involving neighborhoods in Wisconsin, we form a chain using the pipe operator %&gt;% and save the resulting data frame in atlas_wisconsin: atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) Keep in mind, there are many more advanced data wrangling functions than just the 2 listed in the introduction to this chapter; you’ll see some examples of these in Section 13.4. However, just with these 2 verb-named functions you’ll be able to perform a broad array of data wrangling tasks for the rest of this course. 13.2 filter rows Figure 13.1: Diagram of The filter() function here works much like the “Filter” option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only those rows that match that criteria. We begin by focusing only on neighborhoods from the state of North Carolina. The state code (or airport code) for Portland, Oregon is 37. Run the following and look at the resulting spreadsheet to ensure that only neighborhoods from North Carolina are chosen here: atlas_nc&lt;- atlas %&gt;% filter(state == 37) View(atlas_nc) Note the following: The ordering of the commands: Take the atlas data frame then filter the data frame so that only thosewith state equals 37 are included. We test for equality using the double equal sign == and not a single equal sign =. In other words filter(state = 37) will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it. You can use other mathematical operations beyond just == to form criteria: &gt; corresponds to “greater than” &lt; corresponds to “less than” &gt;= corresponds to “greater than or equal to” &lt;= corresponds to “less than or equal to” != corresponds to “not equal to”. The ! is used in many programming languages to indicate “not”. Furthermore, you can combine multiple criteria together using operators that make comparisons: | corresponds to “or” &amp; corresponds to “and” To see many of these in action, let’s filter atlas for all rows that: Had a mean household income over 30,000 in the year 2000 and Are in North Carolina or Massachusetts; and Had an employment rate in the year 2000 of at least 80% Run the following: atlas_filter1 &lt;- atlas %&gt;% filter(hhinc_mean2000 &gt;30000 &amp; (state ==37 | state == 25) &amp; emp2000 &gt;= 0.8) View(atlas_filter1) Note that even though colloquially speaking one might say “all neighborhoods from North Carolina and Massachusetts” in terms of computer operations, we really mean “all neighborhoods from North Carolina or Massachusetts.” For a given row in the data, state can be 37, 25, or something else, but not 37 and 25 at the same time. Furthermore, note the careful use of parentheses around the state ==37 | state == 25. We can often skip the use of &amp; and just separate our conditions with a comma. In other words the code above will return the identical output atlas_filter1 as this code below: atlas_filter1 &lt;- atlas %&gt;% filter(hhinc_mean2000 &gt;30000, (state ==37 | state == 25), emp2000 &gt;= 0.8) View(atlas_filter1) Let’s present another example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to neighborhoods that are not in North Carolina or Massachusetts. atlas_not_nc_ma &lt;- atlas %&gt;% filter(!(state ==37 | state == 25)) View(atlas_not_nc_ma) Again, note the careful use of parentheses around the (state ==37 | state == 25). If we didn’t use parentheses as follows: atlas %&gt;% filter(!state ==37 | state == 25) We would be returning all neighborhoods not in 37 or those in 25, which is an entirely different resulting data frame. Now say we have a large list of airports we want to filter for, say NC (37), MA (25), FL(12) and PA(42). We could continue to use the | or operator as so: atlas_many_states &lt;- atlas %&gt;% filter(state ==37 | state == 25 | state == 12 | state == 42) View(atlas_many_states) but as we progressively include more airports, this will get unwieldy. A slightly shorter approach uses the %in% operator: atlas_many_states &lt;- atlas %&gt;% filter(state %in% c(37, 25, 12, 42)) View(atlas_many_states) What this code is doing is filtering atlas for all neighborhoods where state is in the list of airports c(37, 25, 12, 42). Recall from Chapter 11 that the c() function “combines” or “concatenates” values in a vector of values. Both outputs of atlas_many_states are the same, but as you can see the latter takes much less time to code. As a final note we point out that filter() should often be among the first verbs you apply to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations your care about. 13.3 mutate existing variables Figure 13.2: Mutate diagram from Data Wrangling with dplyr and tidyr cheatsheet A common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of income in terms of the logarithm of income instead of in dollars. We will apply this to the variable hhinc_mean2000 (the mean household income in 2000). You want to implement the following formula: \\[ \\text{income} = log(\\text{income}) \\] We can apply this formula to the hhinc_mean2000 variable using the mutate() function, which takes existing variables and mutates them to create new ones. atlas &lt;- atlas %&gt;% mutate(log_hhinc_mean2000= log(hhinc_mean2000)) View(atlas) Note that we have overwritten the original atlas data frame with a new version that now includes the additional variable log_hhinc_mean2000. In other words, the mutate() command outputs a new data frame which then gets saved over the original atlas data frame. Furthermore, note how in mutate() we used log_hhinc_mean2000= log(hhinc_mean2000) to create a new variable log_hhinc_mean2000. Why did we overwrite the data frame atlas instead of assigning the result to a new data frame like atlas_new, but on the other hand why did we not overwrite hhinc_mean2000, but instead created a new variable called temp_in_C? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames. On the other hand, had we used mutate(hhinc_mean2000 = log(hhinc_mean2000) instead of mutate(log_hhinc_mean2000= log(hhinc_mean2000)), we would have overwritten the original variable hhinc_mean2000 and lost its values. 13.4 Other verbs Here are some other useful data wrangling verbs that might come in handy: select() only a subset of variables/columns rename() variables/columns to have new names Return only the top_n() values of a variable 13.4.1 select variables Figure 13.3: Select diagram from Data Wrangling with dplyr and tidyr cheatsheet We’ve seen that the atlas data frame contains 62 different variables. You can identify the names of these 19 variables by running the glimpse() function from the dplyr package: glimpse(atlas) However, say you only need two of these variables, say state and emp2000. You can select() these two variables: atlas %&gt;% select(state, emp2000) This function makes exploring data frames with a very large number of variables easier for humans to process by restricting consideration to only those we care about, like our example with state and emp2000 above. This might make viewing the dataset using the View() spreadsheet viewer more digestible. However, as far as the computer is concerned, it doesn’t care how many additional variables are in the data frame in question, so long as state and state are included. Lastly, the helper functions starts_with(), ends_with(), and contains() can be used to select variables/column that match those conditions. For example: atlas_begin_kfr &lt;- atlas %&gt;% select(starts_with(&quot;kfr&quot;)) atlas_begin_kfr 13.4.2 rename variables Another useful function is rename(), which as you may have guessed renames one column to another name. Suppose we want emp2000 and popdensity2010 to be employmentrate2000 and populationdensity2010 instead in the atlas data frame: atlas &lt;- atlas %&gt;% rename(populationdensity2010 = popdensity2010, employmentrate2000 = emp2000) glimpse(atlas) Note that in this case we used a single = sign within the rename(). This is because we are not testing for equality like we would using ==, but instead we want to assign a new variable populationdensity2010 to have the same values as popdensity2010 and then delete the variable popdensity2010. It’s easy to forget if the new name comes before or after the equals sign. I usually remember this as “New Before, Old After” or NBOA. 13.4.3 top_n values of a variable We can also return the top n values of a variable using the top_n() function. For example, we can return a data frame of the top neighborhoods by mobility for children from parents in the percentile 25. Observe that we set the number of values to return to n = 10 and wt = kfr_pooled_p25 to indicate that we want the rows of corresponding to the top 10 values of kfr_pooled_p25. See the help file for top_n() by running ?top_n for more information. atlas %&gt;% top_n(n = 10, wt = kfr_pooled_p25) Let’s further arrange() these results in descending order of kfr_pooled_p25: atlas %&gt;% top_n(n = 10, wt = kfr_pooled_p25) %&gt;% arrange(desc(kfr_pooled_p25)) You can read more about the function arrange() here, but the logic is fairly simple. We are organizing the neighborhoods in descending order in the variable kfr_pooled_p25. 13.5 Conclusion 13.5.1 Additional resources If you want to further unlock the power of the dplyr package for data wrangling, we suggest you that you check out RStudio’s “Data Transformation with dplyr” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular more-intermediate level and advanced data wrangling functions, while providing quick and easy to read visual descriptions. You can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Data Transformation with dplyr”: Figure 13.4: Data Transformation with dplyr cheatsheat On top of data wrangling verbs and examples we presented in this section, if you’d like to see more examples of using the dplyr package for data wrangling check out Chapter 5 of Garrett Grolemund and Hadley Wickham’s and Garrett’s book (rds2016?). "],["regression.html", "Chapter 14 Simple and Multivariate Regression 14.1 One numerical explanatory variable 14.2 Related topics 14.3 Two numerical explanatory variables 14.4 How to read p-values 14.5 Conclusion", " Chapter 14 Simple and Multivariate Regression As a reminder, this is not a course on econometrics, so I will not delve deep into the issues of regression. Nonetheless, there are basic intuitions we can construct when analyzing the data with these models. I will summarize the main issues in this chapter. Nonetheless, feel free to dive deeper here. Now that we are equipped with data visualization skills from Chapter 12, and data wrangling skills from Chapter 13, we now proceed with data modeling. The fundamental premise of data modeling is to make explicit the relationship between: an outcome variable \\(y\\), also called a dependent variable and an explanatory/predictor variable \\(x\\), also called an independent variable or covariate. Another way to state this is using mathematical terminology: we will model the outcome variable \\(y\\) as a function of the explanatory/predictor variable \\(x\\). Why do we have two different labels, explanatory and predictor, for the variable \\(x\\)? That’s because roughly speaking data modeling can be used for two purposes: Modeling for prediction: You want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables. You don’t care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about \\(y\\), you’re fine. For example, if we know many individuals’ risk factors for lung cancer, such as smoking habits and age, can we predict whether or not they will develop lung cancer? Here we wouldn’t care so much about distinguishing the degree to which the different risk factors contribute to lung cancer, but instead only on whether or not they could be put together to make reliable predictions. Modeling for explanation: You want to explicitly describe the relationship between an outcome variable \\(y\\) and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these. Continuing our example from above, we would now be interested in describing the individual effects of the different risk factors and quantifying the magnitude of these effects. One reason could be to design an intervention to reduce lung cancer cases in a population, such as targeting smokers of a specific age group with an advertisement for smoking cessation programs. In this course, we’ll focus more on this latter purpose. Data modeling is used in a wide variety of fields, including statistical inference, causal inference, artificial intelligence, and machine learning. There are many techniques for data modeling, such as tree-based models, neural networks and deep learning, and supervised learning. We will briefly touch on those at the end of the course. For now, we’ll focus on one particular technique: linear regression, one of the most commonly-used and easy-to-understand approaches to modeling. Recall our discussion in Subsection 11.7.4 on numerical and categorical variables. Linear regression involves: an outcome variable \\(y\\) that is numerical and explanatory variables \\(x_i\\) (e.g. \\(x_1, x_2, ...\\)) that are either numerical or categorical. With linear regression there is always only one numerical outcome variable \\(y\\) but we have choices on both the number and the type of explanatory variables to use. We’re going to cover the following regression scenarios: Only one explanatory variable: simple linear regression In Section 14.1, this explanatory variable will be a single numerical explanatory variable \\(x\\). In the next Section on Experiments we will use an explanatory variable that is a categorical explanatory variable \\(x\\) (being in a treatment or in a control). This is an example of a broader class of regressions using explanatory variables, but we will see those later. More than one explanatory variable: multiple regression We’ll focus on two numerical explanatory variables, \\(x_1\\) and \\(x_2\\), in Section 14.3. We’ll use one numerical and one categorical explanatory variable. As before, we will see this in later sections. We’ll study the first of each of these two types of regression scenarios using the atlas data! Needed packages Let’s now load all the packages needed for this chapter (this assumes you’ve already installed them). In this chapter we introduce some new packages: The tidyverse “umbrella” package. You can load the tidyverse package by running library(tidyverse), which loads several other packages we have used thus far, including: ggplot2 for data visualization dplyr for data wrangling As well as the more advanced purrr, tidyr, readr, tibble, stringr, and forcats packages The skimr (R-skimr?) package, which provides a simple-to-use function to quickly compute a wide array of commonly-used summary statistics. If needed, read Section 11.6 for information on how to install and load R packages. We will also load the atlas dataset. library(tidyverse) library(skimr) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) 14.1 One numerical explanatory variable Let’s say you want to explore what correlates with economic mobility for children of poor parents (those at percentile 25 of the parental income distribution). To try to keep things simple, you want to understand if neighborhoods that have more jobs, tend to provide more or less economic mobility for this population. We will try to keep things simple for now and explain the differences in mobility with the average job growth rate between 2004 and 2013 (a rough measurement of how much jobs grow in that area) correlates with economic mobility for children from poor parents We’ll achieve ways to address these questions by modeling the relationship between these two variables with a particular kind of linear regression called simple linear regression. Simple linear regression is the most basic form of linear regression. With it we have A numerical outcome variable \\(y\\). In this case, economic mobility for children of parents in the percentile 25. A single numerical explanatory variable \\(x\\). In this case, the average annual job growth rate between 2004 and 2013 for that neighborhood. 14.1.1 Exploratory data analysis A crucial step before doing any kind of modeling or analysis is performing an exploratory data analysis, or EDA, of all our data. Exploratory data analysis can give you a sense of the distribution of the data and whether there are outliers and/or missing values. Most importantly, it can inform how to build your model. There are many approaches to exploratory data analysis; here are three: Most fundamentally: just looking at the raw values, in a spreadsheet for example. While this may seem trivial, many people ignore this crucial step! Computing summary statistics like means, medians, and standard deviations. Creating data visualizations. Let’s load the atlas data, select only a subset of the variables (including the ‘names’, which are the codes of tract, country and state), and look at the raw values. Recall you can look at the raw values by running View() in the console in RStudio to pop-up the spreadsheet viewer with the data frame of interest as the argument to View(). Here, however, we present only a snapshot of five randomly chosen rows: atlas_mob_jobs &lt;- atlas %&gt;% select(tract, county, state, kfr_pooled_p25, ann_avg_job_growth_2004_2013) atlas_mob_jobs %&gt;% sample_n(5) Table 14.1: Table 14.2: Random sample of 5 neighborhoods tract county state kfr_pooled_p25 ann_avg_job_growth_2004_2013 90200 61 37 28016 0.020 602004 37 6 30825 0.000 16000 13 34 48741 0.030 40101 5 44 35800 -0.024 211400 79 42 34588 0.013 You can see the full description of each variable in the Section on Data Description for Project 1 and the bulk of Lecture 1 on the Geography of Economic Mobility, but let’s summarize what each variable represents: tract: FIPS code for the tract country: FIPS code for the county state: FIPS code for the state kfr_pooled_p25: Numerical variable of the average income that children that grew up in that neighborhood from parents in the percentile 25th receive when adults. ann_avg_job_growth_2004_2013: Numerical variable of average job growth between 2004 and 2013. Notice that the numbers on job growth need to be multiplied by 100 to get percentages. That is, 0 is 0\\%, while 1 would be 100\\%. An alternative way to look at the raw data values is by choosing a random sample of the rows in atlas_mob_jobs by piping it into the sample_n() function from the dplyr package. Here we set the size argument to be 5, indicating that we want a random sample of 5 rows. We display the results in Table 14.3. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows. atlas_mob_jobs %&gt;% sample_n(size = 5) Table 14.3: Table 14.4: A random sample of 5 neighborhoods tract county state kfr_pooled_p25 ann_avg_job_growth_2004_2013 9200 157 47 34943 -0.090 2605 99 6 32270 0.014 15100 71 36 29352 -0.102 950700 17 16 33694 0.024 52902 183 37 33021 -0.017 Now that we’ve looked at the raw values in our atlas_mob_jobs data frame and got a preliminary sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s start by computing the mean and median of our numerical outcome variable kfr_pooled_p25 and our numerical explanatory variable on ob growth denoted as ann_avg_job_growth_2004_2013. We’ll do this by using the summarize() function from dplyr along with the mean() and median() summary functions, in a function that although we did not cover, is fairly intuitive: summarize. It will take some data, and summarize it according to what you ask. ## # A tibble: 1 × 4 ## mean_ann_avg_job_growth_2004_2013 mean_kfr_pooled_p25 median_ann_avg_job_growth_2004_2013 median_kfr_pooled_p25 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0153 34443. 0.00850 33733. atlas_mob_jobs %&gt;% summarize(mean_ann_avg_job_growth_2004_2013 = mean(ann_avg_job_growth_2004_2013, na.rm=TRUE), mean_kfr_pooled_p25 = mean(kfr_pooled_p25, na.rm=TRUE), median_ann_avg_job_growth_2004_2013 = median(ann_avg_job_growth_2004_2013, na.rm=TRUE), median_kfr_pooled_p25 = median(kfr_pooled_p25, na.rm=TRUE)) However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? Typing out all these summary statistic functions in summarize() would be long and tedious. Instead, let’s use the convenient skim() function from the skimr package. This function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our atlas_mob_jobs data frame, select() only the outcome and explanatory variables mobility kfr_pooled_p25 and job growth ann_avg_job_growth_2004_2013, and pipe them into the skim() function: atlas_mob_jobs %&gt;% select(kfr_pooled_p25, ann_avg_job_growth_2004_2013) %&gt;% skim() Skim summary statistics n obs: 73278 n variables: 2 ── Variable type:numeric ─────────────────────────────────────────────────────── variable missing complete n mean sd p0 p25 p50 p75 p100 kfr_pooled_p25 1267 0.983 34443. 8169. 0 28973 33733. 39167. 105732. ann_avg_job_growth_2004_2013 2614 0.964 0.0153 0.0762 -0.607 -0.0189 0.00850 0.0410 1.34 (Note that for formatting purposes, the inline histogram that is usually printed with skim() has been removed. This can be done by running skim_with(numeric = list(hist = NULL)) prior to using the skim() function as well.) For our two numerical variables mobility kfr_pooled_p25 and job growth ann_avg_job_growth_2004_2013 it returns: missing: the number of missing values complete: the number of non-missing or complete values n: the total number of values mean: the average sd: the standard deviation p0: the 0th percentile: the value at which 0% of observations are smaller than it (the minimum value) p25: the 25th percentile: the value at which 25% of observations are smaller than it (the 1st quartile) p50: the 50th percentile: the value at which 50% of observations are smaller than it (the 2nd quartile and more commonly called the median) p75: the 75th percentile: the value at which 75% of observations are smaller than it (the 3rd quartile) p100: the 100th percentile: the value at which 100% of observations are smaller than it (the maximum value) Looking at this output, we get an idea of how the values of both variables distribute. For example, the mean mobility was $ 34,443 whereas the mean job growth was 0.0153 or 1.53%. Furthermore, the middle 50% of mobility were between $28,973 and $39,167 (the first and third quartiles) whereas the middle 50% of job growth were between -0.0189 (-1.89%) and 0.0410 (+4.10%). The skim() function only returns what are known as univariate summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist bivariate summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the correlation coefficient. Generally speaking, coefficients are quantitative expressions of a specific phenomenon. A correlation coefficient is a quantitative expression of the strength of the linear relationship between two numerical variables. Its value ranges between -1 and 1 where: -1 indicates a perfect negative relationship: As the value of one variable goes up, the value of the other variable tends to go down following along a straight line. 0 indicates no relationship: The values of both variables go up/down independently of each other. +1 indicates a perfect positive relationship: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion. Figure 14.1 gives examples of different correlation coefficient values for hypothetical numerical variables \\(x\\) and \\(y\\). We see that while for a correlation coefficient of -0.75 there is still a negative relationship between \\(x\\) and \\(y\\), it is not as strong as the negative relationship between \\(x\\) and \\(y\\) when the correlation coefficient is -1. Figure 14.1: Different correlation coefficients The correlation coefficient is computed using the cor() function, where the inputs to the function are the two numerical variables for which we want to quantify the strength of the linear relationship. Note, however, that because the dataset has missing values for some neighborhoods, if you run cor() without the option use = \"pairwise.complete.obs\", you will get NA - which is a missing value. See that we add this option below: atlas_mob_jobs %&gt;% summarise(correlation = cor(kfr_pooled_p25, ann_avg_job_growth_2004_2013, use = &quot;pairwise.complete.obs&quot;)) ## # A tibble: 1 × 1 ## correlation ## &lt;dbl&gt; ## 1 0.0716 You can also use the cor() function directly instead of using it inside summarise, but you will need to use the $ syntax to access the specific variables within a data frame (See Subsection 11.7.4): cor(x = atlas_mob_jobs$ann_avg_job_growth_2004_2013, y = atlas_mob_jobs$kfr_pooled_p25, use = &quot;pairwise.complete.obs&quot;) ## [1] 0.0716 In our case, the correlation coefficient of NA indicates that the relationship between mobility and job growth is “weakly positive” There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren’t close to -1, 0, and 1. For help developing such intuition and more discussion on the correlation coefficient see Subsection 14.2.1 below. Let’s now proceed by visualizing this data. Since both the kfr_pooled_p25 and ann_avg_job_growth_2004_2013 variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let’s do this using geom_point() and set informative axes labels and title and display the result in Figure 14.2. ggplot(atlas_mob_jobs, aes(x = ann_avg_job_growth_2004_2013, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Average Job Growth Rate 2004-2013&quot;, y = &quot;Mobility for Children of Parents in P25&quot;, title = &quot;Relationship of mobility and job growth&quot;) Figure 14.2: Economic mobility for children of parents at percentile 25 across the US Observe the following: Most average job growth rates lie between -0.25 (-25%) and 0.25 (+25%). Most mobility outcomes lie between $18,000 and $50,000. Recall our earlier computation of the correlation coefficient, which describes the strength of the linear relationship between two numerical variables. Looking at Figure 14.2, it is not immediately apparent that these two variables are positively related. This is to be expected given the positive, but rather weak (close to 0), correlation coefficient of NA. Going back to scatterplot in Figure 14.2, let’s improve on it by adding a “regression line” in Figure 14.3. This is easily done by adding a new layer to the ggplot code that created Figure 14.2: + geom_smooth(method = \"lm\"). A regression line is a “best fitting” line in that of all possible lines you could draw on this plot, it is “best” in terms of some mathematical criteria. There is a formal definition for what “best” means, but we do not need to go into detail here. Suffice to say, this line minimizes errors in the prediction in the sense that the square of the distance between our prediction (the line) and the real data. ggplot(atlas_mob_jobs, aes(x = ann_avg_job_growth_2004_2013, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Average Job Growth Rate 2004-2013&quot;, y = &quot;Mobility for Children of Parents in P25&quot;, title = &quot;Relationship of mobility and job growth&quot;) + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 14.3: Regression line When viewed on this plot, the regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable kfr_pooled_p25 and the explanatory variable ann_avg_job_growth_2004_2013. The positive slope of the blue line is consistent with our observed correlation coefficient of NA suggesting that there is a positive relationship between kfr_pooled_p25 and ann_avg_job_growth_2004_2013. We’ll see later however that while the correlation coefficient is not equal to the slope of this line, they always have the same sign: positive or negative. You can barely see it. perhaps, but the blue line has some very thin grey bands surrounding it. What are those? These are standard error bands, which can be thought of as error/uncertainty bands. Let’s skip this idea for now and suppress these grey bars by adding the argument se = FALSE to geom_smooth(method = \"lm\"). We’ll briefly introduce standard errors below, but we can skip that for now. ggplot(atlas_mob_jobs, aes(x = ann_avg_job_growth_2004_2013, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Average Job Growth Rate 2004-2013&quot;, y = &quot;Mobility for Children of Parents in P25&quot;, title = &quot;Relationship of mobility and job growth&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 14.4: Regression line without error bands Learning check (LC5.1) Conduct a new exploratory data analysis with the same outcome variable \\(y\\) being kfr_pooled_p25 but with poor_share2010 as the new explanatory variable \\(x\\). Remember, this involves three things: Looking at the raw values. Computing summary statistics of the variables of interest. Creating informative visualizations. What can you say about the relationship between the share of people living under the poverty line in 2010 and mobility measures for children of parents in p25 based on this exploration? 14.1.2 Simple linear regression You may recall from secondary school / high school algebra, in general, the equation of a line is \\(y = a + bx\\), which is defined by two coefficients. Recall we defined this earlier as “quantitative expressions of a specific property of a phenomenon.” These two coefficients are: the intercept coefficient \\(a\\), or the value of \\(y\\) when \\(x = 0\\), and the slope coefficient \\(b\\), or the increase in \\(y\\) for every increase of one in \\(x\\). However, when defining a line specifically for regression, like the blue regression line in Figure 14.4, we use slightly different notation: the equation of the regression line is \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) where the intercept coefficient is \\(b_0\\), or the value of \\(\\widehat{y}\\) when \\(x=0\\), and the slope coefficient \\(b_1\\), or the increase in \\(\\widehat{y}\\) for every increase of one in \\(x\\). Why do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression, which we’ll introduce in the next Subsection ?? when we discuss fitted values. For now, let’s ignore the hat and treat the equation of the line as you would from secondary school / high school algebra recognizing the slope and the intercept. We know looking at Figure 14.4 that the slope coefficient corresponding to ann_avg_job_growth_2004_2013 should be positive. Why? Because as ann_avg_job_growth_2004_2013 increases, professors tend to roughly have higher teaching evaluation kfr_pooled_p25. However, what are the specific values of the intercept and slope coefficients? Let’s not worry about computing these by hand, but instead let the computer do the work for us. Specifically, let’s use R! Let’s get the value of the intercept and slope coefficients by outputting something called the linear regression table. We will fit the linear regression model to the data using the lm() function and save this to mob_model. lm stands for “linear model.” When we say “fit”, we are saying find the best fitting line to this data. The lm() function that “fits” the linear regression model is typically used as lm(y ~ x, data = data_frame_name) where: y is the outcome variable, followed by a tilde (~). This is likely the key to the left of “1” on your keyboard. In our case, y is set to kfr_pooled_p25. x is the explanatory variable. In our case, x is set to ann_avg_job_growth_2004_2013. We call the combination y ~ x a model formula. data_frame_name is the name of the data frame that contains the variables y and x. In our case, data_frame_name is the atlas_mob_jobs data frame. mob_model &lt;- lm(kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013, data = atlas_mob_jobs) mob_model ## ## Call: ## lm(formula = kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013, data = atlas_mob_jobs) ## ## Coefficients: ## (Intercept) ann_avg_job_growth_2004_2013 ## 34276 7783 This output is telling us that the Intercept coefficient \\(b_0\\) of the regression line is 3.8803, and the slope coefficient for by_avg is 0.0666. Therefore the blue regression line in Figure 14.4 is \\[\\widehat{\\text{mobilityp25}} = b_0 + b_{\\text{avg job growth}} \\cdot\\text{avg job growth} = 34276.27 + 7782.82\\cdot\\text{ avg job growth}\\] where The intercept coefficient \\(b_0 = 34276.27\\) means for instructors that had a hypothetical average job growth of 0, we would expect them to have on average mobility of $ 34,276. Of more interest is the slope coefficient associated with ann_avg_job_growth_2004_2013: \\(b_{\\text{bty avg}} = +7782.82\\). This is a numerical quantity that summarizes the relationship between the outcome and explanatory variables. Note that the sign is positive, suggesting a positive relationship between job growth and mobility, meaning as average job growth goes up, so does the mobility measure. The slope’s precise interpretation is: For every increase of 1 unit in ann_avg_job_growth_2004_2013, there is an associated increase of, on average, 7782.82 units of kfr_pooled_p25. A very important point to mention here is that you need to recall that the variable ann_avg_job_growth_2004_2013 is coded such that a 1 unit increment is equivalent to increasing the average annual job growth by 100%. Given that changing the average growth rate by 100% is a very large change, perhaps we should choose a smaller change to get a sense of how much the correlation explains mobility. That is, we can interpret the previous set of results by dividing the coefficient on average job growth rate by 100, and saying: For every increase of 1% in the average job growth rate ann_avg_job_growth_2004_2013, there is an associated increase of, on average, 77.82 units of mobility. Such interpretations need be carefully worded: We only stated that there is an associated increase, and not necessarily a causal increase. For example, perhaps it’s not that job growth directly affects mobility, but instead areas with higher mobility attract firms to locate there, thereby increasing job creation. Avoiding such reasoning can be summarized by the adage “correlation is not necessarily causation.” In other words, just because two variables are correlated, it doesn’t mean one directly causes the other. We discuss these ideas more in Subsection 14.2.2 and in following chapters. We say that this associated increase is on average 77.82 dollars of mobility kfr_pooled_p25 and not that the associated increase is exactly 77.82 dollars of kfr_pooled_p25 across all values of ann_avg_job_growth_2004_2013. This is because the slope is the average increase across all points as shown by the regression line in Figure 14.4. Now that we’ve learned how to compute the equation for the blue regression line in Figure 14.4 and interpreted all its terms, let’s take our modeling one step further. This time after fitting the model using the lm(), let’s get something called the regression table using the summary() function: # Fit regression model: mob_model &lt;- lm(kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013, data = atlas_mob_jobs) # Get regression results: summary(mob_model) ## ## Call: ## lm(formula = kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013, data = atlas_mob_jobs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35002 -5437 -719 4656 69932 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34276.3 31.2 1097 &lt;0.0000000000000002 *** ## ann_avg_job_growth_2004_2013 7782.8 409.2 19 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8110 on 70230 degrees of freedom ## (3046 observations deleted due to missingness) ## Multiple R-squared: 0.00512, Adjusted R-squared: 0.00511 ## F-statistic: 362 on 1 and 70230 DF, p-value: &lt;0.0000000000000002 Note how we took the output of the model fit saved in mob_model and used it as an input to the subsequent summary() function. The raw output of the summary() function above gives lots of information about the regression model that we won’t cover in this introductory course (e.g., Multiple R-squared, F-statistic, etc.). We will only consider the “Coefficients” section of the output. We can print these relevant results only by accessing the coefficients object stored in the summary results. summary(mob_model)$coefficients Table 14.5: Table 14.6: Linear regression table Estimate Std. Error t value Pr(&gt;&amp;#124;t&amp;#124;) (Intercept) 34276 31.2 1097 0 ann_avg_job_growth_2004_2013 7783 409.2 19 0 For now since we are only using regression as an exploratory data analysis tool, we will only focus on the “Estimate” column that contains the estimates of the intercept and slope for the best fit line for our data. The remaining three columns refer to statistical concepts known as standard errors, t-statistics, and p-values. They are all interrelated, but we will briefly only talk about p-values at the end of this Chapter. For now, we can skip that.. Learning check (LC5.2) Fit a new simple linear regression using lm(kfr_pooled_p25 ~ poor_share2010, data = atlas_mob_jobs) where poor_share2010 is the new explanatory variable \\(x\\). Get information about the “best-fitting” line from the regression table by applying the summary() function. How do the regression results match up with the results from your exploratory data analysis above? Investigating the residuals of these linear regressions is usually a good idea, but I will leave that for your Econometric classes. If you want to see how to explore the residuals in R, you can look here. 14.2 Related topics 14.2.1 Correlation coefficient Let’s re-plot Figure 14.1, but now consider a broader range of correlation coefficient values in Figure 14.5. Figure 14.5: Different Correlation Coefficients As we suggested in Subsection 14.1.1, interpreting coefficients that are not close to the extreme values of -1 and 1 can be subjective. To help develop your sense of correlation coefficients, we suggest you play the following 80’s-style video game called “Guess the correlation” at http://guessthecorrelation.com/. 14.2.2 Correlation is not necessarily causation You’ll note throughout this chapter we’ve been very cautious in making statements of the “associated effect” of explanatory variables on the outcome variables, for example our statement from Subsection 14.1.2 that “for every increase of 1 unit in ann_avg_job_growth_2004_2013, there is an associated increase of, on average, 7782.823 units of kfr_pooled_p25.” We say this because we are careful not to make causal statements. So while average job growth rate ann_avg_job_growth_2004_2013 is positively correlated with teaching kfr_pooled_p25, it does not imply that it directly cause mobility to increase. For example, let’s say a neighbrhood has their ann_avg_job_growth_2004_2013 increase, but only after taking steps to try to boost public investment in the area. Does this mean that they will suddenly be a better neighborhood for kids of low income parents? Maybe? Here is another example, a not-so-great medical doctor goes through their medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares “Sleeping with shoes on cause headaches!” Figure 14.6: Does sleeping with shoes on cause headaches? However as some of you might have guessed, if someone is sleeping with their shoes on it’s probably because they are intoxicated. Furthermore, drinking more tends to cause more hangovers, and hence more headaches. In this instance, alcohol is what’s known as a confounding/lurking variable. It “lurks” behind the scenes, confounding or making less apparent, the causal effect (if any) of “sleeping with shoes on” with waking up with a headache. We can summarize this notion in Figure 14.7 with a causal graph where: Y: Is an outcome variable, here “waking up with a headache.” X: Is a treatment variable whose causal effect we are interested in, here “sleeping with shoes on.” Figure 14.7: Causal graph. So for example, many such studies use regression modeling where the outcome variable is set to Y and the explanatory/predictor variable is X, much as you’ve started learning how to do in this chapter. However, Figure 14.7 also includes a third variable with arrows pointing at both X and Y. Z: Is a confounding variable that affects both X &amp; Y, thus “confounding” their relationship. So as we said, alcohol will both cause people to be more likely to sleep with their shoes on as well as more likely to wake up with a headache. Thus when evaluating what causes one to wake up with a headache, its hard to tease out the effect of sleeping with shoes on versus just the alcohol. Thus our model needs to also use Z as an explanatory/predictor variable as well, in other words our doctor needs to take into account who had been drinking the night before. We’ll start covering multiple regression models that allows us to incorporate more than one variable in the next chapter. Establishing causation is a tricky problem and frequently takes either carefully designed experiments or methods to control for the effects of potential confounding variables. Both these approaches attempt either to remove all confounding variables or take them into account as best they can, and only focus on the behavior of an outcome variable in the presence of the levels of the other variable(s). Be careful as you read studies to make sure that the writers aren’t falling into this fallacy of correlation implying causation. If you spot one, you may want to send them a link to Spurious Correlations. 14.3 Two numerical explanatory variables Let’s first consider a multiple regression model with two numerical explanatory variables. We will continue with our previous example coming from the atlas data. In this section, we’ll fit a regression model where we have A numerical outcome variable \\(y\\), the mobility measure for children of parents in the percentile 25th. Two explanatory variables: One numerical explanatory variable \\(x_1\\), average job growth rate between 2003 and 2014. Another numerical explanatory variable \\(x_2\\), the percentage of the population below the poverty line in 2010. 14.3.1 Exploratory data analysis Let’s do something similar to what we did before, but include the variable of the share of the population below the poverty line in 2010 poor_share2010 in the data that we keep track off, and call it atlas_mob_jobs_poor. atlas_mob_jobs_poor &lt;- atlas %&gt;% select(tract, county, state, kfr_pooled_p25, ann_avg_job_growth_2004_2013, poor_share2010) Recall the three common steps in an exploratory data analysis we saw in Subsection 14.1.1: Looking at the raw data values. Computing summary statistics. Creating data visualizations. Let us know skim the data with the skim() function: atlas_mob_jobs_poor %&gt;% select(kfr_pooled_p25, ann_avg_job_growth_2004_2013, poor_share2010) %&gt;% skim() Skim summary statistics n obs: 73278 n variables: 2 ── Variable type:numeric ─────────────────────────────────────────────────────── variable missing complete n mean sd p0 p25 p50 p75 p100 kfr_pooled_p25 1267 0.983 34443. 8169. 0 28973 33733. 39167. 105732. ann_avg_job_growth_2004_2013 2614 0.964 0.0153 0.0762 -0.607 -0.0189 0.00850 0.0410 1.34 poor_share2010 345 0.995 0.151 0.127 0 0.0591 0.116 0.205 1 Observe the summary statistics for the min and max of the variable poor_share2010 are 0 and 1, respectively. This should be a flag for you that the share is once more measured as 0 being 0% and 1 being 100%. Note that the average share of population living below the poverty line in 2010 is 15.1%, and that 25% of neighborhoods have a poverty share of 5.91% or less, while 75% of neighborhoods had a poverty share of 20.5% or less. Since our outcome variable kfr_pooled_p25 and the explanatory variables ann_avg_job_growth_2004_2013 and poor_share2010 are numerical, we can compute the correlation coefficient between the different possible pairs of these variables. First, we can run the cor() command as seen in Subsection 14.1.1 twice, once for each explanatory variable: cor(atlas_mob_jobs_poor$kfr_pooled_p25, atlas_mob_jobs_poor$ann_avg_job_growth_2004_2013, use = &quot;pairwise.complete.obs&quot;) cor(atlas_mob_jobs_poor$kfr_pooled_p25, atlas_mob_jobs_poor$poor_share2010, use = &quot;pairwise.complete.obs&quot;) Or we can simultaneously compute them by returning a correlation matrix which we display in Table 14.7. We can read off the correlation coefficient for any pair of variables by looking them up in the appropriate row/column combination. atlas_mob_jobs_poor %&gt;% select(kfr_pooled_p25, ann_avg_job_growth_2004_2013, poor_share2010) %&gt;% cor() Table 14.7: Table 14.8: Correlation coefficients between mobility, job growth and poverty rate. kfr_pooled_p25 ann_avg_job_growth_2004_2013 poor_share2010 kfr_pooled_p25 1.000 0.072 -0.544 ann_avg_job_growth_2004_2013 0.072 1.000 -0.063 poor_share2010 -0.544 -0.063 1.000 For example, the correlation coefficient of: kfr_pooled_p25 with itself is 1 as we would expect based on the definition of the correlation coefficient. kfr_pooled_p25 with ann_avg_job_growth_2004_2013 is 0.072. This indicates a very weak positive linear relationship. kfr_pooled_p25 with poor_share2010 is -0.544. This is suggestive of a strong negative linear relationship. At least, much stronger than the one between kfr_pooled_p25 and ann_avg_job_growth_2004_2013. As an added bonus, we can read off the correlation coefficient between the two explanatory variables of ann_avg_job_growth_2004_2013 and poor_share2010 as -0.063. Let’s visualize the relationship of the outcome variable with each of the two explanatory variables in two separate plots in Figure 14.8. ggplot(atlas_mob_jobs_poor, aes(x = ann_avg_job_growth_2004_2013, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Average Job Growth 2003-2014&quot;, y = &quot;Mobility for Children of Parents in P25 (in $)&quot;, title = &quot;Mobility and job growth&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(atlas_mob_jobs_poor, aes(x = poor_share2010, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Proverty Share in 2010&quot;, y = &quot;Mobility for Children of Parents in P25 (in $)&quot;, title = &quot;Mobility and poverty share&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 14.8: Relationship between mobility and job growth/poverty share. Observe there is a weak positive relationship between mobility and job growth: as job growth increases so also does mobility but in a fairly flat rate. This is consistent with the weak positive correlation coefficient of 0.072 we computed earlier. In the case of poverty share, the strong negative relationship appear here, which depicts the -0.544 correlation coefficient that we estimated above. However, the two plots in Figure 14.8 only focus on the relationship of the outcome variable with each of the two explanatory variables separately. To visualize the joint relationship of all three variables simultaneously, we need a 3-dimensional (3D) scatterplot as seen in Figure 14.9. Because there are many datapoints in the atlas data, I subset this to data for the state of North Carolina (state==37). These points in the atlas_mob_jobs_poor data frame are marked with a blue point where The numerical outcome variable \\(y\\) kfr_pooled_p25 is on the vertical axis The two numerical explanatory variables, \\(x_1\\) poor_share2010 and \\(x_2\\) ann_avg_job_growth_2004_2013, are on the two axes that form the bottom plane. Figure 14.9: 3D scatterplot and regression plane. Furthermore, we also include the regression plane. Recall from above that regression lines are “best-fitting” in that of all possible lines we can draw through a cloud of points, the regression line minimizes the sum of squared residuals. This concept also extends to models with two numerical explanatory variables. The difference is instead of a “best-fitting” line, we now have a “best-fitting” plane that similarly minimizes the sum of squared residuals. Head to here to open an interactive version of similar plane (using different data) in your browser. 14.3.2 Regression plane Let’s now fit a regression model and get the regression table corresponding to the regression plane in Figure 14.9. We’ll consider a model fit with a formula of the form y ~ x1 + x2, where x1 and x2 represent our two explanatory variables ann_avg_job_growth_2004_2013 and poor_share2010. Just like we did in Chapter 14, let’s get the regression table for this model using our two-step process and display the results in Table 14.9. We first “fit” the linear regression model using the lm(y ~ x1 + x2, data) function and save it in mob_model2. We get the regression table by applying the summary() function to mob_model2. # Fit regression model: mob_model2 &lt;- lm(kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013 + poor_share2010, data = atlas_mob_jobs_poor) # Get regression table: summary(mob_model2)$coefficients Table 14.9: Table 14.10: Multiple regression table Estimate Std. Error t value Pr(&gt;&amp;#124;t&amp;#124;) (Intercept) 39714 41.4 959.1 0 ann_avg_job_growth_2004_2013 3684 345.4 10.7 0 poor_share2010 -36371 214.1 -169.8 0 Let’s interpret the three values in the Estimate column. First, the Intercept value is -$39,714. This intercept represents the mobility for children of parents in p25 in neighborhoods in which ann_avg_job_growth_2004_2013 is 0% and poor_share2010 of 0%. In our data however, the intercept has limited practical interpretation since no neighborhood had ann_avg_job_growth_2004_2013 or poor_share2010 values of 0%. Rather, the intercept is used to situate the regression plane in 3D space. Second, the ann_avg_job_growth_2004_2013 value is $3,684. Taking into account all the other explanatory variables in our model, if you increase job growth in a neighborhood from 0% to 100% (ann_avg_job_growth_2004_2013 from 0 to 1 unit), there is an associated increase of on average $3,684 in economic mobility for p25. Notice once more, that this is equivalent to saying that increasing job growth by 1% has an associated average increase of mobility of $36.84 - which definitely seems like a small amount. Just as we did in Subsection 14.1.2, we are cautious to not imply causality as we saw in Subsection 14.2.2 that “correlation is not necessarily causation.” We do this merely stating there was an associated increase. Furthermore, we preface our interpretation with the statement “taking into account all the other explanatory variables in our model.” Here, by all other explanatory variables we mean poor_share2010. We do this to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model at the same time. Third, poor_share2010 = -$36,371. Taking into account all the other explanatory variables in our model, for every increase of one unit in the variable poor_share2010, in other words decreasing the share poor in a neighborhood from 0% to 100%, there is an associated decrease of on average $36,371 in mobility. Once more, this is equivalent to saying that a reduction of 1% in the share poor is associated with an increase of $363.71 in economic mobility. Putting these results together, the equation of the regression plane that gives us fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{mobility}}\\) is: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2\\\\ \\widehat{\\text{mobility}} &amp;= b_0 + b_{\\text{avg job growth}} \\cdot \\text{avg job growth} + b_{\\text{pov rate}} \\cdot \\text{pov rate}\\\\ &amp;= 39714 + 3684 \\cdot\\text{avg job growth} - 36371 \\cdot\\text{pov rate} \\end{aligned} \\] Recall in the right-hand plot of Figure 14.8 that when plotting the relationship between kfr_pooled_p25 and poor_share2010 in isolation, there appeared to be a positive relationship of. In the last discussed multiple regression however, when jointly modeling the relationship between kfr_pooled_p25, ann_avg_job_growth_2004_2013, and poor_share2010, there appears to be a weaker positive relationship of kfr_pooled_p25 and ann_avg_job_growth_2004_2013 as evidenced by the slope for ann_avg_job_growth_2004_2013 of $3,684 versus the initial $7,783. 14.4 How to read p-values Now that you know what the coefficients in a regression table mean, let’s briefly discuss now how to asses if the relationship you estimated (the coefficients) estimated with sufficient precision or not. Let’s recap our latest regression table. We had this: summary(mob_model2)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39714 41.4 959.1 0.0000000000000000000000000000 ## ann_avg_job_growth_2004_2013 3684 345.4 10.7 0.0000000000000000000000000152 ## poor_share2010 -36371 214.1 -169.8 0.0000000000000000000000000000 We will now focus on the last column of these tables - the one that says Pr(&gt;|t|). These numbers are called p-values. This is a big topic and I will not do it justice. In fact, I will be very loose in this explanation as this is not a course in statistics, but I do want to give you the intuition for how to read it. If you want to dig more, please see here and here. In essence, when we run a regression like the one above, we usually have at hand a sample of observations out of a universe. For example, in the atlas instance, we do not have information on all the neighborhoods in the US, although we do have most of them in the data. But that means that we could have ended with a different set of observations (or neighborhoods) in an alternative world that is slightly different from the one we actually see. To assess how much of an issue this is, we use p-values. I will not get into any details of how you can calculate p-values, but the idea is fairly simple: a p-value is the probability that you would observe data that is as extreme as the data you do if, in fact, the true slope (or coefficient) was zero. Put another way, if we imagine that there is absolutely no real relationship between our variable(s) \\(x\\) and our outcome \\(y\\), how often would we find data that looks like this or even more extreme in the apparent relationship? The answer is the p-value. Why is that useful? Because if the p-value is sufficiently low it is a very simple way to assess our confidence in the slope we are estimating. Important! p-values: probability that you would observe data that is as extreme as the data you do if, in fact, the true slope (or coefficient) were zero. interpretation: rule of thumb is to say that numbers below 0.05 are evidence that the slope is not produced by random chance. 14.4.1 An Example Let’s present our latest regression table for the multivariate case once more. We had this table: summary(mob_model2)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39714 41.4 959.1 0.0000000000000000000000000000 ## ann_avg_job_growth_2004_2013 3684 345.4 10.7 0.0000000000000000000000000152 ## poor_share2010 -36371 214.1 -169.8 0.0000000000000000000000000000 Notice that the p-values for all three estimates (the intercept, and our coefficients for ann_avg_job_growth_2004_2013 and poor_share2010) are 0. What does that mean? Well, that means that it would be very very unlikely that we could get data like the one we have (or more extreme) if the intercept was 0, or if the coefficient of ann_avg_job_growth_2004_2013 were 0, or if the coefficient for poor_share2010 was 0. That is, in a loose way, we believe these estimates are not just coincidence. If those numbers would be higher than 0.05 we should perhaps trust those numbers a bit less, as there would be a higher probability that they were produced almost by chance. 14.5 Conclusion 14.5.1 Additional resources As we suggested in Subsection 14.1.1, interpreting coefficients that are not close to the extreme values of -1, 0, and 1 can be somewhat subjective. To help develop your sense of correlation coefficients, we suggest you play the following 80’s-style video game called “Guess the correlation” at http://guessthecorrelation.com/. "],["rmarkdown.html", "Chapter 15 RMarkdown / Quarto output 15.1 How to create an output file 15.2 Add new code 15.3 Add a title 15.4 Add an image 15.5 Submit your work", " Chapter 15 RMarkdown / Quarto output How do you put normal writing, analysis, code and output into one file in a simple way? Enter RMarkdown/Quarto! Throughout this semester you will deliver your projects in the form of a RMarkdown or Quarto file. There are several cool options when you use this type of output, but I will only briefly describe what you need submit your work. You are encourage to dig deeper if you want to here for RMarkdown and here for Quarto. I will focus here on Quarto for practical purposes. RMarkdown and Quarto are almost the same thing for our purposes. Quarto is a new tool that the folks from RStudio developed, but it uses pretty much the same format as RMarkdown, so I will use them interchangably. 15.1 How to create an output file There are several options when creating a Quarto file. You would go to the New file symbol (white page with green plus), and select the Quarto Document option Then you select the title for the project you want to create, and add your name for the authorship. Select the PDF option (instead of the HTML), but leave all other default options unchanged. Then click Create You should have a new Quarto document now. Save it a name by clicking on the floppy disk. Now just try to render the file. This will create the final PDF document that you need to deliver. To do that, click on Render (it as a blue arrow on the top). A new tab or window should open showing you the PDF document. It is possible you see a window like the one below, but in that case, just click on ‘Try Again’. Alternatively, you should be able to see the PDF on the Files tab on the right of the screen with the same name as the document you saved, but with a ‘.pdf’ extension. 15.1.1 Error when rendering file Maybe you clicked on Render and got this message in the **Background Jobs” tab, by the console: Error in loadNamespace(x) : there is no package called ‘jsonlite’ Calls: .main ... loadNamespace -&gt; withRestarts -&gt; withOneRestart -&gt; doWithOneRestart Execution halted R installation: Version: 4.2.1 Path: /opt/R/4.2.1/lib/R LibPaths: - /cloud/lib/x86_64-pc-linux-gnu-library/4.2 - /opt/R/4.2.1/lib/R/library rmarkdown: (None) The rmarkdown package is not available in this R installation. Install with install.packages(&quot;rmarkdown&quot;) If that is the case, it’s because the package rmarkdown is not installed. There are a couple of options here. First, if you see a yellow messaage on top of your document, saying that “Package rmarkdown required but not installed.”, you can just click in “Install”, wait for it to install, and render the document again. Alternatively, you can go to the console and install that package manually by running: install.packages(&quot;rmarkdown&quot;) Now click on Render once more. It should work, and you should see a new html window with your document. Once you Render you should see a new pop up window with the final pdf document, in which this example, would be something like this: Once more, remember you will be able to see the final pdf document on the bottom right pane called ‘Files’ with the same name as the main Quarto Document. 15.2 Add new code You add code by adding ‘chunks’ of code. You do that by clicking on Insert, then Code Chunk, and then R. Also, notice that if you add to the first line of the chunk a line like this #| echo:false, then when you render you will not see the code, but only the output of the code. This is convenient when the output is too long or distracting, or when you don’t want to show the code necessarily. 15.3 Add a title If you want to add a new title, just type in the main text the following ## (notice the space), and that will give you a Header (or title). You can get subtitles by typing three instead of two pound symbols: ###. 15.4 Add an image What if you want to add an image? For example, you might have downloaded an image like this from the Opportunity Atlas website: How do you add it? Well, you need to do it in two steps. The first, is to upload the image to the directory. To do that, go to the Files tab, and click on Upload, and select your image: You should now see your image in the Files tab. Now, you can add the image to the document by clicking on the image symbol on the top (close to the Insert option). Then browse and select the image you just uploaded, add a caption for it, and add it. You should then see it in the main document. 15.5 Submit your work You will submit your pdf files for Project 1 and the other projects through Canvas. "],["appendixA.html", "A Statistical Background A.1 Basic statistical terms", " A Statistical Background A.1 Basic statistical terms A.1.1 Mean The mean, also known as (AKA) the average, is the most commonly reported measure of center. It is commonly called the “average” though this term can be a little ambiguous. The mean is the sum of all of the data elements divided by how many elements there are. If we have \\(n\\) data points, the mean is given by: \\[\\overline{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] Note that you will often see shorthand notation for a sum of numbers using \\(\\sum\\) notation. For example, we could rewrite the formula for \\(\\bar{x}\\) as: \\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}=\\frac{\\sum_{i = 1}^n x_i}{n}\\] We’ve simply replaced the subscript on each \\(x\\) with a generic index \\(i\\), and use the \\(\\sum\\) notation to indicate we are summing \\(x\\)s with indices (i.e. subscripts) that go from 1 to \\(n\\). When summing numbers in statistics, we’re almost always dealing with indices that start with the value 1 and go up to a value equal to a sample size (e.g. \\(n\\)), so often you will see an even more shorthand version, where it’s assumed you’re summing from \\(i = 1\\) to \\(i = n\\): \\[\\bar{x} = \\frac{\\sum x_i}{n}\\] A.1.2 Median The median is calculated by first sorting a variable’s data from smallest to largest. After sorting the data, the middle element in the list is the median. If the middle falls between two values, then the median is the mean of those two values. A.1.3 Standard deviation We will next discuss the standard deviation of a sample dataset pertaining to one variable. The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far to expect a given data value is from its mean: \\[Standard \\, deviation = \\sqrt{\\frac{(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + \\cdots + (x_n - \\overline{x})^2}{n - 1}} = \\sqrt{\\frac{\\sum_{i= 1}^n(x_i - \\overline{x})^2 }{n - 1}}\\] A.1.4 Five-number summary The five-number summary consists of five values: minimum, first quartile AKA 25th percentile, second quartile AKA median AKA 50th percentile, third quartile AKA 75th, and maximum. The quartiles are calculated as first quartile (\\(Q_1\\)): the median of the first half of the sorted data third quartile (\\(Q_3\\)): the median of the second half of the sorted data The interquartile range is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values is. The five-number summary is not influenced by the presence of outliers in the ways that the mean and standard deviation are. It is, thus, recommended for skewed datasets. A.1.5 Percentiles or rank Just as you can sort the values of a variable form smallest to larges and find the middle element to get the median, you are dividing all the values into two groups with the same number of observations. If you do that but instead of dividing into 2 groups, you divide it into 4 groups, you get each of the quartiles. If you divide the sorted observations into 100 groups of equal number of observations, you get the percentiles. Depending on the application, the percentile can be read as the rank in the distribution. A.1.6 Distribution The distribution of a variable/dataset corresponds to generalizing patterns in the dataset. It often shows how frequently elements in the dataset appear. It shows how the data varies and gives some information about where a typical element in the data might fall. Distributions are most easily seen through data visualization. A.1.7 Outliers Outliers correspond to values in the dataset that fall far outside the range of “ordinary” values. In regards to a boxplot (by default), they correspond to values below \\(Q_1 - (1.5 * IQR)\\) or above \\(Q_3 + (1.5 * IQR)\\). Note that these terms (aside from Distribution) only apply to quantitative variables. "],["project1.html", "Project 1 Stories from the Atlas: Describing Data using Maps, Regressions, and Correlations Instructions Data Description Cheatsheat commands", " Project 1 Stories from the Atlas: Describing Data using Maps, Regressions, and Correlations Posted: Wednesday, August 28, 2024 Part 1: Due at midnight on Tuesday , September 11, 2024 Part 2: Due at midnight on Tuesday , September 17, 2024 The Opportunity Atlas is a freely available interactive mapping tool that traces the roots of outcomes such as poverty and incarceration back to the neighborhoods in which children grew up. Once you are on the main landing page, select Module 1 (Neighborhoods - MOBILITY OUTCOMES). Policymakers, journalists, and the public have begun to explore the Opportunity Atlas, casting new light on the geography of upward mobility in communities across the country. As an example, see Jasmine Garsd’s analysis for the New York City neighborhood of Brownsville in Brooklyn. In this first empirical project, you will use the Opportunity Atlas mapping tool and the underlying data to describe equality of opportunity in your hometown and across the United States. (If you grew up outside the United States, you may select a community in which you have spent some time, such as San Antonio, TX.) The end product will be a short narrative (or story) in which you describe what you have learned from the Atlas, in which you will weave in the narrative along with the data analysis. Below is a list of specific analyses and questions that your narrative must address. The document should contain references, graphs, and maps. This project focuses on the following methods for descriptive data analysis. (The later empirical projects you will do in this class will be focused on causal inference and prediction). Data visualization. Maps are a powerful way to present descriptive statistics for data with a geographic component. You will use maps to display upward mobility statistics for the Census tracts in your hometown. Regression and correlation analysis. You will use linear regressions and correlation coefficients to quantify the statistical relationship between upward mobility and potential explanatory variables. The R data file that you will use in this assignment, atlas.rds, contains an extract of the Opportunity Atlas data. It is also merged on several other variables, which you may use for the correlational analysis. You can load the data by using the usual line: atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_fall2023/master/data/atlas.rds&quot;))) Instructions You will work on RStudio through Posit Cloud for this project. Write the narrative within a Quarto file in the project1 tab in Posit Cloud, and upload the final pdf document to Canvas. The deliverables for Part 1 and Part 2 are the following: Part 1: points 1 to 5 Part 2: points 1 to 10 - this should include any feedback you received for Part 1 Notice that Part 2 includes Part 1 I created a Section to help you get up to speed with Quarto/RMarkdown here. The Quarto/RMarkdown file is where you will write your narrative, run all your analysis, and the output of each code block should be visible. The output should include references, graphs, maps, and tables. Specific questions to address in your narrative Start by looking up the city where you grew up on the Opportunity Atlas. Zoom in to the Census tracts around your home. Figure 1 in your narrative should be a map of the Census tracts in your hometown from the Opportunity Atlas. Examples for Milwaukee, WI (where Professor Chetty grew up) and Los Angeles, CA (discussed in Lecture 1) are shown on the next page. The text of your narrative should describe what you see, and what data are being visualized. Examine the patterns for a number of different groups (e.g., lowest income children, high income children) and outcomes (e.g., earnings in adulthood, incarceration rates). Only choose one or two of these to include in your narrative. (To answer this question, read the Opportunity Atlas manuscript) What period do the data you are analyzing come from? Are you concerned that the neighborhoods you are studying may have changed for kids now growing up there? What evidence do Chetty et al. (2018) provide suggesting that such changes are or are not important? What type of data could you use to test whether your neighborhood has changed in recent years? Now turn to the atlas.rds data set. How does average upward mobility, pooling races and genders, for children with parents at the 25th percentile (kfr pooled_p25) in your home Census tract compare to mean (population-weighted, using count_pooled) upward mobility in your state and in the U.S. overall? Do kids where you grew up have better or worse chances of climbing the income ladder than the average child in America? Hint: The Opportunity Atlas website will give you the tract, county, and state FIPS codes for your home address. For example, searching for “Lynwood Road, Verona, New Jersey” will display Tract 34013021000, Verona, NJ. The first two digits refer to the state code, the next three digits refer to the county code, and the last 6 digits refer to the tract code. In R, you can list these observations with the function filter() as follows (assuming you called the data as atlas as in Table 2). If you only want to see kfr_pooled_p25: atlas_lynwood&lt;-atlas%&gt;% filter(state == 34 &amp; county == 013 &amp; tract == 021000) atlas_lynwood%&gt;% select(kfr_pooled_p25) Or to see all the variables for that tract: View(atlas_lynwood) See the Cheat-sheet Section below for further help. What is the standard deviation of upward mobility (population-weighted) in your home county? Is it larger or smaller than the standard deviation across tracts in your state? Across tracts in the country? What do you learn from these comparisons? Now let’s turn to downward mobility: repeat questions (3) and (4) looking at children who start with parents at the 75th and 100th percentiles. How do the patterns differ? Using a linear regression, estimate the relationship between outcomes of children at the 25th and 75th percentile for the Census tracts in your home county. Generate a scatter plot to visualize this regression. Do areas where children from low-income families do well generally have better outcomes for those from high-income families, too? Next, examine whether the patterns you have looked at above are similar by race. If there is not enough racial heterogeneity in the area of interest (i.e., data is missing for most racial groups), then choose a different area to examine. Using the Census tracts in your home county, can you identify any covariates which help explain some of the patterns you have identified above? Some examples of covariates you might examine include housing prices, income inequality, fraction of children with single parents, job density, etc. For 2 or 3 of these, report estimated correlation coefficients along with their 95% confidence intervals. Open question: formulate a hypothesis for why you see the variation in upward mobility for children who grew up in the Census tracts near your home and provide correlational evidence testing that hypothesis. For this question, many covariates have been provided to you in the atlas.rds file, which are described in the Table under the Data Description section. Putting together all the analyses you did above, what have you learned about the determinants of economic opportunity where you grew up? Identify one or two key lessons or takeaways that you might discuss with a policymaker or journalist if asked about your hometown. Mention any important caveats to your conclusions; for example, can we conclude that the variable you identified as a key predictor in the question above has a causal effect (i.e., changing it would change upward mobility) based on that analysis? Why or why not? Figure A.1: Household Income in Adulthood for Children Raised in Low-Income Households in Milwaukee, WI Notes: This figure shows household income at ages 31-37 for low income children who grew up in Census tracts near Milwaukee, WI. The image was saved from www.opportunity-atlas.org by first searching for “Milwaukee, WI” and then clicking on the “download as image” button. Figure A.2: Incarceration Rates for Black Men Raised in the Lowest-Income Households in Los Angeles, CA Notes: This figure is from the non-technical summary of the Opportunity Atlas and was discussed in Section @ref(lec1_geomobility). Data Description The data consist of n = 73,278 U.S. Census tracts. For more details on the construction of the variables included in this data set, please see Chetty, Raj, John Friedman, Nathaniel Hendren, Maggie R. Jones, and Sonya R. Porter. 2018. “The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility.”, NBER Working Paper No. 25147. Table 1 Definitions of Variables in atlas.rds Variable name Label Obs. (1) (2) (3) 1. Geographic identifiers tract Tract FIPS Code (6-digit) 2010 73,278 county County FIPS Code (3-digit) 73,278 state State FIPS Code (2-digit) 73,278 cz Commuting Zone Identifier (1990 Definition) 72,473 2. Characteristics of Census tracts hhinc_mean2000 Mean Household Income 2000 72,302 mean_commutetime2000 Average Commute Time of Working Adults in 2000 72,313 frac_coll_plus2010 Fraction of Residents with a College Degree or More in 2010 72,993 frac_coll_plus2000 Fraction of Residents with a College Degree or More in 2000 72,343 foreign_share2010 Share of Population Born Outside the U.S. 72,279 med_hhinc2016 Median Household Income in 2016 72,763 med_hhinc1990 Median Household Income in 1999 72,313 popdensity2000 Population Density (per square mile) in 2000 72,469 poor_share2010 Poverty Rate 2010 72,933 poor_share2000 Poverty Rate 2000 72,315 poor_share1990 Poverty Rate 1990 72,323 share_black2010 Share black 2010 73,111 share_hisp2010 Share Hispanic 2010 73,111 share_asian2010 Share Asian 2010 71,945 share_black2000 Share black 2000 72,368 share_white2000 Share white 2000 72,368 share_hisp2000 Share Hispanic 2000 72,368 share_asian2000 Share Asian 2000 71,050 gsmn_math_g3_2013 Average School District Level Standardized Test Scores in 3rd Grade in 2013 72,090 rent_twobed2015 Average Rent for Two-Bedroom Apartment in 2015 56,607 singleparent_share2010 Share of Single-Headed Households with Children 2010 72,564 singleparent_share1990 Share of Single-Headed Households with Children 1990 72,196 singleparent_share2000 Share of Single-Headed Households with Children 2000 72,285 traveltime15_2010 Share of Working Adults w/ Commute Time of 15 Minutes Or Less in 2010 72,939 emp2000 Employment Rate 2000 72,344 mail_return_rate2010 Census Form Rate Return Rate 2010 72,547 ln_wage_growth_hs_grad Log wage growth for HS Grad., 2005-2014 51,635 jobs_total_5mi_2015 Number of Primary Jobs within 5 Miles in 2015 72,311 jobs_highpay_5mi_2015 Number of High-Paying (&gt;USD40,000 annually) Jobs within 5 Miles in 2015 72,311 nonwhite_share2010 Share of People who are not white 2010 73,111 popdensity2010 Population Density (per square mile) in 2010 73,194 ann_avg_job_growth_2004_2013 Average Annual Job Growth Rate 2004-2013 70,664 job_density_2013 Job Density (in square miles) in 2013 72,463 3. Measures of Upward Mobility from the Opportunity Atlas kfr_pooled_p25 Household income ($) at age 31-37 for children with parents at the 25th percentile of the national income distribution 72,011 kfr_pooled_p75 Household income ($) at age 31-37 for children with parents at the 75th percentile of the national income distribution 72,012 kfr_pooled_p100 Household income ($) at age 31-37 for children with parents at the 100th percentile of the national income distribution 71,968 kfr_natam_p25 Household income ($) at age 31-37 for Native American children with parents at the 25th percentile of the national income distribution 1,733 kfr_natam_p75 Household income ($) at age 31-37 for Native American children with parents at the 75th percentile of the national income distribution 1,728 kfr_natam_p100 Household income ($) at age 31-37 for Native American children with parents at the 100th percentile of the national income distribution 1,594 kfr_asian_p25 Household income ($) at age 31-37 for Asian children with parents at the 25th percentile of the national income distribution 15,434 kfr_asian_p75 Household income ($) at age 31-37 for Asian children with parents at the 75th percentile of the national income distribution 15,360 kfr_asian_p100 Household income ($) at age 31-37 for Asian children with parents at the 100th percentile of the national income distribution 13,480 kfr_black_p25 Household income ($) at age 31-37 for Black children with parents at the 25th percentile of the national income distribution 34,086 kfr_black_p75 Household income ($) at age 31-37 for Black children with parents at the 75th percentile of the national income distribution 34,049 kfr_black_p100 Household income ($) at age 31-37 for Black children with parents at the 100th percentile of the national income distribution 32,536 kfr_hisp_p25 Household income ($) at age 31-37 for Hispanic children with parents at the 25th percentile of the national income distribution 37,611 kfr_hisp_p75 Household income ($) at age 31-37 for Hispanic children with parents at the 75th percentile of the national income distribution 37,579 kfr_hisp_p100 Household income ($) at age 31-37 for Hispanic children with parents at the 100th percentile of the national income distribution 35,987 kfr_white_p25 Household income ($) at age 31-37 for white children with parents at the 25th percentile of the national income distribution 67,978 kfr_white_p75 Household income ($) at age 31-37 for white children with parents at the 75th percentile of the national income distribution 67,968 kfr_white_p100 Household income ($) at age 31-37 for white children with parents at the 100th percentile of the national income distribution 67,627 3. Counts of number of children under 18 in 2000 (to calculate weighted summary statistics) count_pooled Count of all children 72,451 count_white Count of White children 72,451 count_black Count of Black children 72,451 count_asian Count of Asian children 72,451 count_hisp Count of Hispanic children 72,451 count_natam Count of Native American children 72,451 Cheatsheat commands R command Description Here I present a summary of the commands you could use to work on this project. There are two important issues you should keep in mind while reading this: Notice that whenever you see yvar this is not a real variable. It is only a place holder for the appropriate variable that you decide to analyze or use. For example, if you want to see the mean across neighborhoods of the average household income as measured in 2000, you would not do mean(atlas$yvar, na.rm=TRUE) but mean(atlas$hhinc_mean2000, na.rm=TRUE). Important! ‘yvar’ is not a real variable. You should replace it for the appropriate variable in your code. The data atlas has missing information for some neighborhoods for some variables. These are called NA or missing. Most R functions do not like that you include missings in the function, because R does not know what to do with that. What is 5+NA ? NA !! So, for many of these functions, we will explicitely tell R to ignore NAs. That is what the option na.rm=TRUE does. It does not not exist for every function, but it does for most of the ones we will use here. Important! Careful with missing values (also called ‘NA’)! We will use ‘na.rm=TRUE’ as an option for several functions to tell R to ignore the missings. Unweighted summary statistics summary(atlas$yvar) mean(atlas$yvar, na.rm=TRUE) sd(atlas$yvar, na.rm=TRUE) Load package If you wanted to install and open the package Hmisc (which you will need to calculate the weighted statistics), run: install.packages(&quot;Hmisc&quot;) library(Hmisc) Weighted summary statistics You can weight means or other statistics. In our case, we want to use the population weighted statistics in several cases. That is, we want to put more weight on the value of a tract in which more people live than in another with lower population. Recall that the population variable is count_pooled. Weighted mean: wtd.mean(atlas$yvar, atlas$count_pooled) Weighted standard deviation: sqrt(wtd.var(atlas$yvar, atlas$count_pooled)) Subset observations State level: If you want to select a subset of observations, you can add the rule for selecting those observations, and the filter function. Here we subset the observations for the State of Wisconsin, and called the resulting dataset atlas_wisconsin. atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) County level: We can do the same but now for a specific county, adding an extra rule after a comma , or &amp;. Here we subset the observations for Milwaukee County in Wisconsin: atlas_milwaukee &lt;- atlas %&gt;% filter(state == 55 &amp; county == 079) Standardize variables You can standardize variables by substracting the mean and dividing by the standard deviation. Let us say that you want to standardize only considering the variables in Milwaukee, then you can do this: atlas_milwaukee&lt;- atlas_milwaukee %&gt;% mutate(x_std=(xvar - mean(atlas_milwaukee$xvar, na.rm=TRUE))/sd(atlas_milwaukee$xvar, na.rm=TRUE)) As an example, let’s say you want to standardize both the measure of mobility and the annual job growth for the data in Wisconsin: atlas_wisconsin&lt;- atlas_wisconsin %&gt;% mutate(kfr_pooled_p25_std=(kfr_pooled_p25 - mean(atlas_wisconsin$kfr_pooled_p25, na.rm=TRUE))/sd(atlas_wisconsin$kfr_pooled_p25, na.rm=TRUE), ann_avg_job_growth_2004_2013_std=(ann_avg_job_growth_2004_2013 - mean(atlas_wisconsin$ann_avg_job_growth_2004_2013, na.rm=TRUE))/sd(atlas_wisconsin$ann_avg_job_growth_2004_2013, na.rm=TRUE)) Run regression I have written a whole section explaining regression in more detail: Section 14. Please see that for further details. But here is a quick help. Simple linear regression Let’s say you want to run a simple regression of variable yvar on variable xvar1 for the county of Milwaukee. We will save the results of that regression in an object call mod1 (we could give it any name). Then you would do this: mod1 &lt;- lm(yvar~xvar1, data = atlas_milwaukee) To see what the outcome of the regression is, you would use the function summary and apply it to our new object mod1, like this: summary(mod1) As an example, we could regress the mobility of children from parents in the 25th percentile (kfr_pooled_p25) on the average annual job growth rate between 2004 and 2013 (ann_avg_job_growth_2004_2013). To do that, we would run: mod1 &lt;- lm(kfr_pooled_p25~ann_avg_job_growth_2004_2013, data = atlas_milwaukee) Multivariate linear regression You might want to understand the relationship between yvar and variable xvar1 while holding fixed another variable xvar2 for neighborhoods only in Milwaukee. You can do this: mod2 &lt;- lm(yvar~xvar1+xvar2 + xvar3, data = atlas_milwaukee) How to read the regression output? To simplify the interpretation, let’s run a regression where you use the standardize both the measure of mobility and the annual job growth for the data in Wisconsin: mod3 &lt;- lm(kfr_pooled_p25_std ~ ann_avg_job_growth_2004_2013_std, data = atlas_wisconsin) summary(mod3) ## ## Call: ## lm(formula = kfr_pooled_p25_std ~ ann_avg_job_growth_2004_2013_std, ## data = atlas_wisconsin) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.829 -0.569 0.035 0.684 5.128 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.00000978 0.02681564 0.00 1.000 ## ann_avg_job_growth_2004_2013_std -0.05131843 0.02679889 -1.91 0.056 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.999 on 1386 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.00264, Adjusted R-squared: 0.00192 ## F-statistic: 3.67 on 1 and 1386 DF, p-value: 0.0557 You should focus here on interpreting the coefficient for ann_avg_job_growth_2004_2013_std. You should pay attention to both the magnitude of the number, and the sign. In this case, you would read it like this: In Wisconsin, increasing one standard deviation the average annual job growth rate is correlated with an reduction of 0.05 standard deviations in the economic mobility for children with parents in the 25th percertile. Plotting the linear relationship You need to load the ggplot2 package (which should be installed already). library(ggplot2) Suppose you want to visually see the linear relationship between two variables in the atlas_milwaukee dataset that we filtered above. Then, you can do this: ggplot(data = atlas_milwaukee) + geom_point(aes(x = xvar1, y = yvar)) + geom_smooth(aes(x = xvar1, y = yvar), method = &quot;lm&quot;, se = F) The function geom_smooth() adds the line that you calculated above for mod1, where you ran mod1 &lt;- lm(yvar~xvar1, data = atlas_milwaukee). For more, see Section 12. "],["project2.html", "Project 2 Do Smaller Classes Improve Test Scores? Evidence from a Regression Discontinuity Design Instructions Questions to address in your replication Data Description Cheatsheat commands", " Project 2 Do Smaller Classes Improve Test Scores? Evidence from a Regression Discontinuity Design Posted: Wednesday, August 28, 2024 Due: At midnight on Tuesday, October 11, 2024 In this empirical project, you will use a regression discontinuity design to estimate the causal effect of class size on test scores. To answer some of the questions, you will need to refer to the following papers: Chetty, Raj, John N. Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore Schanzenbach, and Danny Yagan. 2011. “How Does Your Kindergarten Classroom Affect Your Earnings? Evidence from Project STAR,” Quarterly Journal of Economics 126(4): 1593–1660. Angrist, Joshua D., and Victor Lavy. 1999. “Using Maimonides’ Rule to Estimate the Effect of Class Size on Scholastic Achievement,” Quarterly Journal of Economics 114(2): 533–575. The data file grade5.rds consists of test scores in fifth grade classes at public elementary schools in Israel. These data were originally used in Angrist and Lavy (1999). The graphs below were drawn using the same data. Note: These figures plot class size as a function of total school enrollment for fourth grade and fifth grade classes in pubic schools in Israel in 1991 Instructions You will work on Posit Cloud for this project. Write the replication within a Quarto/RMarkdown file in the project2 tab in Posit Cloud. Recall that I created a Section to help you get up to speed with Quarto/RMarkdown here. The Quarto/RMarkdown file is where you will write your report, run all your analysis, and the output of each code block should be visible. The output should include references, graphs, maps, and tables. Questions to address in your replication Explain why a simple comparison of test scores in small classes versus large classes would not measure the causal effect of class size. Would this simple comparison likely be biased upwards or biased downwards relative to that true causal effect? Explain. (To answer this and the next question, read Chetty et al. 2011). How did the Tennessee STAR experiment overcome this problem? What did it find? Figures like Figure 1 above are called binned scatter plots. They are built by averaging the value of a variable at evenly spaced bins of the running variable (the variable for which the discontinuity happens). We will construct a graphical regression discontinuity analysis, focusing on the 40 student school enrollment threshold. See Table in the Data Description Section and the cheat sheet for more guidance. a. Draw a binned scatter plot to visualize how class size changes at the 40 student school enrollment threshold. Display a linear or quadratic regression line based on what you see in the data. b. Draw binned scatter plots to visualize how math and verbal test scores change at the 40 student school enrollment threshold. Display a linear or quadratic regression line based on what you see in the data. c. Draw binned scatter plots to test whether (i) the percent of disadvantaged students, (ii) the fraction of religious schools, and (iii) the fraction of female students evolve smoothly across the 40 student school enrollment threshold. Display a linear or quadratic regression line based on what you see in the data. d. Produce a histogram of the number of schools by total school enrollment. Note that you must collapse the data by school to produce this graph. Regression analysis. Run the regressions that correspond to your three graphs in 3a and 3b to quantify the discontinuities that you see in the data. In estimating these regressions, use all the observations with school enrollment less than 80. Report a 95% confidence interval for each of these estimates. See Table in the Data Description Section and the cheat sheet for more guidance. Recall that any quasi experiment requires an identification assumption to make it as good as an experiment. What is the identification assumption for regression discontinuity design? Explain whether your graphs in 3c and 3d are consistent with that assumption. (To answer this question, read Angrist and Lavy (1999)). If all schools followed the class size rule exactly as described in Angrist and Lavy (1999), how much would you expect class size to change at the 40 student enrollment threshold? Explain why the actual change in class size that you see in the data is less than this. Suppose your school superintendent is considering a reform to reduce class sizes in your school from 40 to 35. Use your estimates above to predict the change in math and verbal test scores that would result from this reform. Hint: divide the RD estimate of the change in test scores by the change in number of students per class at the threshold. Now suppose you are asked for advice by another school that is considering reducing class size from 20 to 15 students – a 5 unit reduction as above. Would you feel confident in making the same prediction as you did above about the impacts this change will have? Why or why not? Compare your estimates in 7 with the estimates from the Tennessee STAR experiment (Chetty et al. 2011). Give two reasons that your estimates might differ from those of these other studies. Chetty et al. (2011) show that being assigned to a smaller class in Kindergarten raises Kindergarten test scores, but has little impact in later grades. Does this “fade out” effect mean that class size doesn’t really matter in the long run? Why or why not? Given the evidence above, would you encourage your hometown school to reduce class size by hiring more teachers if the goal is to maximize students’ long-term outcomes (e.g., college attendance rates, earnings)? Explain clearly what other data you would need to make a scientific recommendation and how you would use that data. Data Description The file grade5.rdsconsist of n = 2,019 fifth grade classes at 1,002 public schools in Israel in 1991. For more details on the construction of the variables included in this data set, please see Angrist and Lavy (1999). Table 1 Definitions of Variables in grade5.rds ** Variable ** ** Label ** (1) (2) schlcode School id code school_enrollment Total school enrollment in fifth grade grade Class grade 5 = fifth grade for all observations in grade5.dta classize Number of students in the class avgmath Average composite year-end math score in the class, on a scale of 1 to 100, from a national elementary school test. avgverb Average composite year-end verbal score in the class, on a scale of 1 to 100, from a national elementary school test. disadvantaged Percent of class coming from a disadvantaged background, as defined by an index used by the Ministry of Education to allocate supplementary hours of instruction and other school resources. The index is based on fathers’ education, fathers’ continent of birth, and family size. female Fraction of students in the class that are female religious 1 = School is a religious public school 0 = School is a secular public school Cheatsheat commands R command Description Here I present a summary of the commands you could use to work on this project. There are two important issues you should keep in mind while reading this: Notice that whenever you see yvar this is not a real variable. As in Project 1, it is only a place holder for the appropriate variable that you decide to analyze or use. Important! ‘yvar’ is not a real variable. You should replace it for the appropriate variable in your code. Packages needed You should use the usual packages for data wrangling and visualization: library(tidyverse) Additionally, we will use the following packages for the estimation of the Regression Discontinuity Design (remember that if this code does not run, you need to install the library first): library(rdrobust) Subset observations If you want to subset the data for schools with enrollment between 20 and 60 and create a dataset from those observations, you would do the following: narrow_data &lt;- grade5%&gt;% filter(school_enrollment &lt;= 60 &amp; school_enrollment &gt;= 20) Draw binned scatterplots For the plot with a linear fit, we will use the function rdplot from the rdrobust package. The following command produces a binned scatter plot of yvar against the total school enrollment with a linear best fit line, restricting the graph to observations with total school enrollment in [20,60]. You can do so by doing this: rdplot(narrow_data$yvar, narrow_data$school_enrollment, c = 40.5, p = 1, nbins = 20) For the plot with a quadratic fit, you can similarly do this, now changing the option p from 1 to 2: rdplot(narrow_data$yvar, narrow_data$school_enrollment, c = 40.5, p = 2, nbins = 20) Collapse or summarize datasets The next two commands show how to create a graph showing the number of schools that have each value of school_enrollment. First, we collapse the data to convert from school-grade level data to school level data. You can create datasets that are based on summaries of other data. For example, to create a dataset by school instead of by grade (as the original grade5.rds is), you can use the function group_by and select the variables that you want to use for the summary, proceeded by the function summarise, in which you include the specific statistic you want to create. Int he following example, we want the average of the variable school_enrollment by school: narrow_data_school &lt;- narrow_data%&gt;% group_by(schlcode)%&gt;% summarise(school_enrollment = mean(school_enrollment, na.rm = TRUE)) Notice that we used the na.rm=TRUE option to avoid any complications coming from missing values. Draw graph of count of schools Second, we draw a graph of the counts of schools, restricting the graph to schools with between 20 and 60 students enrolled. Here we draw a graph of the counts of schools with the package ggplot2 (which is loaded by tidyverse), restricting the graph to schools with between 20 and 60 students enrolled: ggplot(narrow_data_school, aes(school_enrollment)) + geom_histogram(bins = 40) + geom_vline(xintercept=40.5, color = &quot;red&quot;) Estimate the discontinuity These commands show how to run a regression to quantify the discontinuity in yvar at the 40 student threshold. We first subset the data to observations with school_enrollment between 0 and 80. grade_u80 &lt;- grade5%&gt;% filter(school_enrollment &lt;= 80) Next we generate an indicator variable (that is a variable that is either equal to 1 when the condition is true, or 0 when it is not) for school_enrollment being above 40. grade_u80 &lt;- grade5%&gt;% mutate(above40=(school_enrollment &gt; 40)) You can verify that above40 is only equal to 1 when school_enrollment is greater than 40. We then generate a variable that equals school_enrollment minus 40, and the interaction term between this variable and the indicator for school_enrollment being above 40. grade_u80 &lt;- grade_u80%&gt;% mutate(x=school_enrollment - 40, x_above = above40*x) Finally, we run a linear regression of yvar on these three variables, restricting the regression to observations with school_enrollment between 0 and 80 (we will call this model mod1). mod1 &lt;- lm(yvar~above40 + x + x_above, data = grade_u80) summary(mod1) Interpretation The coefficient on x_above is the estimate of the discontinuity in yvar at the threshold. You can also see the p-values in the table to see if there is evidence of a statistical jump or not. Recall that we interpret that there is evidence of such a jump if the p-value is below 0.05. For more on regressions, see Section 14. For more on data visualization, see Section 12. For more on data wrangling, see Section 13. "],["project3.html", "Project 3 Social Capital and Economic Mobility Instructions Data Description Cheatsheat commands", " Project 3 Social Capital and Economic Mobility Posted: Wednesday, August 28, 2024 Due: Midnight on Friday, November 8, 2024 You have already explored the Opportunity Atlas in Project 1. Now you will explore the new data on economic mobility that some of the previous researchers and new ones have made freely available in the Social Capital Atlas. It also provides an interactive mapping tool, as before, that attempts to trace the strength of our relationships and communities. In this empirical project, you will use new data on social capital to estimate it’s relationship to economic mobility. To answer some of the questions you will need to refer to the following papers (both of which are in Blackboard under Supplementary Readings): Chetty, Raj; Jackson, Matthew O; Kuchler, Theresa; Stroebel, Johannes; Hendren, Nathaniel; Fluegge, Robert B; Gong, Sara; Gonzalez, Federico; Grondin, Armelle; Jacob, Matthew; Johnston, Drew; Koenen, Martin; Laguna-Muggenburg, Eduardo; Mudekereza, Florian; Rutter, Tom; Thor, Nicolaj; Townsend, Wilbur; Zhang, Ruby; Bailey, Mike; Barberá, Pablo; Bhole, Monica; Wernerfelt, Nils, 2022. “Social capital I: measurement and associations with economic mobility”, Nature 608 (7921): 108-121 Chetty, Raj; Jackson, Matthew O; Kuchler, Theresa; Stroebel, Johannes; Hendren, Nathaniel; Fluegge, Robert B; Gong, Sara; Gonzalez, Federico; Grondin, Armelle; Jacob, Matthew; Johnston, Drew; Koenen, Martin; Laguna-Muggenburg, Eduardo; Mudekereza, Florian; Rutter, Tom; Thor, Nicolaj; Townsend, Wilbur; Zhang, Ruby; Bailey, Mike; Barberá, Pablo; Bhole, Monica; Wernerfelt, Nils, 2022. “Social capital II: determinants of economic connectedness”, Nature 608 (7921): 122-134 The data file atlas_socialk.rds consists of the same information in atlas that you used for Project 1, but it now includes information from the Social Captal Atlas as well. This version includes the new estimates of social capital described in the papers above at the ZIP level, which in this case have been connected with the tract-level data. You can load the data by using the following code: atlas_socialk &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas_socialk_tract.rds&quot;))) Instructions As usual, you will work on Posit Cloud for this project. Write your responses within a Quarto/RMarkdown here file in the project3 tab in Posit Cloud. Specific questions to address in your narrative Once more, start by looking up the city where you grew up on the Social Capital Atlas. Zoom in to the area around your home (or the one you explored in Project 1). Figure 1 should be a map of the Zip codes in your hometown from the Social Capital Atlas. Figure 2 should be a map of the High Schools in your hometown from the Social Capital Atlas. Lastly, Figure 3 should be a map of the colleges in your hometown from the Social Capital Atlas (if there are none, show how the closest set of colleges look in the map). Describe the data being used by the authors. What period do the data you are analyzing come from? How should we interpret any possible correlation between these measures of social capital and those of mobility (take into account when each dataset is being measured)? Now turn to the atlas_socialk.rds data set. How does economic connectedness (ec_zip), cohesiveness (clustering_zip) and civic engagement (volunteering_rate_zip and civic_organizations_zip) look like in your home Census tract compare to mean (population-weighted, using count_pooled) across the state and the U.S. overall? Hint: Same as before. You can find the tract, county, and state FIPS codes for your home address from the Opportunity Atlas. Recall that it’s usually a good idea to load the data and packages at the very beggining of your Quarto file. You will need to load tidyverse and Hmisc. What is the standard deviation of the different social capital measures (population-weighted) in your home county? Is it larger or smaller than the standard deviation across tracts in your state? Across tracts in the country? What do you learn from these comparisons? Using a linear regression, estimate the relationship between economic connectedness (ec_zip) and economic mobility (kfr_pooled_p25) across the US and in your state. Generate a scatter plot to visualize this regression. Interpret what you find. In particular, is the relationship statistically significant? Would you conclude that there is a causal effect between the variables? Why? How does the results in (5) change if you consider ec_high_zip instead? How do you interpret this difference? What happens to the results in (5) if you consider adjusting for other covariates? Identify 2 or 3 additional covariates which could explain mobility differences and include them in a multiple regression which includes ec_zip. Some examples of covariates you might examine include housing prices, income inequality, fraction of children with single parents, job density, etc. For 2 or 3 of these, report estimated correlation coefficients along with their 95% confidence intervals. Putting together all the analyses you did above, what have you learned about the relationship between social capital and economic opportunity? Mention any important caveats to your conclusions; for example, can we conclude that the variable you identified as a key predictor in the question above has a causal effect (i.e., changing it would change upward mobility) based on that analysis? Why or why not? Data Description The data consist of n = 73,278 U.S. Census tracts. For more details on the construction of the variables included in this data set, please see Chetty, Raj, John Friedman, Nathaniel Hendren, Maggie R. Jones, and Sonya R. Porter. 2018. “The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility.”, NBER Working Paper No. 25147. Table 1 Definitions of Variables in atlas.rds Variable name Label Obs. (1) (2) (3) 1. Geographic identifiers tract Tract FIPS Code (6-digit) 2010 73,278 county County FIPS Code (3-digit) 73,278 state State FIPS Code (2-digit) 73,278 cz Commuting Zone Identifier (1990 Definition) 72,473 2. Characteristics of Census tracts hhinc_mean2000 Mean Household Income 2000 72,302 mean_commutetime2000 Average Commute Time of Working Adults in 2000 72,313 frac_coll_plus2010 Fraction of Residents with a College Degree or More in 2010 72,993 frac_coll_plus2000 Fraction of Residents with a College Degree or More in 2000 72,343 foreign_share2010 Share of Population Born Outside the U.S. 72,279 med_hhinc2016 Median Household Income in 2016 72,763 med_hhinc1990 Median Household Income in 1999 72,313 popdensity2000 Population Density (per square mile) in 2000 72,469 poor_share2010 Poverty Rate 2010 72,933 poor_share2000 Poverty Rate 2000 72,315 poor_share1990 Poverty Rate 1990 72,323 share_black2010 Share black 2010 73,111 share_hisp2010 Share Hispanic 2010 73,111 share_asian2010 Share Asian 2010 71,945 share_black2000 Share black 2000 72,368 share_white2000 Share white 2000 72,368 share_hisp2000 Share Hispanic 2000 72,368 share_asian2000 Share Asian 2000 71,050 gsmn_math_g3_2013 Average School District Level Standardized Test Scores in 3rd Grade in 2013 72,090 rent_twobed2015 Average Rent for Two-Bedroom Apartment in 2015 56,607 singleparent_share2010 Share of Single-Headed Households with Children 2010 72,564 singleparent_share1990 Share of Single-Headed Households with Children 1990 72,196 singleparent_share2000 Share of Single-Headed Households with Children 2000 72,285 traveltime15_2010 Share of Working Adults w/ Commute Time of 15 Minutes Or Less in 2010 72,939 emp2000 Employment Rate 2000 72,344 mail_return_rate2010 Census Form Rate Return Rate 2010 72,547 ln_wage_growth_hs_grad Log wage growth for HS Grad., 2005-2014 51,635 jobs_total_5mi_2015 Number of Primary Jobs within 5 Miles in 2015 72,311 jobs_highpay_5mi_2015 Number of High-Paying (&gt;USD40,000 annually) Jobs within 5 Miles in 2015 72,311 nonwhite_share2010 Share of People who are not white 2010 73,111 popdensity2010 Population Density (per square mile) in 2010 73,194 ann_avg_job_growth_2004_2013 Average Annual Job Growth Rate 2004-2013 70,664 job_density_2013 Job Density (in square miles) in 2013 72,463 3. Measures of Upward Mobility from the Opportunity Atlas kfr_pooled_p25 Household income ($) at age 31-37 for children with parents at the 25th percentile of the national income distribution 72,011 kfr_pooled_p75 Household income ($) at age 31-37 for children with parents at the 75th percentile of the national income distribution 72,012 kfr_pooled_p100 Household income ($) at age 31-37 for children with parents at the 100th percentile of the national income distribution 71,968 kfr_natam_p25 Household income ($) at age 31-37 for Native American children with parents at the 25th percentile of the national income distribution 1,733 kfr_natam_p75 Household income ($) at age 31-37 for Native American children with parents at the 75th percentile of the national income distribution 1,728 kfr_natam_p100 Household income ($) at age 31-37 for Native American children with parents at the 100th percentile of the national income distribution 1,594 kfr_asian_p25 Household income ($) at age 31-37 for Asian children with parents at the 25th percentile of the national income distribution 15,434 kfr_asian_p75 Household income ($) at age 31-37 for Asian children with parents at the 75th percentile of the national income distribution 15,360 kfr_asian_p100 Household income ($) at age 31-37 for Asian children with parents at the 100th percentile of the national income distribution 13,480 kfr_black_p25 Household income ($) at age 31-37 for Black children with parents at the 25th percentile of the national income distribution 34,086 kfr_black_p75 Household income ($) at age 31-37 for Black children with parents at the 75th percentile of the national income distribution 34,049 kfr_black_p100 Household income ($) at age 31-37 for Black children with parents at the 100th percentile of the national income distribution 32,536 kfr_hisp_p25 Household income ($) at age 31-37 for Hispanic children with parents at the 25th percentile of the national income distribution 37,611 kfr_hisp_p75 Household income ($) at age 31-37 for Hispanic children with parents at the 75th percentile of the national income distribution 37,579 kfr_hisp_p100 Household income ($) at age 31-37 for Hispanic children with parents at the 100th percentile of the national income distribution 35,987 kfr_white_p25 Household income ($) at age 31-37 for white children with parents at the 25th percentile of the national income distribution 67,978 kfr_white_p75 Household income ($) at age 31-37 for white children with parents at the 75th percentile of the national income distribution 67,968 kfr_white_p100 Household income ($) at age 31-37 for white children with parents at the 100th percentile of the national income distribution 67,627 3. Counts of number of children under 18 in 2000 (to calculate weighted summary statistics) count_pooled Count of all children 72,451 count_white Count of White children 72,451 count_black Count of Black children 72,451 count_asian Count of Asian children 72,451 count_hisp Count of Hispanic children 72,451 count_natam Count of Native American children 72,451 4. Measures of Social Capital ec_zip Baseline definition of economic connectedness: two times the share of high-SES friends among low-SES individuals, averaged over all low-SES individuals in the ZIP code. See equations (1), (2), and (3) of Chetty et al. (2022a) for a formal definition. 71,516 ec_high_zip Economic connectedness for high-SES individuals: two times the share of high-SES friends among high-SES individuals, averaged over all high-SES individuals in the ZIP code. 71,516 clustering_zip The average fraction of an individual’s friend pairs who are also friends with each other. See equations (4) and (5) of Chetty et al. (2022a). They include links to people outside the ZIP code when calculating individual clustering (equation 4), but only average individual clustering over users in the relevant ZIP code to compute clustering at the ZIP code level (equation 5). 71,950 volunteering_rate_zip The percentage of Facebook users who are members of a group which is predicted to be about ‘volunteering’ or ‘activism’ based on group title and other group characteristics. We do not include groups that have the privacy setting ‘secret’ enabled. We additionally manually review the 50 largest such groups in the United States and the largest group in each state, and remove the very small number of groups that are clearly misclassified. 71,950 civic_organizations_zip The number of Facebook Pages predicted to be “Public Good” pages based on page title, category, and other page characteristics, per 1,000 users in the ZIP code. They remove pages that do not have a website linked, do not have a description on their Facebook page or do not have an address listed. We then assign the page to a ZIP code on the basis of its listed address. 71,938 To see all other social capital variables not defined above, see here. Cheatsheat commands R command Description Here I present a summary of the commands you could use to work on this project. There are two important issues you should keep in mind while reading this: Notice that whenever you see yvar this is not a real variable. It is only a place holder for the appropriate variable that you decide to analyze or use. For example, if you want to see the mean across neighborhoods of the average household income as measured in 2000, you would not do mean(atlas$yvar, na.rm=TRUE) but mean(atlas$hhinc_mean2000, na.rm=TRUE). Important! ‘yvar’ is not a real variable. You should replace it for the appropriate variable in your code. The data atlas has missing information for some neighborhoods for some variables. These are called NA or missing. Most R functions do not like that you include missings in the function, because R does not know what to do with that. What is 5+NA ? NA !! So, for many of these functions, we will explicitely tell R to ignore NAs. That is what the option na.rm=TRUE does. It does not not exist for every function, but it does for most of the ones we will use here. Important! Careful with missing values (also called ‘NA’)! We will use ‘na.rm=TRUE’ as an option for several functions to tell R to ignore the missings. Unweighted summary statistics summary(atlas$yvar) mean(atlas$yvar, na.rm=TRUE) sd(atlas$yvar, na.rm=TRUE) Load package If you wanted to install and open the package Hmisc (which you will need to calculate the weighted statistics), run: install.packages(&quot;Hmisc&quot;) library(Hmisc) Weighted summary statistics You can weight means or other statistics. In our case, we want to use the population weighted statistics in several cases. That is, we want to put more weight on the value of a tract in which more people live than in another with lower population. Recall that the population variable is count_pooled. Weighted mean: wtd.mean(atlas$yvar, atlas$count_pooled) Weighted standard deviation: sqrt(wtd.var(atlas$yvar, atlas$count_pooled)) Subset observations State level: If you want to select a subset of observations, you can add the rule for selecting those observations, and the filter function. Here we subset the observations for the State of Wisconsin, and called the resulting dataset atlas_wisconsin. atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) County level: We can do the same but now for a specific county, adding an extra rule after a comma , or &amp;. Here we subset the observations for Milwaukee County in Wisconsin: atlas_milwaukee &lt;- atlas %&gt;% filter(state == 55 &amp; county == 079) Standardize variables You can standardize variables by substracting the mean and dividing by the standard deviation. Let us say that you want to standardize only considering the variables in Milwaukee, then you can do this: atlas_milwaukee&lt;- atlas_milwaukee %&gt;% mutate(x_std=(xvar - mean(atlas_milwaukee$xvar, na.rm=TRUE))/sd(atlas_milwaukee$xvar, na.rm=TRUE)) As an example, let’s say you want to standardize both the measure of mobility and the annual job growth for the data in Wisconsin: atlas_wisconsin&lt;- atlas_wisconsin %&gt;% mutate(kfr_pooled_p25_std=(kfr_pooled_p25 - mean(atlas_wisconsin$kfr_pooled_p25, na.rm=TRUE))/sd(atlas_wisconsin$kfr_pooled_p25, na.rm=TRUE), ann_avg_job_growth_2004_2013_std=(ann_avg_job_growth_2004_2013 - mean(atlas_wisconsin$ann_avg_job_growth_2004_2013, na.rm=TRUE))/sd(atlas_wisconsin$ann_avg_job_growth_2004_2013, na.rm=TRUE)) Run regression I have written a whole section explaining regression in more detail: Section 14. Please see that for further details. But here is a quick help. Simple linear regression Let’s say you want to run a simple regression of variable yvar on variable xvar1 for the county of Milwaukee. We will save the results of that regression in an object call mod1 (we could give it any name). Then you would do this: mod1 &lt;- lm(yvar~xvar1, data = atlas_milwaukee) To see what the outcome of the regression is, you would use the function summary and apply it to our new object mod1, like this: summary(mod1) As an example, we could regress the mobility of children from parents in the 25th percentile (kfr_pooled_p25) on the average annual job growth rate between 2004 and 2013 (ann_avg_job_growth_2004_2013). To do that, we would run: mod1 &lt;- lm(kfr_pooled_p25~ann_avg_job_growth_2004_2013, data = atlas_milwaukee) Multivariate linear regression You might want to understand the relationship between yvar and variable xvar1 while holding fixed another variable xvar2 for neighborhoods only in Milwaukee. You can do this: mod2 &lt;- lm(yvar~xvar1+xvar2 + xvar3, data = atlas_milwaukee) How to read the regression output? To simplify the interpretation, let’s run a regression where you use the standardize both the measure of mobility and the annual job growth for the data in Wisconsin: mod3 &lt;- lm(kfr_pooled_p25_std ~ ann_avg_job_growth_2004_2013_std, data = atlas_wisconsin) summary(mod3) ## ## Call: ## lm(formula = kfr_pooled_p25_std ~ ann_avg_job_growth_2004_2013_std, ## data = atlas_wisconsin) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.829 -0.569 0.035 0.684 5.128 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.00000978 0.02681564 0.00 1.000 ## ann_avg_job_growth_2004_2013_std -0.05131843 0.02679889 -1.91 0.056 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.999 on 1386 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.00264, Adjusted R-squared: 0.00192 ## F-statistic: 3.67 on 1 and 1386 DF, p-value: 0.0557 You should focus here on interpreting the coefficient for ann_avg_job_growth_2004_2013_std. You should pay attention to both the magnitude of the number, and the sign. In this case, you would read it like this: In Wisconsin, increasing one standard deviation the average annual job growth rate is correlated with an reduction of 0.05 standard deviations in the economic mobility for children with parents in the 25th percertile. Confidence Intervals To see the confidence intervals for the coefficients, you can run the function confint on the saved linear regression model: confint(mod3) ## 2.5 % 97.5 % ## (Intercept) -0.0526 0.05259 ## ann_avg_job_growth_2004_2013_std -0.1039 0.00125 It will give you the range in which the estimated would fall 95 our of 100 times if you would run the same exercise with a different sample of the population. Plotting the linear relationship You need to load the ggplot2 package (which should be installed already). library(ggplot2) Suppose you want to visually see the linear relationship between two variables in the atlas_milwaukee dataset that we filtered above. Then, you can do this: ggplot(data = atlas_milwaukee) + geom_point(aes(x = xvar1, y = yvar)) + geom_smooth(aes(x = xvar1, y = yvar), method = &quot;lm&quot;, se = F) The function geom_smooth() adds the line that you calculated above for mod1, where you ran mod1 &lt;- lm(yvar~xvar1, data = atlas_milwaukee). For more, see Section 12. "],["project4.html", "Project 4 Using Google DataCommons to Predict Social Mobility Instructions Data Description Cheatsheat commands", " Project 4 Using Google DataCommons to Predict Social Mobility Posted: Wednesday, August 28, 2024 Due: Midnight on Friday, November 27, 2024 You have already explored the Opportunity Atlas in Project 1 and the Social Capital Atlas. In this this empirical project, you will use the Opportunity Atlas mapping tool and the underlying data, plus data from Google DataCommons, to predict intergenerational mobility using machine learning methods. The measure of intergenerational mobility that we will focus on, as usual, is the mean rank of a child whose parents were at the 25th percentile of the national income distribution in each county (kfr_pooled_p25). Your goal is to construct the best predictions of this outcome using other variables, an important step in creating forecasts of upward mobility that could be used for future generations, before data on their outcomes become available. I am passing along a “training” dataset, made of a random sample of 70% of all neighborhoods in the original Opportunity Atlas (train.rds). You will use predictors, presented in Table 1, to predict the variable kfr_pooled_p25. There are 121 predictors in these data. Obviously, you do not need to use all the 121 variables for your prediction. You then need to test your model on the 50% that I have passed along in a different file for testing (test.rds). Important! You do not need to use all the variables in atlas_training for your prediction! Follow the instructions for each question. You can load the data by using the following code: train&lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas_train.rds&quot;))) test&lt;-readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas_test.rds&quot;))) Instructions As usual, you will work on Posit Cloud for this project. Write your responses within a Quarto/RMarkdown here file in the project4 tab in Posit Cloud. Specific questions Load the atlas_training.rds file. Produce simple summary statistics (mean and standard deviation) for the 10 predictors you selected from the data and krf_pooled_p25. Run a linear regression of krf_pooled_p25 using only 10 predictors, inspect the results, and comment on what you findings. That is, interpret the predicted changes in mobility as your 10 predictors change. How well does your linear regression predict krf_pooled_p25 in-sample? Present the RMSE. How well does your linear regression predict krf_pooled_p25 out-of-sample? Present the RMSE. Implement a decision tree model to predict krf_pooled_p25 using the code in below (covered in class). Plot the decision tree if possible. What are the main predictors? How well does your decision tree predict krf_pooled_p25 in-sample? Present the RMSE. How well does your decision tree predict krf_pooled_p25 out-of-sample? Present the RMSE. Which model performs a better prediction? Data Description The total data (train + test) consist of n = 73,278 U.S. Census tracts. For more details on the construction of the variables included in this data set, please see Chetty, Raj, John Friedman, Nathaniel Hendren, Maggie R. Jones, and Sonya R. Porter. 2018. “The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility.”, NBER Working Paper No. 25147. Table 1 Definitions of Variables in train and test Variable name Label Obs. (1) (2) (3) 1. Geographic identifiers tract Tract FIPS Code (6-digit) 2010 73,278 county County FIPS Code (3-digit) 73,278 state State FIPS Code (2-digit) 73,278 cz Commuting Zone Identifier (1990 Definition) 72,473 2. Characteristics of Census tracts hhinc_mean2000 Mean Household Income 2000 72,302 mean_commutetime2000 Average Commute Time of Working Adults in 2000 72,313 frac_coll_plus2010 Fraction of Residents with a College Degree or More in 2010 72,993 frac_coll_plus2000 Fraction of Residents with a College Degree or More in 2000 72,343 foreign_share2010 Share of Population Born Outside the U.S. 72,279 med_hhinc2016 Median Household Income in 2016 72,763 med_hhinc1990 Median Household Income in 1999 72,313 popdensity2000 Population Density (per square mile) in 2000 72,469 poor_share2010 Poverty Rate 2010 72,933 poor_share2000 Poverty Rate 2000 72,315 poor_share1990 Poverty Rate 1990 72,323 share_black2010 Share black 2010 73,111 share_hisp2010 Share Hispanic 2010 73,111 share_asian2010 Share Asian 2010 71,945 share_black2000 Share black 2000 72,368 share_white2000 Share white 2000 72,368 share_hisp2000 Share Hispanic 2000 72,368 share_asian2000 Share Asian 2000 71,050 gsmn_math_g3_2013 Average School District Level Standardized Test Scores in 3rd Grade in 2013 72,090 rent_twobed2015 Average Rent for Two-Bedroom Apartment in 2015 56,607 singleparent_share2010 Share of Single-Headed Households with Children 2010 72,564 singleparent_share1990 Share of Single-Headed Households with Children 1990 72,196 singleparent_share2000 Share of Single-Headed Households with Children 2000 72,285 traveltime15_2010 Share of Working Adults w/ Commute Time of 15 Minutes Or Less in 2010 72,939 emp2000 Employment Rate 2000 72,344 mail_return_rate2010 Census Form Rate Return Rate 2010 72,547 ln_wage_growth_hs_grad Log wage growth for HS Grad., 2005-2014 51,635 jobs_total_5mi_2015 Number of Primary Jobs within 5 Miles in 2015 72,311 jobs_highpay_5mi_2015 Number of High-Paying (&gt;USD40,000 annually) Jobs within 5 Miles in 2015 72,311 nonwhite_share2010 Share of People who are not white 2010 73,111 popdensity2010 Population Density (per square mile) in 2010 73,194 ann_avg_job_growth_2004_2013 Average Annual Job Growth Rate 2004-2013 70,664 job_density_2013 Job Density (in square miles) in 2013 72,463 3. Measures of Upward Mobility from the Opportunity Atlas kfr_pooled_p25 Household income ($) at age 31-37 for children with parents at the 25th percentile of the national income distribution 72,011 kfr_pooled_p75 Household income ($) at age 31-37 for children with parents at the 75th percentile of the national income distribution 72,012 kfr_pooled_p100 Household income ($) at age 31-37 for children with parents at the 100th percentile of the national income distribution 71,968 kfr_natam_p25 Household income ($) at age 31-37 for Native American children with parents at the 25th percentile of the national income distribution 1,733 kfr_natam_p75 Household income ($) at age 31-37 for Native American children with parents at the 75th percentile of the national income distribution 1,728 kfr_natam_p100 Household income ($) at age 31-37 for Native American children with parents at the 100th percentile of the national income distribution 1,594 kfr_asian_p25 Household income ($) at age 31-37 for Asian children with parents at the 25th percentile of the national income distribution 15,434 kfr_asian_p75 Household income ($) at age 31-37 for Asian children with parents at the 75th percentile of the national income distribution 15,360 kfr_asian_p100 Household income ($) at age 31-37 for Asian children with parents at the 100th percentile of the national income distribution 13,480 kfr_black_p25 Household income ($) at age 31-37 for Black children with parents at the 25th percentile of the national income distribution 34,086 kfr_black_p75 Household income ($) at age 31-37 for Black children with parents at the 75th percentile of the national income distribution 34,049 kfr_black_p100 Household income ($) at age 31-37 for Black children with parents at the 100th percentile of the national income distribution 32,536 kfr_hisp_p25 Household income ($) at age 31-37 for Hispanic children with parents at the 25th percentile of the national income distribution 37,611 kfr_hisp_p75 Household income ($) at age 31-37 for Hispanic children with parents at the 75th percentile of the national income distribution 37,579 kfr_hisp_p100 Household income ($) at age 31-37 for Hispanic children with parents at the 100th percentile of the national income distribution 35,987 kfr_white_p25 Household income ($) at age 31-37 for white children with parents at the 25th percentile of the national income distribution 67,978 kfr_white_p75 Household income ($) at age 31-37 for white children with parents at the 75th percentile of the national income distribution 67,968 kfr_white_p100 Household income ($) at age 31-37 for white children with parents at the 100th percentile of the national income distribution 67,627 3. Counts of number of children under 18 in 2000 (to calculate weighted summary statistics) count_pooled Count of all children 72,451 count_white Count of White children 72,451 count_black Count of Black children 72,451 count_asian Count of Asian children 72,451 count_hisp Count of Hispanic children 72,451 count_natam Count of Native American children 72,451 4. Measures of Social Capital ec_zip Baseline definition of economic connectedness: two times the share of high-SES friends among low-SES individuals, averaged over all low-SES individuals in the ZIP code. See equations (1), (2), and (3) of Chetty et al. (2022a) for a formal definition. 71,516 ec_high_zip Economic connectedness for high-SES individuals: two times the share of high-SES friends among high-SES individuals, averaged over all high-SES individuals in the ZIP code. 71,516 clustering_zip The average fraction of an individual’s friend pairs who are also friends with each other. See equations (4) and (5) of Chetty et al. (2022a). They include links to people outside the ZIP code when calculating individual clustering (equation 4), but only average individual clustering over users in the relevant ZIP code to compute clustering at the ZIP code level (equation 5). 71,950 volunteering_rate_zip The percentage of Facebook users who are members of a group which is predicted to be about ‘volunteering’ or ‘activism’ based on group title and other group characteristics. We do not include groups that have the privacy setting ‘secret’ enabled. We additionally manually review the 50 largest such groups in the United States and the largest group in each state, and remove the very small number of groups that are clearly misclassified. 71,950 civic_organizations_zip The number of Facebook Pages predicted to be “Public Good” pages based on page title, category, and other page characteristics, per 1,000 users in the ZIP code. They remove pages that do not have a website linked, do not have a description on their Facebook page or do not have an address listed. We then assign the page to a ZIP code on the basis of its listed address. 71,938 5. Other variables GenderIncome Inequality_2018 Gender Income Inequality in 2018, for person 15 years or older with income. MedianIncome Person_2020 Median Income per person in 2020 MedianAgePerson_2020 Median Age per person in 2020 CountPersonNoHealth Insurance_2020 Number of people with no health insurance (public or private) in 2020. CountPerson Divorced_2020 Number of people divorced in 2020 CountGedOrAlternative Credential_2020 Number of people with GED or alternative credential in 2020 PersonWithDisability_2019 Count of people with some type of disability in 2019 CountHouseholdInternet WithoutSubscription_2020 Number of households with internet access without subscription in 2020 LimitedEnglishSpeaking Household_SpanishSpokenAtHome_2019 Count of households that speak limited English and speak Spanish at home in 2019 Household_WithFoodStamps InThePast12Months_AbovePovertyLevelInThePast12Months_2019 Number of households in 2019 that received food stamps and that are above the poverty level in the past 12 months Count_Household_With 0AvailableVehicles_2020 number of households that have 0 vehicles in 2020 Count_Person_Single MotherFamilyHousehold_2020 Number of single mother family households in 2020 Count_Person_Single FatherFamilyHousehold_2020 Number of single father family households in 2020 Count_NotAUS Citizen_2020 Number of people that were not US Citizens in 2020 Count_Person_Speak EnglishNotAtAll_2020 Number of people that do not speak English at all in 2020 Count_Medicare Enrollee_2016 Number of people enrolled in Medicare in 2016 Count_Death_2017 Number of deaths in 2017 LowerConfInterval_Percent_ Person_BingeDrinking_2018 Percent of people that practice binge drinking in 2018 (reported as the lower confidence interval) LowerConfInterval_Percent_ Person_Obesity_2018 Percent of people with obesity in 2018 (reported as the lower confidence interval) Value Percent_Person_ WithDiabetes_2018 Percent of population with diabetes in 2018 Median_Cost_HousingUnit_ WithMortgage_2020 Median Cost of a housing unit with mortgage in 2020 Count_Person_19To 34Years_2020 Count of people that are between 19 to 34 years old in 2020 Median_Income_Household_ HouseholderRaceHispanic OrLatino_2020 Median household income for households where the householder’s race is Hispanic or Latino in 2020 Median_Income_Household_ HouseholderRaceWhite Alone_2020 Median household income for households where the householder’s race is White only in 2020 count_hh_bachhigher_ married_belowp2019 Number of households in 2019 where the householder has a bachelor’s degree or higher, for a married household below the poverty level in the past 12 months To see all other social capital variables not defined above, see here. Cheatsheat commands R command Description How to tell R that you have a categorical variable? Recall that if you modify one variable in one data set, you should do the same on the other one as well (datasets are train and test) Below X is just a placeholder for the actual variable you want to converto into a factor. atlas_train$X&lt;-as.factor(atlas_train$X) Load package You will need three new packages for this project. Remember to install them first. install.packages(&quot;caret&quot;) install.packages(&quot;rpart&quot;) install.packages(&quot;rpart.plot&quot;) library(caret) library(rpart) library(rpart.plot) Run regression As before, I have written a whole section explaining regression in more detail: Section 14. Please see that for further details. But here is a quick help. Multivariate linear regression You might want to understand the relationship between yvar and variable xvar1 while holding fixed another variable xvar2 for neighborhoods only in Milwaukee. You can do this: mod2 &lt;- lm(yvar~xvar1+xvar2 + xvar3, data = train) You would see the output from the model by running: summary(mod2) Measures of accuracy in prediction in a Multiple regression To assess how good is your model at predicting the outcome, you can use the Root Mean Square Error measurement. That will take the error in the prediction for each observation, square it, average it, and then take the root of that. You can estimate this measure in-sample (within the train dataset), or out-of-sample (for the test dataset). To do it in-sample, you can calculate it by checking the prediction directly from the estimated linear model: sqrt(mean(mod2$residuals^2)) where residuals is the error that the model predicts for each observation` To do it out-of-sample, you need to use the function predict that requires a model, and a dataset with the same variables for the prediction. In this case, you would do something like this: predict_mlr_model&lt;-predict(mlr_model,test) and then you can do this: actual_values&lt;- test$kfr_pooled_p25 rmse_mlrmodel_outsample &lt;- sqrt(mean((actual_values - predict_mlr_model)^2, na.rm=TRUE)) rmse_mlrmodel_outsample Estimating a Decision Tree To estimate the decision tree, you can choose which variables you want to select for the prediction by listing them as before. Say, you want to use x1, x2 and x3 for the prediction, you can do: tree &lt;- rpart(kfr_pooled_p25 ~x1+x2+x3, data = train) If you wan to use all other variables for the prediction, you can do: tree &lt;- rpart(kfr_pooled_p25 ~., data = train) To plot the tree you estimated, just do this: #Plotting the regression tree rpart.plot(tree) Measures of accuracy in a Decision Tree The measurement is the same, but to calculate it you need to do use the predict function as before. For in-sample prediction you would do: #In-sample prediction p &lt;- predict(tree, train) #Root mean squared error = 1944.108 (in sample) sqrt(mean((train$kfr_pooled_p25-p)^2)) For out-of-sample prediction, you can run: pred_tree_outofsample&lt;-predict(tree,test) #RMSE out of sample = 5965.278 sqrt(mean((test$kfr_pooled_p25-pred_tree_outofsample)^2, na.rm=TRUE )) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
